<!DOCTYPE html><html>
<head><meta name="author" content="Christopher Ferrall"><link             href='http://fonts.googleapis.com/css?family=PT+Mono|Open+Sans:400italic,700italic,400,700,800,300&subset=latin,latin-ext,greek-ext,greek' rel='stylesheet' type='text/css'></link>
<link rel="icon" href="img/452.png" type="image/png">
<link rel="stylesheet" type="text/css" href="css/doc.css"></link>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]], processEscapes: true}});</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><title>Applied Econometrics</title></head><body>

<div class="tp" style="background:url(img/452big.png) no-repeat  bottom center; background-size:70%;" ><h1>Applied Econometrics</h1><hr/><em>Econometric Methods Applied to Cross-Section and Panel Data Sets</em><br/>&nbsp;<br/>&nbsp;<br/><h3 class="preface">Christopher Ferrall<br/>&nbsp;<br/>Queen's University<br/></h3><br/>Version 1.0<br/>Printed: 2017-03-16<br/>&nbsp;</br>&nbsp;</br>CODE:  __________</div>
<h1 class="preface">Front Matter</h1><OL type="i">
<h2 class="preface"><LI>Table of Contents</LI></h2><div id="split">
<span style="font-size:small;">
<OL>
<OL type="I" class="toc1">
<LI><a href="s001.html" target="contentx">Introduction &amp; Overview</a></LI>
<OL type="A" class="toc2">
<LI><a href="s002.html" target="contentx">Outline</a></LI>
<LI><a href="s003.html" target="contentx">Schedule</a></LI>
<LI><a href="s004.html" target="contentx">Resources</a></LI>
<LI><a href="s005.html" target="contentx">Assessment<br/>Assignments &amp; Grades</a></LI>
<LI><a href="s006.html" target="contentx">StaRta<br/>Getting Started with Stata</a></LI>
</OL>
<DT><a href="ex001.html" target="contentx">Exercises for <em>Introduction &amp; Overview</em></a></DT>
<LI><a href="s007.html" target="contentx">What To Do<br/>Definitions, results, procedures</a></LI>
<OL type="A" class="toc2">
<LI><a href="s009.html" target="contentx">Review &amp; Reenforcement of Econ 250/351</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s010.html" target="contentx">Probability &amp; Random Variables</a></LI>
<LI><a href="s011.html" target="contentx">IID Sampling</a></LI>
<LI><a href="s012.html" target="contentx">Population Parameters</a></LI>
<LI><a href="s013.html" target="contentx">The Bernoulli Model for Binary Outcomes</a></LI>
<LI><a href="s014.html" target="contentx">The Normal &amp; Related Distributions</a></LI>
<LI><a href="s015.html" target="contentx">The One Variable Normal Linear Regression Model</a></LI>
<LI><a href="s016.html" target="contentx">Hypothesis Tests &amp; Confidence Intervals</a></LI>
</details>
</OL>
<LI><a href="s017.html" target="contentx">MLE<br/>Likelihood &amp; Maximum Likelihood Estimation</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s018.html" target="contentx">Likelihood</a></LI>
<LI><a href="s019.html" target="contentx">MLE: Maximum Likelihood Estimation</a></LI>
<LI><a href="s020.html" target="contentx">What makes an estimator good?</a></LI>
</details>
</OL>
<LI><a href="s021.html" target="contentx">Two Variable Models</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s022.html" target="contentx">The 2-Variable Normal Linear Regression Model</a></LI>
<LI><a href="s023.html" target="contentx">The 2-Variable Probit Model</a></LI>
<LI><a href="s024.html" target="contentx">Economics &amp; Econometrics: An Example</a></LI>
<LI><a href="s025.html" target="contentx">MLE &amp; OLS on Two-Variable nLRM</a></LI>
</details>
</OL>
<LI><a href="s026.html" target="contentx">Multivariate Models</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s027.html" target="contentx">Matrix Essentials</a></LI>
<LI><a href="s028.html" target="contentx">Linear Expectation Models in Matrix Form</a></LI>
<LI><a href="s029.html" target="contentx">Likelihood Ratio Tests</a></LI>
<LI><a href="s030.html" target="contentx">Tests of Overall Significance</a></LI>
</details>
</OL>
<LI><a href="s031.html" target="contentx">Beyond IID Sampling &amp; Regression/Probit</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s032.html" target="contentx">Heteroscedasticity &amp; Robust Standard Errors &amp; Clustering </a></LI>
<LI><a href="s033.html" target="contentx">Panel Data Models: Fixed &amp; Random Effects</a></LI>
<LI><a href="s034.html" target="contentx">Instrumental Variables</a></LI>
<LI><a href="s035.html" target="contentx">Ordered Probit, Multinomial Probit &amp; Tobit</a></LI>
</details>
</OL>
</OL>
<DT><a href="ex007.html" target="contentx">Exercises for <em>What To Do<br/>Definitions, results, procedures</em></a></DT>
<LI><a href="s036.html" target="contentx">How To Do It<br/>Data &amp; Stata</a></LI>
<OL type="A" class="toc2">
<LI><a href="s038.html" target="contentx">Data: Variables, Observations, Data Sets</a></LI>
<LI><a href="s039.html" target="contentx">Basic Stata</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s040.html" target="contentx">Learn how to use Stata effectively</a></LI>
<LI><a href="s041.html" target="contentx">Get Data In &amp; Out of Stata</a></LI>
<LI><a href="s042.html" target="contentx">Understand Your Data</a></LI>
<LI><a href="s043.html" target="contentx">Manipulate Data</a></LI>
</details>
</OL>
<LI><a href="s044.html" target="contentx">Computers Help Humans Understand Statistics<br/>Simulation &amp; Monte Carlo</a></LI>
<LI><a href="s045.html" target="contentx">More Stata</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s046.html" target="contentx">Must I Paint You a Picture?<br/>Graphing Data and Prediction</a></LI>
<LI><a href="s047.html" target="contentx">Something Stands for Nothing<br/>Handling Missing Values and Survey Data</a></LI>
<LI><a href="s048.html" target="contentx">Including Dummy/Indicator/Factor Variables in a Model</a></LI>
</details>
</OL>
</OL>
<DT><a href="ex036.html" target="contentx">Exercises for <em>How To Do It<br/>Data &amp; Stata</em></a></DT>
<LI><a href="s049.html" target="contentx">Group Tournaments, Partner Tasks, Course Project</a></LI>
<OL type="A" class="toc2">
<LI><a href="s051.html" target="contentx">In Class Demos and Tutorials</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s052.html" target="contentx">LR Tests and Robust Standard Errors</a></LI>
</details>
</OL>
<LI><a href="s053.html" target="contentx">In Class Group Tournaments</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s054.html" target="contentx">Don't Look Back in Anger: Econ 250/351 Review Questions</a></LI>
<LI><a href="s055.html" target="contentx">Should Have Been an Engineer Tournament</a></LI>
<LI><a href="s056.html" target="contentx">Levittation Tournament</a></LI>
<LI><a href="s057.html" target="contentx">Dissect an Applied Econometrics Article</a></LI>
</details>
</OL>
<LI><a href="s058.html" target="contentx">Takehome Partner Tasks</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s059.html" target="contentx">Bill Joel Sociology</a></LI>
<LI><a href="s060.html" target="contentx">Homer Simpson Econometrics</a></LI>
<LI><a href="s061.html" target="contentx">Proposition Joe Prospectus</a></LI>
</details>
</OL>
<LI><a href="s062.html" target="contentx">Quizzes and Optional Stata Challenges</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s063.html" target="contentx">Stata Challenge 1</a></LI>
<LI><a href="s064.html" target="contentx">Stata Quiz 1</a></LI>
<LI><a href="s065.html" target="contentx">Stata Quiz 2</a></LI>
</details>
</OL>
<LI><a href="s066.html" target="contentx">Course Project</a></LI>
<OL type="1" class="toc3">
<details class="toc" open><summary>sections</summary>
<LI><a href="s067.html" target="contentx">Overview</a></LI>
<LI><a href="s068.html" target="contentx">The Process</a></LI>
<LI><a href="s069.html" target="contentx">Some Data Sources</a></LI>
<LI><a href="s070.html" target="contentx">Creating a Research Report</a></LI>
<LI><a href="s071.html" target="contentx">Some Dos &amp; Don'ts</a></LI>
<a href="figlist.html" target="contentx">List of Figures &amp; Definitions &amp; Results</a>
<a href="glossary.html" target="contentx">Glossary of Defined Terms &amp; Special Symbols</a>
</span>
</div><div class="break"> </div>
<h2 class="preface"><LI>List of Figures &amp; Definitions &amp; Results</LI></h2><div id="split">
<span style="font-size:small;">
<OL>
<li><a href="#Fig1"> Random Variables </a></li>
<li><a href="#Fig2"> Cumulative Distribution Function (cdf) </a></li>
<li><a href="#Fig3"> Probability Density Function (pdf) </a></li>
<li><a href="#Fig4"> Independence </a></li>
<li><a href="#Fig5"> The Bernoulli Model </a></li>
<li><a href="#Fig1"> Bernoulli Distribution </a></li>
<li><a href="#Fig6"> Bernoulli IID Sampling Distribution </a></li>
<li><a href="#Fig7"> The Standard Normal Distribution </a></li>
<li><a href="#Fig8"> General Normal Distribution </a></li>
<li><a href="#Fig2"> Bell Curves </a></li>
<li><a href="#Fig9"> Three Functions of Normal Random Variables  </a></li>
<li><a href="#Fig3"> Some Chi-Squared Distributions </a></li>
<li><a href="#Fig10"> Required Elements of a Statistical Test </a></li>
<li><a href="#Fig4"> Degrees of Freedom of a Restriction (LR test only) </a></li>
<li><a href="#Fig5"> Fisher's Subtle Shift in Perspective </a></li>
<li><a href="#Fig11"> How to Derive the Likelihood under IID sampling </a></li>
<li><a href="#Fig6"> Two Bernoulli Log Likelihood Functions </a></li>
<li><a href="#Fig7"> nLRM Log-likelihood function for a particular sample </a></li>
<li><a href="#Fig12"> Maximum Likelihood Estimator </a></li>
<li><a href="#Fig13"> Bernoulli MLE. </a></li>
<li><a href="#Fig14"> One Variable Normal MLE </a></li>
<li><a href="#Fig15"> Good Properties of Estimators </a></li>
<li><a href="#Fig8"> Biased but Inconsistent Estimator </a></li>
<li><a href="#Fig16"> The Asymptotic Properties of MLE </a></li>
<li><a href="#Fig17"> The Gauss-Markov Theorem: OLS is BLUE </a></li>
<li><a href="#Fig9"> The Normal LRM </a></li>
<li><a href="#Fig18"> Alternative Definition of nLRM With Listed Assumptions </a></li>
<li><a href="#Fig10"> Binary Response Model </a></li>
<li><a href="#Fig19"> Definition of the Two-Variable Probit, version A. </a></li>
<li><a href="#Fig20"> Definition of the Probit Model, version B. </a></li>
<li><a href="#Fig21"> The log-likelihood function and ML estimates on the 2-Variable Probit </a></li>
<li><a href="#Fig11"> Labour Supply and Labour Force Participation </a></li>
<li><a href="#Fig22"> Definition of $K_x$-variable Normal Linear Regression Model. </a></li>
<li><a href="#Fig23"> The $K_x$-variable Probit Model (binary response version). </a></li>
<li><a href="#Fig12"> Restriction on the Parameter Space </a></li>
<li><a href="#Fig24"> The (strangely named) Likelihood Ratio Statistic </a></li>
<li><a href="#Fig25"> Distribution of the Likelihood Ratio </a></li>
<li><a href="#Fig26"> Recipe for Conducting a Likelihood Ratio Test </a></li>
<li><a href="#Fig13"> Bird's-eye View of a Session using Stata </a></li>
<li><a href="#Fig14"> Level 2 of the Stata Sandwich </a></li>
<li><a href="#Fig27"> The Standard Uniform Distribution </a></li>
<li><a href="#Fig15"> Inverting the CDF to Simulate a Random Experiment </a></li>
<li><a href="#Fig28"> The Lag operator $Ly$ </a></li>
<li><a href="#Fig29"> Features of a Pseudo-Random Number Generator (pRNG) </a></li>
<li><a href="#Fig16"> The Autocorrelation Function </a></li>
<li><a href="#Fig30"> How to Simulate an IID Sample (continuous) </a></li>
<li><a href="#Fig17"> Simulating a Normal Random Variable using Pseudo $U$.</a></li>
<li><a href="#Fig31"> A Monte Carlo Experiment </a></li>
</OL>
</span>
</div><div class="break"> </div>
<h2 class="preface"><LI>Glossary of Defined Terms &amp; Special Symbols</LI></h2><div id="split">
<span style="font-size:small;">
<OL>
<LI><a href="s010.html# <span>a <dfn i" target="contentx">random variable</a><DD>a <dfn id="randomvariable">random variable</dfn> is a <u>numerical</u> value associated with the outcome of a random experiment&emsp;&emsp; <em>See:Probability &amp; Random Variables</em></DD></LI>
<LI><a href="s010.html# <dfn" target="contentx">the joint cumulative distribution function (joint cdf)</a><DD>The <dfn id="jcdf">the joint cumulative distribution function (joint cdf)</dfn> of two or more random variables is the bivariate function that gives the probability that both are <em>below</em> certain values&emsp;&emsp; <em>See:Probability &amp; Random Variables</em></DD></LI>
<LI><a href="s010.html# and $Y_2$ be two disc" target="contentx">marginal distribution of a random variable</a><DD>the <dfn id="marginal-distribution">marginal distribution of a random variable</dfn> derives its distribution from the joint distribution of with other random variables&emsp;&emsp; <em>See:Probability &amp; Random Variables</em></DD></LI>
<LI><a href="s012.html#dfn id="pr" target="contentx">probability model</a><DD><dfn id="probmodel">probability model</dfn> is a mathematical representation of a random phenomenon [Yale Stats Course]&emsp;&emsp; <em>See:Population Parameters</em></DD></LI>
<LI><a href="s012.html#lity is des" target="contentx">parametric probability models</a><DD><dfn id="parammodel">parametric probability models</dfn>, which means the unknown and learnable aspects of the population is limited to parameters to be estimated from the data.&emsp;&emsp; <em>See:Population Parameters</em></DD></LI>
<LI><a href="s012.html#dfn id="pa" target="contentx">population parameter</a><DD><dfn id="parameter">population parameter</dfn> or just a <em>parameter</em> is a quantity in a probability model that helps determine the probability of events.&emsp;&emsp; <em>See:Population Parameters</em></DD></LI>
<LI><a href="s012.html#><dfn id="p" target="contentx">Parameter Space, $\Theta$</a><DD><dfn id="paramspace">Parameter Space, $\Theta$</dfn> is the feasible ranges of all parameters in the probability model.&emsp;&emsp; <em>See:Population Parameters</em></DD></LI>
<LI><a href="s012.html#<dfn id="e" target="contentx">estimator</a><DD><dfn id="estimator">estimator</dfn> of a parameter, which we can generically call $\hat\theta$, is a formula or an expression that depends on data.&emsp;&emsp; <em>See:Population Parameters</em></DD></LI>
<LI><a href="s016.html#istical pro" target="contentx">hypothesis</a><DD>a <dfn id="hypothesis">hypothesis</dfn> is simply a particular value or set of values of population parameters.&emsp;&emsp; <em>See:Hypothesis Tests &amp; Confidence Intervals</em></DD></LI>
<LI><a href="s016.html#still fol" target="contentx">null hypothesis</a><DD>the hypothesis to test the <dfn title="H0">null hypothesis</dfn> ("the null" or "H nought")&emsp;&emsp; <em>See:Hypothesis Tests &amp; Confidence Intervals</em></DD></LI>
<LI><a href="s016.html#d II errors" target="contentx">maintained hypothesis</a><DD>all the other elements of the statistical procedure (other than whether the parameters satisfy $H_0$ or not) can be considered the <dfn id="maintained">maintained hypothesis</dfn>&emsp;&emsp; <em>See:Hypothesis Tests &amp; Confidence Intervals</em></DD></LI>
<LI><a href="s016.html#n id="" target="contentx">Significance</a><DD><dfn id="alpha">Significance</dfn> (or level of significance) is the probability of making a Type I error and often denoted $\alpha$&emsp;&emsp; <em>See:Hypothesis Tests &amp; Confidence Intervals</em></DD></LI>
<LI><a href="s016.html#n id="" target="contentx">Power</a><DD><dfn id="power">Power</dfn> of a test is defined $1-\beta$, or 1 minus the probability of a Type II error&emsp;&emsp; <em>See:Hypothesis Tests &amp; Confidence Intervals</em></DD></LI>
<LI><a href="s016.html# <dfn id" target="contentx">critical region</a><DD>The <dfn id="critreg">critical region</dfn> of a test is the set of values of the computed test statistic that result in a rejection of $H_0$ in favour of $H_A$&emsp;&emsp; <em>See:Hypothesis Tests &amp; Confidence Intervals</em></DD></LI>
<LI><a href="s016.html#hink" target="contentx">degrees of freedom</a><DD><dfn id="dof">degrees of freedom</dfn> for a restriction ${\cal R}$ is the dimensions $\Theta$ <em>minus</em> the dimensions of ${\cal R}$&emsp;&emsp; <em>See:Hypothesis Tests &amp; Confidence Intervals</em></DD></LI>
<LI><a href="s019.html#an>maximizati" target="contentx">unrestricted ML</a><DD>maximization is over the whole parameter space we call this the <dfn id="unrestricted">unrestricted ML</dfn>&emsp;&emsp; <em>See:MLE: Maximum Likelihood Estimation</em></DD></LI>
<LI><a href="s020.html#rties b" target="contentx">standard error</a><DD><dfn id="stderr">standard error</dfn> is is used for the standard deviation of an estimator, $se(\hat\theta)$.&emsp;&emsp; <em>See:What makes an estimator good?</em></DD></LI>
<LI><a href="s020.html#n id=" target="contentx">Asymptotic properties or large sample properties</a><DD><dfn id="asym">Asymptotic properties or large sample properties</dfn> of an estimator relates to what happens as the sample size $N$ increases without bound.&emsp;&emsp; <em>See:What makes an estimator good?</em></DD></LI>
<LI><a href="s027.html#e a $n\" target="contentx">random  matrix</a><DD> $A$ is a <dfn id="ranmat">random  matrix</dfn> if  its are random variables.&emsp;&emsp; <em>See:Matrix Essentials</em></DD></LI>
<LI><a href="s029.html#dfn id="rest" target="contentx">restriction or restricted probability model</a><DD><dfn id="restriction">restriction or restricted probability model</dfn> is a subset of the parameter space, ${\cal R}\subset \Theta$.&emsp;&emsp; <em>See:Likelihood Ratio Tests</em></DD></LI>
<LI><a href="s029.html#dfn i" target="contentx">restricted maximum likelihood estimator</a><DD><dfn id="rmle">restricted maximum likelihood estimator</dfn> is the value of $\hat\theta$ that maximizes the likelihood over a restriction ${\cal R}$.&emsp;&emsp; <em>See:Likelihood Ratio Tests</em></DD></LI>
<LI><a href="s040.html#><dfn " target="contentx">Basic Stata Commands</a><DD><DT><dfn id="basic">Basic Stata Commands</dfn></DT><DD><pre>log/close   use/save   merge   save   do/doedit<br/>display   summarize   describe   list tabulate<br/>generate   replace   drop/keep   drop/keep if<br/>mvencode/mvdecode  regress   probit   test   lrtest<br/>xtsetxtreg   xtprobit   ivregress   ivprobit<br/>scatter   line   twoway   hist</pre></DD>&emsp;&emsp; <em>See:Learn how to use Stata effectively</em></DD></LI>
<LI><a href="s044.html#ingle simula" target="contentx">replication</a><DD>A single simulated sample is called a <dfn id="replication">replication</dfn>&emsp;&emsp; <em>See:Computers Help Humans Understand Statistics<br/>Simulation &amp; Monte Carlo</em></DD></LI>
<LI><a href="s047.html#5 or" target="contentx">NaN</a><DD>computer hardware has special codes treated as "missing".  One such code is <dfn id="NaN">NaN</dfn>, which stands for Not A Number&emsp;&emsp; <em>See:Something Stands for Nothing<br/>Handling Missing Values and Survey Data</em></DD></LI>
<LI><a href="s047.html#issings " target="contentx">Stata missing codes</a><DD>Current versions of Stata continue to use <code>.</code> for missing, but now multiple values of <dfn id="missing">Stata missing codes</dfn> are available as <code>.a</code>, <code>.b</code>, etc.&emsp;&emsp; <em>See:Something Stands for Nothing<br/>Handling Missing Values and Survey Data</em></DD></LI>
<LI><a href="s047.html#ey con" target="contentx">Valid Skip</a><DD>In survey data, a <dfn id="valid">Valid Skip</dfn> creates missing information because the question was skipped for a valid reason, because, based on other questions, we know the answer or the question is meaningless&emsp;&emsp; <em>See:Something Stands for Nothing<br/>Handling Missing Values and Survey Data</em></DD></LI>
<LI><a href="s047.html#me surve" target="contentx">Invalid Skip</a><DD>In survey data, a <dfn id="invalid">Invalid Skip</dfn> creates missing information because the question was not asked and we cannot infer the answer from other questions.&emsp;&emsp; <em>See:Something Stands for Nothing<br/>Handling Missing Values and Survey Data</em></DD></LI>
</OL>
</span>
</div><div class="break"> </div>
</OL>
<OL  type="I" class="toc1" >
<h1><a name="s001"><LI>Introduction &amp; Overview</LI></a></h1>

This section of Econ 452 concerns the practice of MICROeconometrics.  Applied econometrics is both a science and an art. This course builds on the foundation of Econ 250 and Econ 351 to develop both aspects. Classes will consist of a mix of lectures, demonstrations and individual and group exercises.  The meetings will take advantage of the active learning classroom, Ellis 319.</p>

The primary determinant of the course grade is the Course Project. The Course Project requires students to find data to apply microeconometric techniques in order to address a question of interest.  Students can find look for data in several repositories of research data sets covering Canada, the U.S. and other countries.  They can also look for other sources of data.</p>

Several take-home and in-class tasks are assigned before the project. There are three types of tasks:
<ul>
  <li>Individual in-class quizzes</li>
  <li>Partner take-home assignments</li>
  <li>Group in-class activities</li>
</ul>
These tasks are assessed (graded) for demonstrating a minimum standard. That is, they are all Pass/Fail.  Your base grade before the Course Project is determined by how many of these tasks you pass, as explained below.</p>

These notes are divided into five parts:
<ul>
  <li>This first introductory section </li>
  <li>An overview of statistics and econometrics (<q>what to do</q> in the Course Project)</li>
  <li>Explanation of Stata and microeconometric data sets(<q>how to do</q> the Course Project)</li>
  <li>Description of some of the take-home and in-class tasks</li>
  <LI>Econometric research reports and the Course Project</LI>
</ul>
The notes are still in progress.  Not everything covered in class or needed to do well on the Course Project is contained in this version.
<OL  type="A" class="toc2" >
<h2><a name="s002"><LI>Outline</LI></a></h2>

<OL type="I">
<LI>Review &amp; Reenforcement of Econ 250/351</LI>
<OL type="A">
<LI>Probability, Random Variables, Distributions, Expectations, Hypothesis Testing</LI>
<LI>Normal and Related Distributions</LI>
<LI>Two Univariate Probability Models: Bernoulli and one-variable normal linear "regression".</LI>
<LI>Likelihood and Maximum Likelihood
</LI><LI>Bivariate Normal Linear Regression and Bivariate Probit (and Logit)
</LI><LI>Implementation in Stata</LI>
</OL>

<h4>Milestone A</h4>
 <blockquote>At this point a successful student will be able to use Stata to manipulate and explore data sets and to interpret the some of the output of the regress and probit commands. They will be somewhat familiar with how Stata deals with missing data, weighted sampling and categorical variables. Student understanding of background material will be refreshed and augmented by the concept of likelihood and some technical aspects of probability not emphasized earlier.</blockquote>

<LI>Multivariate Econometric Models</LI>

<OL type="A">Concepts
<LI>Conditional Distributions, Expectations
</LI><LI>Matrix Notation and Random Vectors
</LI><LI>Multivariate Linear Regression and Probit/Logit
</LI><LI>MLE and/or OLS
</LI><LI>Likelihood Ratio and Other Tests
</LI><LI>Functional Form and Specification of Econometric Models
</LI><LI>Implementation in Stata
</LI></OL>

<h5>Milestone B</h5>
 <blockquote>At this point a successful student will be able to choose among common specifications of multivariate regressions and probits.  They will be able to interpret all the output of related Stata commands and use the results and other commands to test hypotheses, form predictions with confidence intervals and identify potential problems with the model.</blockquote>

<LI>Advanced Techniques for more complex data</LI>
<OL type="A">
<LI>Panel Data Methods (fixed and random effects)
</LI><LI>Instrumental Variables
</LI><LI>Tobit, Ordered Probit, Multinomial Logit (probit)
</LI><LI>Implementation in Stata</LI></OL>

<h5>Milestone C</h5>
 <blockquote>Successful students will have a basic understanding of how to relax limitations of standard (ideal) assumptions of econometric models to handle complex data sets and deeper questions of interest.  They will understand the basic use of Stata commands to carry out the estimation.
</blockquote>

<LI>The Research Process</LI>

<OL type="A">
<LI>Steps in an Applied Econometric Project
</LI><LI>Format and Content of a Report
</LI><LI>Reading and Assessing Applied Econometrics</LI></OL>

<h5>Milestone D</h5>
<blockquote>Successful students will be able to read an applied econometrics article using techniques covered in the Econ 250-351-452 sequence to find the key aspects and possible weaknesses of the analysis. Given a topic and estimation results, students will understand how to write a report to communicate their results in a way expected by trained economists. </blockquote>

</OL> 
<h2><a name="s003"><LI>Schedule</LI></a></h2>
<div class="break"></div><UL>
<LI>Week 1</LI>
    <OL class="compact">
    <LI>Overview, Self-Assessment (30m)</LI>
    <LI>Group Tournament A</LI>
    <LI>Extra Stata Tutorial 1</LI>
    </OL>
<LI>Week 2</LI>
    <OL class="compact">
    <LI>Review and Augmenting Econ 250/351 Material</LI>
    <LI>Likelihood</LI>
    <LI>Extra Stata Tutorial 2</LI>
    </OL>
<LI>Week 3</LI>
    <OL class="compact">
    <LI>MLE</LI>
    <LI></LI>
    <LI>Individual Stata Quiz 1 (30m)</LI>
    <LI>Group Tournament B (40m)</LI>
    </OL>
<LI>Week 4</LI>
    <OL class="compact">
    <LI>Matrix Notation and  Multivariate Models</LI>
    <li></li>
    </OL>

<LI>Week 5</LI>
    <OL class="compact">
    <LI>Multivariate Probit</LI>
    <LI>Likelihood Ratio and Other Tests</LI>
    <LI>Partner Task 1 Due (Billy Joel Sociology)</LI>
    </OL>

<LI>Week 6</LI>
    <OL class="compact">
    <LI>Functional Form and Specification</LI>
    <li>Monte Carlo</li>
    <LI>Stata Quiz 2 (30m)</LI>
    </OL>

<LI>Week 7</LI>
    <OL class="compact">
   <LI>Panel Data Methods</LI>
    <LI>Instrumental Variables</LI>
    <LI>Group Tournament C</LI>
    </OL>

<LI>Week 8</LI>
    <OL class="compact">
    <LI>Other Advanced Techniques</LI>
    <LI>Partner Task 2 Due (Homer Simpson)</LI>
    <LI>Homer Simpson Due</LI>
    </OL>

<LI>Week 9</LI>
    Catch Up

<LI>Week 10</LI>
    <OL class="compact">
    <LI>Steps in An Econometric Project</LI>
    <LI>Report Format and Content</LI>
    <LI>Decoding / Assessing a Report</LI>
    <LI>Partner Task 3 Due (Prop Joe Proposal)</LI>
    </OL>

<LI>Week 11</LI>
    <OL>
    <LI>Work on Projects</LI>
    <LI>Group Tournament D (40m)</LI>
    </OL>

<LI>Week 12</LI>
  Work on Projects

</UL> 
<h2><a name="s004"><LI>Resources</LI></a></h2>

<DT>onQ</DT>

<DD>onQ/Econ452 will be used for submitting Course Projects, Peer-Review Reports and other tasks.</DD>

<DT>Stata</DT>

<DD>Students are expected to purchase a copy of Stata/IC for use in this course. (Details provided later in the notes.) Stata is also installed on the computers in Dunning 350 for your use, but that room has limited opening times and <em>should not be solely relied on for completing the projects</em>.</DD>

<DT>CF452 Stata</DT>
<DD>Stata data sets and programs that supplement our study of econometrics are provided at the CF452 site accessed directly in Stata. This Stata material is hosted here: <code>http://edith.econ.queensu.ca/CF/452</code>.  Explanation for accessing this information is given in the <em>Starta</em> section (I.D).</DD>


<DT>Hardware</DT>

<DD>Students are expected to have access to a laptop with Stata installed on it during class and extra Stata tutorials.  Two students can sit together and use one laptop, and this will count as access.  In this case both students are encouraged to spend time doing the work and not have one student always looking on. But you may find having access to Stata in any lecture useful. Students may also find it helpful to access other material online during class. </DD>

<DT>Lecture notes &amp; Textbook</DT>

<DD>These notes supplement prerequisite material. Some material may be presented in class that is not in these notes. It is the student's responsibility to obtain material present in class from other students.</DD>

<DD>No other text is required for this course. The references below cover much of the material and are readily available if you wish to supplement the notes.
<DD>Supplementary texts</DD>
<UL>
<LI>Your textbooks and lecture notes from the prerequisite courses</LI>
<LI><em>Introductory Econometrics: A Modern Approach</em>, Jeff Wooldridge, South-Western.</LI>
<LI><em>Using Stata For Principles of Econometrics</em>, Lee C. Adkins And R. Carter Hill, Wiley.</LI>
<LI><em>Basic Econometrics</em>, Damodar N. Gujarati, McGraw Hill.</LI>
<LI>Wikipedia entries on the statistics topics is typically accurate, although some entries may follow different notation and conventions.</LI></UL>
</DD>

<DT>TurnItIn.Com</DT>

<DD>You will submit the report of the Course Project as a PDF to the DropBox for the project onQ.  Every submitted report will be uploaded by us to turnitin.com which will confirm that your text does not contain plagiarized material.  Turnitin is very sophisticated.  It is not fooled by changing a few selected words from a paragraph.  </DD>

<figure><H4>A snip of the summary of turnitin analysis</H4><img src="img/TurnItInSnip.png"/>
<figcaption>
This screenshot is part of the summary of the report that turnitin produced for Econ 452 F2015.   I can see a line-by-line comparison of each report and external sources.  Almost all reports were honest.  The matches were due to discussing reference papers and using data sets that other papers use.  One student thought they could fool the system.
</figcaption>
</figure>


<DT>Example Articles and Past Projects</DT>

<DD>Several articles are used as models for students learning to carry out and report an econometric analysis.  Links to these items are onQ.</DD>

<h2><a name="s005"><LI>Assessment<br/>Assignments &amp; Grades</LI></a></h2>
<h4>PRELIMINARY TASKS/QUIZZES/ACTIVITIES</h4>

These items are all marked as <mark>Pass</mark>/Attempt/Fail with corresponding scores <mark>2/1/0</mark>.  Fail includes not attempting / not attending / not meeting a very minimum standard for an attempt (below D+ on a typical scale).  Attempt (1 point) would include attending but not winning a tournament and submitting a quiz/homework of roughly D+ to C+ quality.

<h4> Quizzes (Individual)</h4>

There will be two individual in-class, paper-only quizzes to test basic understanding of Stata.   These quizzes will be short and time limited.  They will each be followed immediately in the same class by a group tournament.

<h4> Take Home Tasks (Partners)</h4>

<ul>
<li>Billy Joel Sociology</li>
<li>Homer Simpson Econometrics</li>
<li>Prop Joe Prospectus</li>
</ul>

Students can work on these tasks alone or with a partner.  Partnerships can be different for each task but partners are "pre-committed" before the task is due.  Once committed both partners get the same score on the task.

<h4>Inclass Group Tournaments</h4> Activities (Randomly Assigned Groups of 5 or 6 students)
<ul>
  <LI>Don't Look Back in Anger: Econ 250/351 Review Questions.</LI>
  <li>Should Have Been an Engineer: data description, variable construction, summary stats</li>
  <li>Levittation: Choose a Y variable, X variable and Model Specification</li>
  <li>Dissect an Applied Econometrics Article</li>
</ul>

Notes: Activities are competitive between groups.  At most half of the groups receive a 1, others receive a 0.  Attendance is taken 15 minutes after class starts and only group members present from then on are eligible for the point.</p>

Based on these items, each student will have a score between 0 and <mark>16</mark>.  This score determines the minimum course grade of the student.  (See MVP below.)
<dd><pre>
#passed     MINIMUM Course Grade 
---------------------------------
  0-5           D-
  6-11           D<mark>+</mark>
   12+           C<mark>+</mark>
</pre>
NB: If for some reason the number of individual/partner tasks actually assigned falls below 5, due to instructor illness or other course interruptions, then the cut-offs will be modified.</dd>

<h4>COURSE PROJECT</h4>

The course project can be done alone or with a partner.  As with take home tasks, partners pre-commit.

<DT>MVP (Most Valuable Partner)</DT>
<DD>Up to two students working with a partner may be given the MVP award.  This requires that their contribution to the Course Project was clearly much greater than equal.  It is not tied directly to the quality of the project.  Instead it is a mechanism to compensate students whose partner's shirk.  A project MVP award is worth 1 point added to the number of passes.</DD>

<DT>Mapping of Preliminary Tasks and Project Score to Final Course Grades</DT>
<DD><pre>
Project|    # of tasks passed
Score  |  <mark>0-5     6-11     12+</mark>
-------|-----------------------
  0    |   <mark>F       F        F</mark>
  1    |   D-      D+       C+
  2    |   D       C-       B-
  3    |   D+      C        B
  4    |   C-      C+       B+
  5    |   C       B-       A-
  6    |   B-      B        A
  7    |   B       B+       A+
</pre></DD>

<H4>Course Project Milestones</H4>

The guide below gives "milestones" that projects will pass to get different scores. But because of the wide variety of projects and the many dimensions to consider the grade will be based on the report in full. For example, someone might have something that doesn't make sense in their specification but otherwise did a really good job.  So in one aspect the project might not meet the "5 points" standard but in other ways look like an "6 point" paper. In this case you can think that the project would get some "credit" towards the "6 point" milestone but not all the points toward the "5 points" milestone. However, in the end there is no arguing over "points", there is just a score that sums up the judgment of the TA and the instructor.</p>
<DL>
<DT>Milestone 0:

<DD><blockquote>A score of 0 will be given if a complete report is not submitted on time; or if what was submitted did not meet a very minimum standard; or there is compelling evidence of academic dishonesty.  In the latter situation students will be given a provisional score of 0 and then provided an opportunity to explain the situation in person.</blockquote></DD>

<DT>Milestone 1: (Just) Meeting Standard [1 points]</DT>

<DD><blockquote>The report does not contain plagiarized elements; data was acquired that meets the minimum standards; an econometric model covered in 452 was estimated on the data; the report contains proper English and gives the reader a reasonable idea of what was done and why; the log file produce the results presented in the report.</blockquote></DD>

<DT>Milestone 2: Getting Beyond the Minimum [3 points]</DT>

<DD><blockquote>The data are handled properly with no obvious errors;  the specification makes sense;  the relevant literature provides some support for the analysis;  the report is complete and formatted according to the models provided; the report makes it clear what was done and why.</blockquote></DD>

<DT>Milestone 3: One or More Excellent Feature [2 points] </DT>

<DD><blockquote>One or more of the following apply: complicated data were accessed and handled properly; sophisticated model or set of models were estimated correctly; the topic is complex but also closely connected to its literature; the results are used in a sophisticated way with real insight; in style the report is similar in quality to a published applied econometrics article. </blockquote></DD>

<DT>Milestone 4:  Undergraduate(s) produced this for a course? [1 point]</DT>

<DD><blockquote>The project and the report are similar in ambition, originality and clarity to the top half of the summer research essays produced by Econ Master's students every year. </blockquote></DD>

</DL> 
<h2><a name="s006"><LI>StaRta<br/>Getting Started with Stata</LI></a></h2>

<DT>Why do we use <code>Stata</code>?</DT>
There pros and cons to any resources required in a class (textbooks, software, etc).
<UL class="ul">The pros of Stata include:
<LI>It has been the leading statistical package in micro-econometrics (and other disciplines) for at least two decades.  It is a platform and a standard that comes up with many resources.</LI>
<LI>Because it is well-established standard many data sets you might use for this class are provided in Stata format</LI>
<LI>It makes handling data and carrying out the estimation for the final project easy (at least as easy as possible).</LI>
<LI>For producing verifiable results it is better than using a spreadsheet like Excel</LI>
</UL>
<UL class="ul">The cons of Stata include:
<LI>It is a commercial product so it is not free.  The "Grad Plan" purchasing arrangement we use for you to purchase Stata is priced just low enough that it is justified, but high enough that you will not see it as cheap.</LI>
<LI>It is a complex and comprehensive package not designed for beginners (although eventually most people find it intuitive). </LI>
</UL>

<h4>Get Stata Going on your computer (&asymp; 30m)</h4>
<OL class="steps">
<LI>Purchase Stata using the Grad Plan link:<a href="http://www.stata.com/coursegp">http://www.stata.com/coursegp</a>. The GradPlan ID is <b>CF452</b>.</LI>
<DT><mark>IMPORTANT: Which version?</mark></DT>
<DD>Versions of Stata vary by <em>number</em> (Stata 13 or Stata 14) and by <em>flavour</em>.</DD>
<DD>VERSION:  The current version of Stata is 14.  I have Stata 13, and you can use that version if you have a license for it, and probably Stata 12. I expect all the instructions and material I prepare for you will work with Stata 14, but I can't be sure until we start using it.</DD>
<DD>FLAVOUR: You must have access to the <b>Stata InterCooled</b> or <b>I/C</b> flavour.  Do <em>not</em> purchase the Small Stata flavour as it has limits that make it <u>unusable</u> for this course.</DD>
<img width="20%" src="img/stata-purchase.png"/>

<LI>Two emails will be sent to you from Stata.  The second will contain your <b>License</b> and <b>Activation Key</b>, When the email arrives from Stata, download and install Stata on your computer following the instructions. You should back up the <em>installer program</em> to a USB drive for backup purposes.</LI>

<LI>Start Stata by clicking on the icon.  Enter your license information to activate it.</LI>

<LI>Set Your <em>Working Directory</em>. When you install Stata it asks you to choose a folder/directory where Stata will put any files you work with.  I strongly suggested you create a directory for Econ452 at this point and select that as your <em>working directory</em>.  You can change the working directory in Stata under the <code>File</code> menu, but each time you start it begins at  the directory you set when you initialize Stata.</LI>

<LI><mark>Mac Users:</mark>  Stata on Macs can end up starting Stata</LI>

<LI>Here is what Stata will look like once you have installed and authorized it: <img width="80%" src="img/stata-start.png" />
The <em>Working Directory</em> is listed at the bottom of the window.  The two primary elements of the Stata screen is the <b>command window</b> and the <b>results window</b>.  Other windows include a history of commands and information about the data set you are currently using.  You can move and resize windows. If you want Stata to look like it does at first you reset the layout with this command:
<dd><pre>&bull;  window manage prefs default</pre></dd>
</LI>
</OL>

<h4>Get CF452 Stata Going (&asymp; 10m)</h4>

<OL class="compact">
<LI>Get the Econ 452 set up material (do not type the &bull;, just what follows it)</LI>
<DT>In the Stata command window:</DT>
<DD><pre>
&bull;  net from http://edith.econ.queensu.ca/CF/452
&bull;  net install CF452
</pre>
If this is successful you do not need to do this again.
</dd>

<LI>Run the Econ452 setup command inside Stata</LI>
<DT>In the Stata command window:</DT>
<DD><pre>&bull;  CF452</pre></dd>
<DD>This checks the website the Econ 452 Stata site for updates.<dd>
<DD>It also prompts you to enter your Econ 452 Code.  This will be needed later on for various things.</DD>
</OL>

<h4>Learn how to get help about Stata (&asymp; 5m)</h4>

<DD>Read the page that appears in the Stata viewer after doing this:
<pre>
MENU PATH                 OR               COMMAND <hr class="plain">
File &#10148; Help &#10148; Advice                     &bull; help advice
</pre>
<b>Read that page.</b>  Notice that it does <u>not</u> say "Search for advice using Google."  Most of the helpful information about Stata is inside the program and its PDF documentation.  These items are not indexed by Google.  The search command inside Stata will search this material.

<DT>Menus vs. Commands</DT>

In these notes I will show Stata Menu paths (using &#10148;) and the equivalent command line (after a &bull; because in the old days Stata prompted you with a period).  Stata started in the 1980s and was purely  &quot;command line&quot; driven.  Now almost every command can be performed from the drop down menus.  However, most people using Stata seriously, even people like you who did not grow up typing DOS commands, will do most of their work using commands.   The menus are useful when you are learning how a command is constructed.  But the menu options simply build up the Stata command for you.  When finished Stata pastes the command into the command window and runs it for you.  After a few times it is much easier and quicker to type the command yourself, or "page up" in the command menu and modify an earlier command than to click through the menus.  So within a few hours of Stata use many uses of the menus will be much slower and you will work with and edit commands directly.  Even then the menu system is helpful for building up commands that are complex and/or rarely used.
</p>

<h4>Use Help to learn how to do something useful in Stata (&asymp; 10m)</h4>
<DD><pre>
File &#10148; Help &#10148; Display                          &bull; help display
           OR
File &#10148; Data &#10148; Other Utilities &#10148; Hand Calculator
</pre></DD>
Read the information. You will probably find it completely unhelpful, but scroll down to the <u>Examples</u> section to see that <code>display</code> can be very simple. Stata help entries show many details for advance use, so it takes some practice to read it, and usually the examples of simple commands is what you are looking for..</p>
If I asked you to print out &quot;The answer is 2+2&quot; you would want to make Stata display:</DT>
<DD><pre>The answer is 4</pre></DD>
</OL>
<h1><a name="s007"><LI>What To Do<br/>Definitions, results, procedures</LI></a></h1>
<blockquote class="toc"><h4>Contents</h4>
<OL type="A" class="toc2">
<LI><a href="s009.html" target="contentx">Review &amp; Reenforcement of Econ 250/351</a></LI>
<LI><a href="s017.html" target="contentx">MLE<br/>Likelihood &amp; Maximum Likelihood Estimation</a></LI>
<LI><a href="s021.html" target="contentx">Two Variable Models</a></LI>
<LI><a href="s026.html" target="contentx">Multivariate Models</a></LI>
<LI><a href="s031.html" target="contentx">Beyond IID Sampling &amp; Regression/Probit</a></LI>
</OL>
</blockquote>
<OL  type="A" class="toc2" >
<h2><a name="s009"><LI>Review &amp; Reenforcement of Econ 250/351</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s010"><LI>Probability &amp; Random Variables</LI></a></h3>

<h5>Basic Concepts </h5>
<OL class="steps">
<li>A <em>random experiment</em>  is a process leading to two or more possible <em> outcomes</em>, with uncertainty as to which outcome will occur. For example, rolling a die or flipping a coin are random experiments. For this review of concepts, we will refer often to this random experiment:<p> <strong>RanEx: </strong> <em>Stop a person on the street and ask them questions</em>.</li>
<li>If we list or describe every possible outcome that might occur in a random experiment we call that the <em>sample space</em>.  To be complete, the sample space is not just the set of all outcomes; it also includes the probability that each outcome will appear in a random experiment.  The easiest case is when each outcome is equally likely (like rolling a die or flipping a fair coin).  But cases where outcomes are not equally likely are also easy to imagine.
    </p>
    <strong>RanEx: </strong> if I stop a person at the corner of Union and University it is much more likely that person is a Queen's student than a St. Lawrence College student (among other categories).  In that case, some people are more likely to show up in the random experiment than others so the probability of each possible outcome is not equal.</li>
<li>To get fancy, let $\Omega$ be a sample space, which includes both possible outcomes and a probability associated with each outcome. Yes, this is applied econometrics &hellip; and so notation like $\Omega$ is not really necessary, but it saves me having to write "the sample space" over and over again for the next page.</p>
    Sometimes the sample space is called the <em>population</em>.  The only issue with that term is it might make you think each outcome is equally likely (like Heads or Tails or each person in the population of Kingston).  But two populations with the same outcomes but different probabilities assigned to outcomes are two different sample spaces.
    </p><strong>RanEx: </strong> Stopping a person at Union and University at 10am during the semester is one sample space. Another sample space is stopping a person at Bagot and Princess at noon because the chances you (or anyone else) are the person stopped is different in the two experiments.</li>
<li>An <em>event</em> is just another name for a subset of the sample space.  So if $A$ is some event of the random experiment then $A \subseteq \Omega$.  If $y$ is an outcome then $y \in \Omega$.  And $y$ may either be an element of $A$ or not. When stopping a person on the corner is the experiment, one event is that the person is male.  The set of males is a subset of the population of all people, so it fits. </li>
</OL>

<DT>Notes</DT>
For the mathematically inclined student, the technical issues occur when the sample space has an uncountable number of outcomes. So technically $A$ and $B$ are not any two sets of the sample space, but any two members of the Borel $\sigma$ algebra on $\Omega$.  A upper level mathematics course in measure theory would explain what the last sentence means.  But for a finite sample space (say all the people living in Kingston), these technical issues go away.  Each person is a subset of the sample space and there is no problem which each of them (and all other subsets) having a probability assigned to them.</p>

Even without the technical issues, what probability is can be a philosophically sticky issue.  This is the relative frequency notion of probability. You may be interested in the Frequentist/Classical versus Bayesian Controversy: \rev{wiki/Probability}. Here we just will say that the probability of an event as <em>the relative frequency of the event's occurrence in repeated random experiments</em>.  For example, if we stop $N$ people on the corner we find that $m_N$ were male, then $m_N/N$ is th relative frequency of the male event.  Now we have to imagine letting $N$ get arbitrarily large (that is, let it go to infinity).  Then the limit of $m_N/N$ would be the probability of the event male.</p>

<h5>Required Properties of Probability</h5>

Probability must satisfy some conditions (assumptions) to make sense.  You probably know these conditions intuitively and you probably studied them in a prerequisite course.  And it is just as likely that you don't remember them or cannot state them.  So here they are:

<OL class="steps">
<LI>$P(\Omega)=1$. <DD>That is, something happens in a random experiment, so the chances of one of the possible events happening is 1.</DD></LI>
<LI>Let $A$ be any event (any subset of $\Omega$).  Then,  $0\le P(A) \le 1$.  <DD>That is, probability of an event must be between 0 and 1, because the relative frequency of events is a ratio;</DD></LI>
<LI>Let $B$ also be any event.  Then,  $P(A\cup B) = P(A)+P(B) - P(A\cap B)$. <DD>That is, the probability that $A$ or $B$ occurs is the sum of their individual probabilities minus the probability that they both occur). One way to think about this is that the first two terms "double count" outcomes that are in both $A$ and $B$ so we have to subtract off their probability to eliminate the double count.</DD></li>
</OL>

Just about everything else in this review of probability and statistics follows from those simple but abstract concepts.  All the other terms introduced later are based on the foundation above.


Probability relates to chances <em>before</em> the random experiment happens.  It is the <em>ex ante</em> chance. Once an outcome happens (<em>ex post</em>) probability is no longer relevant.  Either an event occurred or it did not.  However, <em>ex post</em> the frequency of an event in repeated experiments can teach us about its <em>ex ante</em> probability.



In words, <span>a <dfn id="randomvariable">random variable</dfn> is a <u>numerical</u> value associated with the outcome of a random experiment</span>. For example: Stop a person on the street and ask them their age in years. Then we can think of Age as a random variable. A way to visualize the random variable is </DT><DD>as the column on the sheet of paper or the spreadsheet used to record values associated with an experiment. For a particular person Age is a number, The concept recorded in the column is the random variable. This distinction is similar to "demand" versus "quantity demanded." So, technically,

<a name="Fig1"></a>
<div class="alg"><h4>Definition 1.  Random Variables </h4>
<UL>
 <LI>A random variable is a real-valued function of the outcome of a random experiment.  That is, if $Y$ is a random variable we write $Y:\Omega\to \Re$.</LI>

 <LI>A discrete random variable takes on only a finite number of different values with positive probability.</LI>

 <LI>A continuous random variable can take on any value in one or more intervals of the real line, such $(0,1)$ or $(-\infty,+\infty)$.</LI>
 </UL>
</div>

We distinguish between two types of random variables: discrete and continuous.  Another useful way to think of a random variable is the collection of possible measurements.   Once the random event happens the value of the random variable is realized.  Based on its definition a random variable is neither random nor a variable.  It is a deterministic function of something that is random (the outcome). What is random and variable is the realized value the random variable takes on in a particular experiment.</p>

If $Y$ is discrete then $P(Y=y)>0$ only if $y \in \{y^1,y^2,\dots,y^M\}$. The possible values of $Y$ have superscripts to distinguish them from different random variables, such as <var>Y<sub>1</sub></var>, <var>Y<sub>2</sub></var>. Actually a discrete random variable can take on an infinite but countable number of values such as the positive integers 1, 2, 3 ... </p>

Discrete random variables are often used to code qualitative aspects of outcomes.  For example, ask a person what race they identify as.  Answers might include <code>White</code>, <code>Black</code>, <code>Southeast Asian</code>, etc.  Race itself is not numerical so it is not a random variable.  When we use numerical values to encode non-numerical information it becomes a random variable.  So <code>Answer</code> is not a random variable  but <code>RaceCode</code> is:
<DD><pre>
Answer                  RaceCode
------------------------------
Black                      1
White                      2
SoutheastAsian             3
&vellip; </pre></DD>
A special case of a discrete random variable is a <em>binary random variable</em>.  This is the case when $M=2$ and $y^1=0$ and $y^2=1$. Using 0 and 1 is arbitrary.  The values could be -25.2 and 63.999.  However, using 0 and 1 for a binary concept is much convenient. </p>

A (pure) continuous random variable takes on no particular value with positive probability. When using computers to store outcomes it is always discrete, but that does not mean we cannot model the variable as continuous.  Similarly we may for some reasons discretize a continuous variable and treat it as a discrete (for example measure age in decades rather than milliseconds).</p>

What is important for a model is that it does not imply any actual observation is impossible <em>ex ante</em>.  The fact that the model says some impossible observations are possible is not itself a problem.  For example, we might model attendance at a soccer match as a continuous random variable even thought it is impossible to observe a crowd of 3235.666292 people.  True, there won't be any observations like that in the data, but that's okay.  There are some many different attendance levels that it is a pain to force only integer values to be possible <em>ex ante</em>. On the other hand a model of match outcomes that used a binary random variable would be a serious issue because.  There will be three different observations in the data (one or the other team wins or it is a draw), but a binary random variable only allows two values.  And those three different observed results really are different.</p>

A random variable can be a combination of continuous and discrete (see censored regression example below).  We won't bother giving this a name right now, but we will see examples later on.

Consider the concepts we have now to describe stopping a person on the street:
  <DD>the sample space = anyone who might walk by)</DD>
  <DD>probability = the chance each person walks by</DD>
  <DD>random variables = columns on paper or a spreadsheet to record aspects of the person we stop, such as Age, Height, Income, including making up codes to record qualitative information (such as 0=Male,1=Female).</DD>

Next we combine the probability concept with the random variable concept, which leads to <em>distribution</em>. In fact, to a large extent the abstract idea of sample space, events and probability are really there to explain where distributions of things we can measure come from.


<a name="Fig2"></a>
<div class="alg"><h4>Definition 2.  Cumulative Distribution Function (cdf) </h4>
The cumulative distribution function (cdf) $F(y)=Prob(Y\le y)$.} of a random variable $Y$ is the probability that $Y$ is below a given value. We often use $F(y)$ to denote the cdf.  We relate the cdf to probability as:
$$F(y) \equiv Prob( Y\le y).$$
We write $Y\quad \sim\quad F(y)$ to mean $Y$ is a random variable with cdf $F(y)$.
</div>

the cdf is just a way to summarize how the probability of events determines the pattern of a thing we can measure from an experiment.  There are other ways to summarize this information.  One reason <em>not</em> to start with the cdf is that it is not really the way most people's brain would visualize uncertain outcomes.  The reason to start with the cdf is that the definition is the same for discrete and continuous random variables.   However, because of those technical issues we are trying to avoid, we have to make a slight distinction between kinds of random variables when talking about the more intuitive notion of probability or density function.</p>

<a name="Fig3"></a>
<div class="alg"><h4>Definition 3.  Probability Density Function (pdf) </h4>
<UL>
<li>The pdf of a <u>discrete</u> random variable $Y$ is the probability $Y$ takes on the argument of the pdf.  We often use $f(y)$ for the pdf.  The pdf is defined for <u>any number $y$</u> as:
        $$f(y)\ \equiv\ Prob(Y=y).$$</li>
<li>The pdf of a <u>continuous random variable</u> $Y$ is <u>not</u> a probability. It is the derivative of the cdf:
    $$f(y) \equiv {d Prob(Y\le y)\over dy} = F^\prime(y).$$</li>
</UL></div>

Starting with the pdf you can derive the cdf from it.  For Discrete:
$$F(y) = \sum_{k: y^k\le y} f(y^k).$$
For Continuous:
$$F(y) = \int_{-\infty}^y f(z)dz.$$

The cdf and/or pdf summarize what is knowable about $Y$ before its value is realized.  Usually cdf's are denoted with capital letters (Greek or Roman), but not always $F()$; sometimes $G$ or $H$, etc.  If we write $F(0)=0.5$ we can't tell which random variable is below 0 with probability 0.5.  So we might write $F_Y(0)$ to make it clearer it is $Y$. We can also use different letters for the cdfs of different random variables.  The cdf of $X$ might be  called F(x), the cdf of $Y$  $G(y)$, etc.</p>

<h5>Joint and Marginal Distributions</h5>

Let $A$ and $B$ be two events. $P(A,B)$ is the probability of A <u>and</u> B jointly occurring in the same random experiment:
$$P(A,B) \equiv P( A \cap B).$$
The definition says that "joint" is like intersecting two sets, because the joint probability of two events is the probability that both happen in the same random experiment.  If in the abstract view events are sets then joint means the outcome is in both events, hence intersection is called for.</p>

Now suppose $Y_1$ and $Y_2$ are two random variables defined on a sample space with probability $P()$.
<span>The <dfn id="jcdf">the joint cumulative distribution function (joint cdf)</dfn> of two or more random variables is the bivariate function that gives the probability that both are <em>below</em> certain values</span>:
$$\hbox{for all } y_1, y_2,\quad F(y_1,y_2) \equiv Prob( Y_1\le y_1, Y_2\le y_2).$$
The joint pdf for discrete random variables is the bivariate function giving the probability the random variables <em>equal</em> certain values:
$$\hbox{for all }y_1, y_2,\quad f(y_1,y_2) \equiv Prob( Y_1= y_1, Y_2= y_2).$$</LI>
The joint pdf for continuous random variables is the bivariate function giving the rate of change in the cdf:
$$\hbox{for all }y_1, y_2,\quad f(y_1,y_2) \equiv {\partial^2 Prob( Y_1= y_1, Y_2= y_2)\over \partial y_1\partial y_2} = {\partial^2 F(y_1,y_2) \over \partial y_1\partial y_2}.$$

Let $Y_1$ and $Y_2$ be two discrete random variables with joint cdf and pdf $F(y_1,y_2)$ and $f(y_1,y_2)$.  $Y_2$. If $Y_2$ is discrete and takes on $M$ different values, denoted $y^1_2,\dots,y^{M}_2$.  Once we have more than one random variable defined it can be come necessary to talk about marginal distributions.  In general, <span>the <dfn id="marginal-distribution">marginal distribution of a random variable</dfn> derives its distribution from the joint distribution of with other random variables</span>.

Then the marginal cdf of $Y_1$ is $F_1(y_1) = Prob(Y_1\le y_1) = F(y_1,y_2^{M})$. The marginal pdf of $Y_1$ is $f_1(y_1) = Prob(Y_1 = y_1) = \sum_{k=1}^{M}\ f(y_1,y^k_2)$}. If $Y_2$ is continuous then the marginal cdf of $Y_1$ is $F_1(y_1) = Prob(Y_1\le y_1) = F(y_1,\infty)$}. The marginal pdf of $Y_1$ is $f_1(y_1) = {d Prob(Y_1 = y_1)\over dy_1 }= \int_{-\infty}^{+\infty}\ f(y_1,y_2)dy_2$.</p>

To get the marginal for $Y_1$ we fix its value as $y_1$ and add or integrate across all possible values of $Y_2$. The marginal functions for $Y_2$ are defined by switching 1 and 2 in the definitions according to whether $Y_1$ is discrete or continuous. Because the cdf is defined for all real numbers we could insert $y_2=\infty$ for the discrete case as well instead of $y_2^M$, because $F(y_1,y_2) = F(y_1,y_2^M)$ for all $y_2>y_2^M$. Think of the marginal as the distribution of the random variable without knowing anything about the other random variable. That is, the marginal cdf/pdf is the same function as the cdf/pdf of the random variable defined earlier but derived from the joint distribution.</p>

<a name="Fig4"></a>
<div class="alg"><h4>Definition 4.  Independence </h4>
Let $Y_1$ and $Y_2$ be two random variables with joint cdf $F(y_1,y_2)$ and joint density $f(y_1,y_2)$. We say that $Y_1$ and $Y_2$ are <em>statistically independent</em> if:
$$\hbox{for all } y_1 \hbox{and}  y_2,\quad f(y_1,y_2) = f_1(y_1) f_2(y_2).$$
More generally, $Y_1,Y_2,\dots,Y_N$ are independent if their joint density can be written
$$f(y_1,y_2,\dots,y_N)\quad =\quad f_1(y_1)f_2(y_2)\cdots f_N(y_N) \quad =\quad \prod_{i=1}^N f_i(y_i).$$
</div>

That is, random variables are independent if (and only if)</p>their <u>joint pdf</u> <em>factors</em> into the <em>product</em> of the individual <u>marginal pdfs</u>. The cdf $F(\cdots)$ can also be used instead of the pdf to define independence (either one implies the other).


<h3><a name="s011"><LI>IID Sampling</LI></a></h3>

Two random variables are <em>identical</em> when their distributions are the same function. So $Y_1 \sim F_1(y)$ and $Y_2 \sim F_2(y)$ are identical if $$F_1(y) = F_2(y)$$
for all values of $y$.  If $Y_1$ and $Y_2$ are identical and defined on the sample space that does not mean their realized values are the same at each outcome. </p>

For example, let $Y_1$ and $Y_2$ be the height of first and second children in a family, respectively, as of age 10. Why age 10?If we measured heights at the same time the children would be different ages and we would <em>not</em> expect their distributions to be the same.  So we are thinking of height at the same age. <em>Within</em> a given household the first child and the second child are not going to be the same height as each other.  So the values of $Y_1$ and $Y_2$ for a single outcome (the household) are going to be different, usually. But <em>across</em> households the distribution of their heights would be the same, so $Y_1$ and $Y_2$ are identical random variables.  (If social or biological reasons would lead second children to be taller than first children or vice versa across the population then they would not be identical.) </p>

Notice that being identical random variables also does not say anything about the <em>joint</em> distribution of the random variables. In the example, families tend to be taller or shorter than average, so we would expect $Y_1$ and $Y_2$ to be positively coordinated. When we combine identical and independent we get the basic structure for our data.</p>

When random variables are independent their joint distribution factors into the product of their marginal distributions.  When those random variables are also identical, the distribution function is the same for each.  We write $Y_i \sim F(y)$, $i=1,\dots,N$ to denote $N$ random variables that are identically and independently distributed.</p>

Independence means for the joint CDF:
$$F(y_1,y_2,\dots,y_N) = \prod_{i=1}^N F_i(y_i).\qquad\hbox{Independent}$$
Identical means the different marginal functions are the same function, which we denote without a subscript:
$$F(y_1,y_2,\dots,y_N) = \prod_{i=1}^N F(y_i).\qquad\hbox{IIDcdf}$$
IID means the same relationship holds for the pdf:
$$f(y_1,y_2,\dots,y_N) = \prod_{i=1}^N f(y_i).\qquad\hbox{IIDpdf}$$
Note that $F$ is used two different ways.In the definitions $F$ without a subscript is used two different ways.  When it has a single argument, as in $F(y_i)$, it is the shared marginal CDF of the random variables.  When it has $N$ arguments it is the joint CDF.</p>

When we try to understand data from individuals, households, firms, countries, etc., we will treat them as IID outcomes.  So our data set will be considered an IID sample. This sounds like we will think of all the observations as the same, but you will see that the part of the model that is IID will <em>not</em> be the observed data itself.  Differences in the data will be accounted for (explained) by <em>observed</em> variables that move the (conditional) distribution around.  But the <em>unexplained</em> difference will be IID.</p>

We will work with the <em>natural logarithm</em> of the sampling pdf. Since both the sample pdf and sample cdf are <em>products</em> their logs are <em>sums</em>:
$$\ln f(y_1,y_2,\dots,y_N) = \ln\biggl(\prod_{i=1}^N f(y_i)\biggr) = \sum_{i=1}^N \ln f(y_i).\qquad\hbox{lnIIDpdf}$$
The logarithms are well-defined as long as none of the terms are $0$.
One reason for working with log probabilities is the following.
If $Y$ is discrete then $f(y_i)$ is between 0 and 1.  The sample pdf multiplies $N$ of these terms together. If $N$ is large then the sample pdf is a very small positive numbers.  Digital computers do not store real numbers exactly, and there is a limit to how small a positive numbers can be before it is simply 0 on the computer.  </p>

For example, suppose $N=3$ and we have a sample probability $0.01\times0.02\times0.02 = 0.0004$.  Suppose your calculator was very bad and the smallest positive number it could store was 0.001.  Then the sample probability would be stored as exactly $0$.   But a zero probability is very different from a small but positive probability. Of course, computers have much more precision than that.  But if some of probabilities are small and/or $N$ is big (like 10,000) then precision becomes a very important decision. Working with logs avoids this problem, since ln(0.01) x ln(0.02) x ln (0.02)  is approx -4.605 -3.912 -3.912 . Notice that although the numerical values still use only 3 digits below zero the result is far from ln(0).</p>

<h5>Example: IID versus Dependent Sampling.</h5>
Let $Y_1$ and $Y_2$ be random variables that code the outcome of the first and second toss, respectively. Consider two ways to toss a coin twice.</p>
First: Toss once and then toss again. Then all possible observed samples:
<DD><pre>
Obs.   Y1    Y2   Prob(y1,y2)
---------------------------------
 HH     1     1       1/4
 HT     1     0       1/4
 TH     0     1       1/4
 TT     0     0       1/4
</pre></DD>
Second: Toss once.  Then flip the coin over (do not re-toss). All possible observed samples:
<DD><pre>
Obs.    Y1   Y2   Prob(y1,y2)
-----------------------------
 HH     1    1         0
 HT     1    0        1/2
 TH     0    1        1/2
 TT     0    0         0
</pre></DD>
The first case is IID sampling. The second case is identical but <em>not</em> independent sampling. It is identical, because in both cases Prob$(Y_1=1) = $Prob$(Y_2=1) = {1\over 2}$. So the chance of four tosses of a fair coin being "heads" equals $(1/2)^4$ only when the sampling mechanism is independent.</p>
<h3><a name="s012"><LI>Population Parameters</LI></a></h3>

A <span><dfn id="probmodel">probability model</dfn> is a mathematical representation of a random phenomenon [Yale Stats Course]</span>. <!--keep HR-->
A probability is designed to learn about the population from which data is derived. The idea of a model is to assume (or to maintain a hypothesis) some aspects of the population are known while leaving other aspects as open to be determined by the data. In Econ 452 we consider <span><dfn id="parammodel">parametric probability models</dfn>, which means the unknown and learnable aspects of the population is limited to parameters to be estimated from the data.</span> There are also semi-parametric and non-parametric models which are designed to let the data speak for themselves more than parametric models.  There are tradeoffs between these types of models but this is not a question we will be concerned with here.</p>

A <span><dfn id="parameter">population parameter</dfn> or just a <em>parameter</em> is a quantity in a probability model that helps determine the probability of events.</span>  By influencing probability, a parameter determines the distribution of random variables related to the random experiment. A parameter is the <u>true</u> value that underlies the random experiment. We also say true parameters "generate the data."  Usually the true parameters are considered <em>unknown</em>, but sometimes we treat a parameter as <em>known</em> or  <em>fixed</em>. Most probability models have more than one parameter.  So sometimes they are grouped together in a vector. </p>

We will use $\theta$ to represent the generic vector of population parameters in a model. What $\theta$ means, what role it plays in the model, depends on the model being discussed. For example, when we write $Y\ \sim\ F(y\ ;\ \theta)$ it means that the cdf of the random variable $Y$ depends on a parameter or vector of parameters $\theta$. The ";" separates the ordinary argument $y$ from the parameter vector. It indicates that the function depends on a single value of $\theta$.</p>

Probability satisfies <a href="probability.html#rules">rules</a>; so parameters of a probability model have to take on values that ensure the probability model follows the rules. The <em>range of values</em> of a parameter for which the model satisfies the rules is called the <em>feasible range</em> or the <em>set of feasible values</em>. Since most models have more than one parameter, the range of feasible values can be a range in multiple dimensions. </p>

The <span><dfn id="paramspace">Parameter Space, $\Theta$</dfn> is the feasible ranges of all parameters in the probability model.</span>

An <span><dfn id="estimator">estimator</dfn> of a parameter, which we can generically call $\hat\theta$, is a formula or an expression that depends on data.</span> When applied to an actual data set it results in an <em>estimate</em>. You can think of an estimator is a plan to do something with the data to learn about a population parameter before looking at the data. When we plug in the data into the formula the estimator becomes an estimate. An estimate is a numerical value (or numerical vector).  Typically it depends on the data we have, but we could just guess the value of parameter without looking at the data.  It can be confusing because we use $\hat\theta$ to represent the formula and sometimes the value generated from it for the data we have.  A parameter is part of the process that generates data (through the model we are using).  An estimate of a parameter,  does not generate data but is generated <u>from</u> data. </p>

Sensible guesses for parameter values should be in the parameter space $\Theta$. This is because that is the set of values that mean the probability model satisfies probability rules. So we can think of $\Theta$ as the range of values we would want our estimator to produce values.  As econometricians we chose the formula (estimator) to apply to data, so we can think of $\Theta$ as a constraint on our choices, just like a budget constrains a consumer's choices.</p>
<h3><a name="s013"><LI>The Bernoulli Model for Binary Outcomes</LI></a></h3>

Now we introduce the simplest versions of the two models that form the basis of the methods you are most likely to use in the final project. One model is for binary random variables which we just reviewed and is called the Bernoulli model You already dealt with binary outcomes in Econ 250 (remember the binomial distribution?).  So really the Bernoulli model is a different way to deal with data that the binomial  distribution did. We switch to the Bernoulli model because it sets the stage for the <em>probit model</em> that comes later. The binomial distribution does not provide such a natural lead in.</p>

This Bernoulli model is <em>univariate</em>. That is, there is only one variable in the model. We do this to fix ideas, but econometrics is really concerned with multivariate models, to learn about the relationship between variables.  So we add one and then many more variables to these models.</p>

Our focus is to illustrate the key elements of a probability model, namely an assumption about the sampling and distribution of data and the role of population parameters. The goal of applying a probability model to data is to learn about the values of the parameters by estimating them.  We derive the sampling distribution for each model we consider as a function of its parameters. This sets up the new concept of <em> maximum likelihood estimation</em> of parameters.

<a name="Fig5"></a>
<div class="alg">
<h4>Definition 5.  The Bernoulli Model </h4>
<DT>We write $Y\sim Bernoulli(\theta)$ to mean</DT>
<OL class="compact">
<li>$Y$ is a binary random variable.</li>
<li>$Prob(Y=1) = \theta$ and $Prob(Y=0)=1-\theta$, where $\theta$ is a population parameter.</li>
<li>The parameter space is $\theta \in \Theta = [0,1]$.  That is, the Bernoulli parameter satisfies: $0\le \theta \le 1$.</li>
<LI>The <b>cdf</b> and <b>pdf</b> of $Y$ are:
$$F_{B}(y) = \cases{ 0          &if $y<0$\cr\cr
                     1-\theta   &if $0\le y \lt 1$\cr\cr
                     1          &if $1 \le y$.\cr}$$
$$f_{B}(y) = \cases{ 1-\theta   & if $y=0$\cr\cr
                     \theta     & if$y=1$\cr\cr
                     0          &otherwise.\cr}$$</LI>
</OL>
</div>

<DT>Examples:</DT>
<DD> a "fair coin" is a Bernoulli random variable with known $\theta=0.5$.</DD>
<DD> an "unfair coin" is a Bernoulli random variable with a parameter $\theta \ne 0.5$.</DD>

<a name="Fig1"></a>
<figure><h4>Exhibit 1.  Bernoulli Distribution </h4><img  width="80%" src="img/bdist.png" alt=""/></figure>

Now let $Y_{i=1,\dots,N}\ \ {\buildrel  iid \over \sim}\ \ Bernoulli(\theta)$. We want to know what the probability of observing any particular sample of values is given the value of $\theta$. We derive the distribution for any sample size by first handling the cases of one or two observations.</p>

<DT>Let $N=1$.</DT>

Then the sample is simply $Y_1$. The data point is either $1$ with probability $\theta$, or $0$ with probability $1-\theta$. A <em>convenient</em> way to combine both cases into a single formula:
$$f(Y_1\ ;\ \theta) = \theta^{Y_1} (1-\theta)^{1-Y_1}.$$

<DT>Let $N=2$.</DT>
Since sampling is IID, the joint probability is the product of the marginals for $Y_1$ and $Y_2$:
$$f(Y_1,Y_2\ ;\ \theta) = \biggl[\theta^{Y_1} (1-\theta)^{1-Y_1} \biggr] \ast \biggl[\theta^{Y_2} (1-\theta)^{1-Y_2} \biggr].$$
Next, group by $\theta$ and $1-\theta$:
$$ = \bigl[\theta^{Y_1+Y_2}\bigr]\bigl[(1-\theta)^{1-Y_1+1-Y_2}\bigr]\quad
             = \quad \bigl[\theta^{Y_1+Y_2}\bigr]\bigl[(1-\theta)^{2-(Y_1+Y_2)}\bigr]$$
Define $S\equiv Y_1+Y_2$ to simplify even further:
$$f\left(Y_1,Y_2\ ;\ \theta\right) = \theta^{S}(1-\theta)^{2-S}  = \theta^{S}(1-\theta)^{N-S}$$

<DT>For $N>2$:</DT>
Use  $x^a x^b = x^{a+b}$ not just once but $N-1$ times. Again define $S = Y_1+Y_2+\dots+Y_N$.  It shows up and so does $N$. The probability of any observed sample of $N$ iid Bernoulli random variables can be written two ways.  <em>Either</em> in terms of the observed outcomes <em>or</em> with $S$ defined as the sum of the observed outcomes. $S$ is called a <em>sufficient statistic</em> because it is sufficient information from the sample to compute $f_B$. You may recall the "binomial" distribution: the distribution of the number of successes in $N$ trials (distribution of $S$ given $N$).  $f_B$ is one part of that distribution.  Since many combinations of $Y_i$ result in the same sum $S$, the binomial probability density includes a combinatoric factor (number of different combinations of $Y$'s with the same $S$).</p>

<a name="Fig6"></a>
<div class="alg">
<h4>Definition 6.  Bernoulli IID Sampling Distribution </h4>
<DT>$Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\ Bernoulli(\theta)$ means $Y_i$ is a  sample of IID Bernoulli random variables.</DT>
The joint probability of the IID sample is:
$$f_B\bigl(Y_{i=1,\dots,N}\ ;\ \theta\bigr)\ =\ \theta^{Y_1+\dots+Y_N} \left(1-\theta\right)^{N-(Y_1+\dots+Y_N)}.$$
Letting $S \equiv \sum_{i=1}^N Y_i$ we can also write this as
$$f_B\bigl(Y_{i=1,\dots,N}\ ;\ \theta\bigr) =\theta^S \left(1-\theta\right)^{N-S}$$.
Since $\theta$ and $1-\theta$ are both between 0 and 1, $f_B$ gets very close to 0 for large $N$.  Therefore, it is often more convenient to work with <u>log probability</u>:
$$\ln f_B(Y_{i=1,\dots,N}\,;\,\theta) = S\ln\theta + (N-S)\ln(1-\theta).$$
</div>

<h3><a name="s014"><LI>The Normal &amp; Related Distributions</LI></a></h3>

<a name="Fig7"></a>
<div class="alg"><h4>Definition 7.  The Standard Normal Distribution </h4>
We write $Z\sim {\cal N}(0,1)$ to mean:
<OL class="compact">
<LI>$Z$ is a continuous random variable that takes on values between $(-\infty,+\infty)$.</LI>
<LI>The pdf of a Z variable is usually denoted $phi$ and equals
$$\phi(z) = {1\over\sqrt{2\pi}}e^{-z^2/2}.$$</LI>
<LI>The CDF of a $Z$ random variable is denoted $\Phi$ and equals
$$\Phi(z)= \int_{-\infty}^z \phi(v)dv.$$
This integral <u>has no closed form expression</u>. This is the reason stats books contain tables of normal values.</LI>
</OL>
</div>

Usually plain N is used for normal, but these notes use $\cal N$ to distinguish it from our symbol for the sample size, $N$.  We define expectation and variance later, but the expectation of Z is 0 and the standard deviation is 1.

<a name="Fig8"></a>
<div class="alg"><h4>Definition 8.  General Normal Distribution </h4>
We write $X\sim {\cal N}(\mu,\sigma^2)$ to mean
<OL class="compact">
<LI>$X$ is a continuous random variable that takes on values between $(-\infty,+\infty)$.</LI>
<LI>It has pdf of the form
$$f(x) = {1\over\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/(2\sigma^2)}.$$
We do not use a special symbol for the pdf/cdf of a general normal random variable, but we reserve
$phi(z)$ and $Phi(z)$ for the special case ${\cal N}(0,1)$.</LI>
<LI>As with the standard normal, the cdf of a normal random variable is an integral with no closed form.</LI>
</OL>
</div>

Some key facts about the normal distribution.
<UL>
<LI>The <b>fundamental</b> fact to know about the normal distribution is this: <b>a linear combination of normal random variables is also normally distributed.</b> This is <em>not</em> true in general: the sum of a non-normal random variables will follow the same distribution as what is
    being summed up.</LI>
<LI>One special case of this is that the sample mean of an IID sample of normal random variables will itself be normally distributed
    (since the sample mean is a linear combination of the observations.) </LI>
<LI>(A) Central Limit Theorem: the sample mean of (almost any kind of) IID random variables will follow a normal distribution <em>asymptotically</em> (as the sample size goes to infinity. In other words, what holds for the sample mean of any sized sample of IID normal variables (the fact above) will be true in a large enough sample <em>regardless</em> of the distribution being sampled.</LI>
</UL>


<a name="Fig2"></a>
<figure><h4>Exhibit 2.  Bell Curves </h4>
<img  width="80%" src="img/zdist.png" alt="The Standard Normal Density and Distribution (THE Bell Curve)"/>
<figcaption>The Standard Normal Density and Distribution (THE Bell Curve)</figcaption>
<img  width="80%" src="img/ndist3.png" alt="Three Different Normal Densities" />
<figcaption>Three Different Normal Densities</figcaption></figure>

<a name="Fig9"></a>
<div class="alg"><h4>Definition 9.  Three Functions of Normal Random Variables  </h4>
<OL class="steps">
<LI>A <q>chi-squared</q> random variable, denoted $\chi_k$, takes on values between $[0,+\infty)$. It depends on a positive integer $k$, called its <em>degrees of freedom.</em>. We write $X \sim \chi^2_k$ to mean $X$ follows the same distribution as
    $$\chi^2_k = \sum_{i=1}^k Z_i^2,$$
    where $Z_{i=1,\dots,k} \ {\buildrel  iid \over \sim}\  {\cal N}(0,1)$. A chi-squared follows the same distribution as the sum of $k$ IID standard normal random variables squared  </LI>
<LI>A <q>t</q> random variable, denoted $t_k$, is the distribution followed by
    $$ t_k = {Z \over \sqrt{X / k}},$$
    where $Z\sim {\cal N}(0,1)$,  $X \sim \chi^2_{k}$, and $Z$ and $X$ are <b>independent</b>.
</LI>

<LI>A $F_{n_1,n_2}$ random variable follows the distribution
        $$ F_{n_1,n_2} = {X_1/n_1 \over X_2/n_2},$$
        where $X_1 \sim \chi^2_{n_1}$, $X_2 \sim \chi^2_{n_2}$, and $X_1$ and $X_2$ are <b>independent</b>. That is, the F distribution is what happens when you take the ratio of two independent chi-squared random variables divided by their degrees of freedom.</LI>
 </LI></OL>
 </div>

<DT>Notes</DT>
    <DD>As $k\to\infty$, $t_k$ becomes ${\cal N}(0,1)$. That is, $t_k \ {\buildrel D \over \to}\  {\cal N}(0,1)$.  So the standard normal distribution is the limit of the t distribution as the degrees of
    freedom go to infinity.</DD>
    <DD>$F_{1,k} = \bigl( t_k \bigr)^2$.  That is, if you square a $_k$ random variable its distribution is the same as $F_{1,k}$.  So any test     that can be conducted with a t-statistic could be conducted with a F-test with identical results.  However, the F distribution generalizes the t distribution so it can be applied in cases where the t statistics is not appropriate.</DD>


<a name="Fig3"></a>
<figure><h4>Exhibit 3.  Some Chi-Squared Distributions </h4>
<img  width="80%" src="img/chidist.png" alt="The Chi-Squared Distribution"/></figure>
<h3><a name="s015"><LI>The One Variable Normal Linear Regression Model</LI></a></h3>

Our second model is designed for data that varies continuously, so for that reason we use a continuous random variable. Although we will discuss it differently this is nearly the same as one of the standard elements of a probability course.  In Econ 250 you discussed using the sample mean to estimate the population mean and the sample variance to estimate the population variance.   The procedures you developed will carry over here almost but not quite exactly.  However, here we begin with a model of the population and only later consider how this relates to the sample mean and variance.</p>

Further, this is a special case of the <em>linear regression model</em> that you studied in Econ 351. Econ 351 introduces regression with two variables.  So it is not completely standard to talk about a one-variable regression. One advantage of doing this is we can put off discussion of conditional probabilities. </p>

We also differ a bit from Econ 250 and 351 by <em>assuming normality</em> at the start. In previous classes assuming the data follows the normal distribution is added on after the basic idea is developed. Here we simply assume it from the start.  This makes it easier to introduce likelihood and maximum likelihood estimation.</p>


<h4>One Variable Normal Linear Regression</h4>
<DT>We write $Y_{i=1,\dots,N} \quad {\buildrel  iid \over \sim}\quad {\cal N}(\beta,\sigma^2)$ to mean</DT>
<OL class="compact">
<LI>$Y_i\ =\ \beta + \sigma z_i, \quad \hbox{and}\quad z_{i=1,\dots,N}\ {\buildrel  iid \over \sim}\  {\cal N}(0,1).\qquad(*)$</LI>
<LI>The parameter vector has two elements: $\theta \equiv \left({\beta\atop\sigma}\right)$.</LI>
<LI>The parameter space is 2-dimensional: $\Theta \equiv (-\infty,\infty) \times [0,\infty)$.</LI>
</OL>

Notes and Connections to Material You Have Already Seen
<DD>The <a href="">general normal distribution</a>, ${\cal N}(\mu,\sigma^2)$ is a random variable which comes from multiplying $Z$ by a standard deviation $\sigma$ and adding a mean $\mu$. </DD>
<DD>We use $\beta$ in nLRM instead of $\mu$ so that it looks just like the multivariate regression model that comes later.  But $\beta$ is still the population mean of $Y$.  </DD>
<DD>So really we are just talking about estimating the parameters of the normal distribution from data on a variable <var>Y</var>.  You might prefer not to add more abstract notation on top of other notation, but the reason I do this is to develop a <em>pattern</em> for describing models so that more complicated models are see as extensions of simpler ones.  The notation used in Econ 250 and 351 is designed to be simple but that makes it harder to extend.</DD>
<DD>As you see, we sometimes list the individual parameters $(\beta,\sigma)$ and sometimes use $\theta$ to represent the pair of them.  </DD>

<DT>Names for Things</DT>
<DD>We call $\beta$
<UL><LI>the "constant term." </LI>
<LI>Also the "intercept" in multivariate regression.  </LI></UL></DD>
<DD>$\sigma z_i$ has many names: <UL>
<LI>"the unobserved term" because by assumption it does not appear in the data.  </LI>
<LI>also "the (population) residual"</LI>
<LI>"the shock"</LI>
<LI>"the disturbance term."</LI>
</LI></UL></DD>

<h4>Two Equivalent Formulations of nLRM1:</h4>
<OL class="compact">
<LI>We could also write (*) as
$$Y_i = \beta\times 1 + \sigma\times z_i.$$
<DD>Now $\beta$ is the coefficient on a constant across $i$ and $\sigma$ is the coefficient on a varying but unobserved value $z_i$, which is standardized to have variance=1. </DD>
<DD>This makes $\sigma$ and $\beta$ more like each other.  They are both coefficients on something</DD>
</LI>
<LI>We can also rewrite (*) slightly by defining a new random variable:
$$Y_i = \beta + \epsilon_{i},\quad \hbox{and}\quad \epsilon_{i=1,\dots} \ {\buildrel  iid \over \sim}\  {\cal N}(0,\sigma^2).\qquad{(**)}$$
<DD>This is the more standard way of specifying a regression model.  </DD>
<DD>For our purposes (*) is better because it incorporates $\sigma$ into the equation along with $\beta$.</DD>
</LI>
</OL>

<h4>nLRM Sampling: Preliminary</h4>

To derive the IID sampling pdf for the regression model, first we need to relate the density of a single observation, $y_i$, to the density of $z_i$, which we have assumed is the normal pdf defined above. First, consider the underlying random variables $Z \sim N(0,1)$ and $Y= \beta + \sigma Z$.  The probability that $Y$ is less than some value $d$ can be expressed in terms of the probability of $Z$:
$$F_Y(x) = Prob(Y \le x) = Prob\left(\beta+\sigma Z \le x\right) = Prob \left(Z \le {x-\beta\over\sigma}\right) = \Phi\left( {x-\beta\over \sigma} \right).$$
The CDF of $Y$ at any number $x$ equals the CDF of $Z$ at some over value that depends on $x$ and the parameters of $Y$.   The last expression uses the <em>inverse relationship between $Z$ and $Y$</em>.  </p>

As usual the pdf is the first derivative of the CDF:
$$f_Y(x) = F^\prime_Y(x) = {d \Phi\left({x-\beta\over\sigma}\right) \over dx} \phi\biggl( {x-\beta\over\sigma} \biggr)/\sigma.$$
The pdf still has the inverse relationship but it is also scaled by $1/\sigma$.  This means the density of $Y$ is either compressed or stretched out relative to the density of $Z$. This extra term in the pdf of the transformed random variable is called the <em>inverse of the Jacobian</em>, but this is the only place where we need to use it, so we will not need to keep that more general notion in mind.</p>

Now express the pdf of $Y$ using the standard normal density
$$f_Y(x) = {1\over \sqrt{2\pi}\sigma}e^{ -{1\over 2\sigma^2}{\left(x-\beta\right)^2}}$$
This is the pdf of the <em>general normal distribution</em>.  That is, we could write $Y \sim {\cal N}(\beta,\sigma^2)$</p>

<h4>IID nLRM Sampling Density and the Log Density</h4>
<DT>Let $Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\  {\cal N}(\beta,\sigma^2) $.  </DT>
<DD>Using the definition for the <a href="NormalDistribution.html#X">normal density</a> we have:</DD>
<OL class="steps">
<LI>The pdf of the sample is
$$f_{{\cal N}}\left(Y_{i=1,\dots,N}\ ;\ \beta,\sigma\right) \equiv \prod_{i=1}^N \quad{1\over \sqrt{2\pi}\sigma} e^{-{1\over 2}\left(Y_i-\beta\over \sigma\right)^2} = \left({1\over \sqrt{2\pi}\sigma}\right)^N \exp\biggl\{ {-1\over 2\sigma^2}\sum_{i=1}^N \left(Y_i-\beta\right)^2\biggr\}$$</LI>
<LI>Taking the natural logarithm results in a simpler log-probability
$$\ln f_{{\cal N}}\left(Y_{i=1,\dots,N}\ ;\ \beta,\sigma\right)  =  -{1\over 2}\left(N\ln2\pi + 2N\ln\sigma +{1\over \sigma^2}\sum_{i=1}^N \left(Y_i-\beta\right)^2
\right)$$</LI>
</OL>
<h3><a name="s016"><LI>Hypothesis Tests &amp; Confidence Intervals</LI></a></h3>

<blockquote class="quote"><em>In relation to any experiment we may speak of this hypothesis as the <q>null hypothesis,</q> and it should be noted that the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation. <br>Every experiment may be said to exist only in order to give the facts a chance of disproving the null hypothesis.</br></em> </p><b>R. A. Fisher, The Design of Experiments</b></blockquote>

We review the procedure taught in Econ 250 and Econ 351 to formally test theories about the data. This is one example of statistical <em>inference</em>. Another very closely related inference is the construction of <em>confidence intervals</em> for parameters which we place less emphasis on in this course.</p>

Fisher's formulation quoted above remains the formal statement of statistical tests.  For example, his statement that the facts can only disprove not prove a statistical hypothesis is embodied in the language we adopt.  We speak of "reject the null" and "fail to reject the null" but never "accept the null."
The philosophy-inclined student is encouraged to explore the philosophy of science at it developed over the twentieth century.  Fisher's approach matches the approach of Karl Popper that science is fundamentally a process of theory <em>falsification</em>.  Science advances by proposing theories that can be falsified by observation.  This view was challenged later by Thomas Kuhn and his notion of scientific <em>paradigms</em> and then further challenged by <em>relativistic</em> views of truth and science.  Within economics students can find similar progression of ideas by people such as Milton Friedman, Lionel Robbins and Dierdre McCloskey.</p>

<a name="Fig10"></a>
<div class="alg"><h4>Definition 10.  Required Elements of a Statistical Test </h4>
<OL class="steps">
<li>A Probability Model.  We focus on parametric models: ones with a parameter $\theta$ that lies in a parameter space $\Theta$.</li>
<LI>Null Hypothesis ($H_0$): a theory that says $\theta \in {\cal R} \subset \Theta$.</LI>
<LI>Alternative Hypothesis ($H_A$): A subset of $\Theta$ that does not intersect ${\cal R}$.  In can be every possible value outside of ${\cal R}$ (as in "two-sided" tests).</LI>
<LI>A <b>feasible</b> Test Statistic: a formula that uses sample data.  It is feasible if its distribution is known under $H_0$ (if $\theta$ actually is in  ${\cal R}$).</LI>
<LI>Level of Significance ($\alpha$): The tester's tolerance for making a type I error. </DD></LI>
<LI>Critical Region: Values based on $H_0$, $H_A$, the test statistic, and $\alpha$ chosen to minimize the chance of a Type II error ($\beta$).</LI>
<HR/>
<LI>Value of the Test Statistic. Plug the sample data into the test statistic formula and get number.</LI>
<LI>Decision: Decide either to <em>reject $H_0$ in favour of $H_A$</em> if the test statistic lands in the Critical Region <u>or</u> <em>fail to reject} $H_0$</em>.</LI>
</OL>
</div>

Note:  all of the elements above the line can and should be determined before the data are used to compute the test statistic.  Choosing the null or $\alpha$ after using the sample data is like stealing the answer key before an exam.  Whatever the outcome, it does not reflect what is claimed (in the latter the student's understanding of the material, in the former the strength of support for the null hypothesis in the data)</p>

In a statistical procedure based on parametric probability model, <span>a <dfn id="hypothesis">hypothesis</dfn> is simply a particular value or set of values of population parameters.</span>  <!-- Keep HR -->
We still follow Fisher in calling <span>the hypothesis to test the <dfn title="H0">null hypothesis</dfn> ("the null" or "H nought")</span>. The null may come from a theory or is a special parameter value for some reason.  Often theories are consistent with parameters within a <b>range</b> not just single values. This value is then compared to the range of values <b>not</b> consistent with the theory. But some tests we discuss are able to handle a null hypothesis that consists of any subset of possible values. In other cases the null can be set to the edge of the range.</p>

The other parameter values being tested against (often labeled $H_A$)  is called "the alternative".  Specifying the alternative being tested against is important to carry out the test properly. It is presumed that students have carried out the procedure of hypothesis testing based $Z$-tests and $t-$tests.  We first discuss some example statistical hypotheses within our two simple models without any reference to a particular test.  </p>

In testing $H_0$ we can make two kinds of mistakes (famously called Type I and Type II errors).  There must be some carefully crafted (maybe even misleading) concepts to boil down all mistakes into two types.  And there are indeed some strong suppositions to make other kinds of mistakes not possible.   Statistics is not the only place this approach occurs.  Trials require the judge or jury to either convict or acquit as charged.  They are not allowed to convict of charges not made (except charges of a lesser degree).</p>

Type I and II errors first assume that what we are testing makes sense. That is, all statistical tests are based on a model of where the data came from and within that model we test different possibilities.  <em>The model itself is not on trial.</em>  So <span>all the other elements of the statistical procedure (other than whether the parameters satisfy $H_0$ or not) can be considered the <dfn id="maintained">maintained hypothesis</dfn></span>. Then we test $H_0$ given that the model itself is correct.  The person performing the test might be unconvinced that the underlying model is truth, but may see it as the best option in terms of models for the data (and for their understanding of statistics and economics).</p>

Given the model (the maintained hypothesis) and the null (the theory on trial), we can say the world must be in one of two states: $H_0$ is FALSE or the $H_0$ is TRUE.
<DD><pre>
                      DECISION BASED ON ONE SAMPLE
                          Reject    Fail to Reject
                     ------------------------------
            H0 True  |   Type I    |   No Error   |
REALITY              |   Error     |              |
FOR ANY              |             |              |
POSSIBLE             ------------------------------
SAMPLE               |             |              |
            H0 False |  No Error   |   Type II    |
                     |             |    Error     |
                     ------------------------------ </pre></DD>
The classical test of significance forces us to make an either/or decision about $H_0$: reject or fail-to-reject.  The test recognizes that the data is not the only possible data that might be observed.  So it won't make sense to say there is a 90% chance that $H_0$ is true.  Chance means probability, which we emphasized earlier is a feature of random events (subsets of the sample space).  $H_0$ being true or false are <b>not</b> events in this sense, so we <u>cannot</u> assign probabilities to them. Instead, we simply decide whether the evidence warrants rejecting the null as true, knowing the decision made be wrong.

<span><dfn id="alpha">Significance</dfn> (or level of significance) is the probability of making a Type I error and often denoted $\alpha$</span>:
<DD>Prob(Type I error)=$\alpha$.</DD>
A test is more significant than another if it is more likely to fail to reject a true hypothesis (thus a smaller $\alpha$).  Conventionally, we define
<DD>Prob(Type II error) = $\beta$.</DD>
<em>(This $\beta$ has nothing to do with the $\beta$ in nLRM)</em>.</p>

<span><dfn id="power">Power</dfn> of a test is defined $1-\beta$, or 1 minus the probability of a Type II error</span>. A test is more powerful than another if it is more likely to reject a false hypothesis (thus $\beta$ smaller). The table emphasizes that errors have probabilities because they relate to what kind of sample we drew, and because the row determines where the true parameter lies.  <em>Ex post</em> we do not know if an error was made or not.   We can only know if an error was made if we know which reality was in effect.  But if we knew that there would be no need to carry out the test. <em>Ex ante</em> we could avoid either type of error completely, but not both.</p>

We could avoid Type I errors completely by never rejecting $H_0$.  This sets $\alpha=0$, and a person who likes this value is perfectly gullible: they have never met a null hypothesis they didn't like.  But in this case we do not need the data to conduct the test because we decided without it.  Unfortunately this is not a perfect strategy because if the null is indeed false we have made the probability of Type II error equal to 1!.  That is, it has zero power to detect false hypotheses ($\beta=1$).

We could avoid Type II errors completely by rejecting any hypothesis no matter what.  This would be the ultimate cynic: any idea someone proposes is wrong. But if the hypothesis happens to be true it ensures a type I error so $\alpha=1$.</p>

Statistical tests are <b>designed</b> to control the <em>ex ante</em> probability of making type I ($\alpha$) and type II ($\beta$) errors.  There is a tradeoff $\alpha$ and $\beta$.   Tests of significance are designed to set $\alpha$ to take a pre-determined value and then make $\beta$ as small as possible.</p>

<span>The <dfn id="critreg">critical region</dfn> of a test is the set of values of the computed test statistic that result in a rejection of $H_0$ in favour of $H_A$</span>.   The set of values should have probability equal to $\alpha$ if $H_0$ is true.  Test statistics are usually designed so that the value of the test statistic is likely to be close to 0 when the data are in closest agreement with the $H_0$.</p>

If $H_0$ is FALSE and $H_A$ is TRUE then we want the critical region to have as much <u>power</u> as possible to detect this. If the test statistic is likely to be close to 0 under the null it means that under $H_A$ the test is likely to be near some other value.  So for most (all?) tests the critical region are values of the test statistic that are either big enough positive or big enough negative <u>or</u> big enough in absolute value.  The critical <em>value</em> of the test statistic is simply the value on that is just big enough.</p>

Putting these two considerations together we see that the critical region is designed so the test statistic lands in it (across possible samples) with probability  $\alpha$ when $H_0$ is TRUE ($\theta \in {\cal R}$). The critical region will be in one or both tails of the distribution depending on the alternative we want the test to have power against (a one-sided or two-sided alternative).

<blockquote class="quote"><em>While introductory texts may introduce degrees of freedom as distribution parameters or through hypothesis testing, it is the underlying geometry that defines degrees of freedom, and is critical to a proper understanding of the concept. Walker (1940)[3] has stated this succinctly: For the person who is unfamiliar with N-dimensional geometry or who knows the contributions to modern sampling theory only from secondhand sources such as textbooks, this concept often seems almost mystical, with no practical meaning.</em>[<a href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)">Wiki</a>]</blockquote>

In a statistical theory degrees of free (DoF) is the <em>rank</em> of a particular matrix.  However, the technical definition is beyond the scope of undergraduate econometrics.  Thus, this not-too-difficult concept becomes a slippery non-technical concept. Any statistical test that is based on a $t$ or $\chi^2$ must define degrees of freedom.  Both of these distributions are really a family of distributions that differ by the degrees of freedom, the number of <u>independent</u> standard normal variables summed and squared.  What is a bit tricky, without some more math, is that the number of independent items is not the same as the total number of things summed.</p>

When we think of $H_0$ as a subset ${\cal R}$ of the sample space $\Theta$. Then degrees of freedom can be determined somewhat intuitively. The <span><dfn id="dof">degrees of freedom</dfn> for a restriction ${\cal R}$ is the dimensions $\Theta$ <em>minus</em> the dimensions of ${\cal R}$</span>.   For example, suppose $\theta = (\theta_1,\theta_2)$ contains two parameters (like the univariate nLRM has two parameters).  And the theory behind the restriction is that $\theta_1=0.0$.  Then the subset ${\cal R}$ are all the parameter vectors that can be written $(0.0,\theta_2)$, because the theory does not restrict the second parameter. Then in this case $\Theta$ has two dimensions and ${\cal R}$ has one dimension.  The degrees of freedom related to testing $H_0: \theta\in{\cal R}$ will equal the difference, $2-1=1$.</p>


<a name="Fig4"></a>
<figure><h4>Exhibit 4.  Degrees of Freedom of a Restriction (LR test only) </h4><img  width="60%" src="img/dof.png" alt=""/>
<figcaption>The degrees of freedom associated with a test equal the difference between the number of free parameters and the number of free dimensions in  ${\cal R}$.</figcaption>
</figure>
</OL>
<h2><a name="s017"><LI>MLE<br/>Likelihood &amp; Maximum Likelihood Estimation</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s018"><LI>Likelihood</LI></a></h3>

R. A. Fisher introduced the notion of likelihood and the method of Maximum Likelihood in 1922. His brilliant and not obvious insight was to think of estimation as reversing the direction of probability.  He coined the technical term <em>likelihood</em> to highlight this distinction:
<DD>Probability: The chance of observing an outcome (before the experiment happens) based on the true population parameters.</DD>
<DD>Likelihood: The same function as probability, but expressing how likely observed outcomes are if population parameters took on estimated values.</DD>
<a name="Fig5"></a>
<div class="alg"><h4>Exhibit 5.  Fisher's Subtle Shift in Perspective </h4>
<DD><pre>
                                           CONCEPT
What is ...            |    Probability                Likelihood
----------------------------------------------------------------------------
                       |
given/pre-determined   |  population parameters         data (sample)
                       |
variable/determined    | data (random outcomes)        parameter estimates   
                       |
------------------------------------------------------------------------------
</pre></DD></div>

Fisher also noticed that in many cases <q>good estimates</q> of probability model parameters were the ones that <em>maximized</em> the likelihood function as defined by him. This observation led him to suggest a new way to think about statistics: <b><em>maximum</em> Likelihood estimation (MLE)</b>.  <q>Good estimates</q> that Fisher noticed include formulas you have learned/derived in Econ 250 and Econ 351 without knowing they can be interpreted as MLEs.</p>

<a name="Fig11"></a>
<div class="alg"><h4>Definition 11.  How to Derive the Likelihood under IID sampling </h4>
<OL class="steps">
<li>MODEL</li>
    Specify the probability model by assuming or deriving distribution(s) for data that depend on unknown parameters (Greeks). Generically:
     $$Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\  F(y;\theta)$$
<LI>PROBABILITY</LI>
    Derive the mathematical form of the sample pdf and its logarithm for an IID sample:
    $$f\left(Y_{i= 1,\dots,N}\,;\,\theta\right) = \prod_{i=1}^N {f(Y_i;\theta)}\qquad \ln f\left(Y_{i= 1,\dots,N}\,;\,\theta\right) = \sum_{i=1}^N \ln f(Y_i;\theta)$$
<LI>LOG-LIKELIHOOD</LI>
    Switch order of arguments; Insert sample data for $Y_i$ to get the sample log-likelihood.
    $$\ln L(\hat\theta\,;\, Y_{i= 1,\dots,N}) = \sum_{i=1}^N \ln f(Y_i;\hat\theta)$$
</OL>
</div>

The expressions in MODEL and PROBABILITY are <em>generic</em>.  To actually do ML estimation the generic $f(Y_i;\theta)$ must be replaced by the mathematical expression for that particular model.</p>

<h5>The IID Bernoulli Log-Likelihood</h5>
Since we already derived the Bernoulli IID Sampling probability, we only need to do step 3 of the recipe above to derive the Bernoulli log-likelihood: Switch arguments and Greeks wear hats:
$$\ln L\bigl(\hat\theta\,;\, Y_{i=1,\dots,N}\bigl) = S\ln\hat\theta + (N-S)\ln(1-\hat\theta)$$
where $S\equiv \sum_{i=1}^N Y_i$.

<a name="Fig6"></a>
<figure><h4>Exhibit 6.  Two Bernoulli Log Likelihood Functions </h4>
<img src="img/blike.jpg" width="80%" alt="Two Bernoulli Log Likelihood Functions"/>
</figure>

<h5>The IID nLRM1 Log-Likelihood.</h5>

Since we already derived the log density for IID normal regression, we again only need to do step 3 of the recipe above to complete derivation of the nLRM log-likelihood. Switch arguments and Greeks wear hats ($\pi$ is Macedonian)
$$\ln L\left(\hat\beta,\hat\sigma\ ;\ Y_{i=1,\dots,N} \right)= -{1\over 2}\left(N\ln2\pi + 2N\ln\hat\sigma+{1\over \hat\sigma^2}\sum_{i=1}^N \left(Y_i-\hat\beta\right)^2\right)$$

<a name="Fig7"></a>
<figure><h4>Exhibit 7.  nLRM Log-likelihood function for a particular sample </h4>
<img width="80%" src="img/log-like.jpg" alt="A nLRM Log-likelihood function"/></figure>


<h3><a name="s019"><LI>MLE: Maximum Likelihood Estimation</LI></a></h3>

<a name="Fig12"></a>
<div class="alg"><h4>Definition 12.  Maximum Likelihood Estimator </h4>
Start with a probability model with parameter vector $\theta$, parameter space $\Theta$, and a sample of data $Y_{i=1,2,\dots,N}$ drawn from the model.  Maximum (log-)likelihood is the highest value across feasible estimates.
$$\ln L^{mle}_U \quad \equiv \quad \max_{ \hat\theta \in \Theta}\quad \ln L\bigl(\hat\theta;\ Y_{i=1,\dots,N}\bigr).$$
MLE is the argument that does the maximization:
$$\hat\theta^{mle}_{U}\quad \equiv\quad \arg\max_{ \hat\theta \in \Theta} \quad \ln L\bigl(\hat\theta ;\ Y_{i=1,\dots,N}\bigr)$$
</div>

Since <span>maximization is over the whole parameter space we call this the <dfn id="unrestricted">unrestricted ML</dfn></span> and the label <em>U</em> to indicate this. When testing hypotheses about a probability model, more than one MLE of $\theta$ is relevant based on restricting choices of $\hat\theta$ to maximize over for that same sample.</p>

When ML is treated as a procedure before knowing the value of the data it produces the estimator which is a function of the data (like demand is a function of the consumer's budget constraint). Here we simply call the formula $\hat\theta$ because in practice we have just one sample.  But the likelihood function would be different for a different sample and therefore so will the MLE.</p>

The true parameter $\theta$ never appears in the definition of MLE, which is good because by assumption we do not know the value of $\theta$! That's why we use the data to learn something about it. MLE is a <u>general strategy</u> for statistical estimation. Why $\hat\theta^{mle}{}$ is a "good estimator" is described in the next chapter. Further, MLE is not always a formula:  The definition of MLE says "find the maximum of the likelihood."  This can lead to a formula when the maximum of a function can be solved for analytically.  In these cases, MLE has a <u>closed form</u>.  But often there is no closed-form solution.  Then MLE requires finding the maximum numerically using a computer.</p>

Why Maximize the Likelihood of the Data? It sounds like a good thing to maximize likelihood, but it is not obvious what will happen if this done. It turns out that MLE is a good procedure for using sample data to learn about the population parameters that generated the data. We will briefly discuss these reasons, but MLE is not as intuitive as the OLS approach emphasized in Econ 351. Later on we compare OLS and MLE as approaches.  Sometimes they agree, sometimes they disagree and in some models one or the other is not applicable.</p>

<h5>MLE for the Bernoulli Model</h5>
Let $Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\  Bernoulli(\theta)$.
<OL class="steps">
<LI>Define log likelihood:
    $$\ln L(\hat\theta; Y_{i=1,\dots,N})\ =\ {\sum_{i=1}^N } Y_i\ln\hat\theta+(N-\sum Y_i)\ln(1-\hat\theta)$$</LI>
<LI>Derive the derivative of the objective with respect to $\hat\theta$:
    $${d\ln L(\hat\theta; Y_{i=1,\dots,N})\over d\hat\theta}\ =\ {{\sum_{i=1}^N Y_i}\over \hat\theta}-{N-\sum_{i=1}^N Y_i\over 1-\hat\theta}$$</LI>
<LI>Impose the FONC to implicitly define the MLE of $\theta$:
   $${{\sum_{i=1}^N Y_i}\over \hat\theta^{mle}_U }-{N-\sum_{i=1}^N Y_i\over (1-\hat\theta^{mle}_U)}\ =\ 0$$</LI>
<LI>Solve for $\hat\theta^{mle}$.
$$\hat\theta^{mle}_U\ = {1\over  N}{\sum_{i=1}^N Y_i}.$$</LI>
<LI>Confirm second order conditions.</LI>
</OL>

<a name="Fig13"></a>
<div class="alg">
<h4>Definition 13.  Bernoulli MLE. </h4>
Let $Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\  Bernoulli(\theta)$.  Then
<OL class="compact">
<LI>$\hat\theta^{mle}_U =  {{\sum Y_i}\over  N} = {\bar Y} = $ <em>the sample average outcome</em>.</LI>
<LI>If there are both some 0s <u>and</u> some 1s in the sample the solution is in the interior of $\Theta$ ($\hat\theta^{mle}_U\in(0,1)$) and satisfies FONC and SONC.</LI>
<LI>Otherwise, if there are only 0s <u>or</u> only 1s then $\hat\theta^{mle}$ is on the boundary of  $\Theta$.</LI>
</OL>
</div>


<h5>MLE for nLRM1</h5>
Let $Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\  {\cal N}(\beta,\sigma^2)$
<OL class="steps">
<LI>Define log likelihood
$$\ln L\left(\hat\beta,\hat\sigma\ ;\ Y_{i=1,\dots,N} \right)= -{1\over 2}\left(N\ln2\pi + 2N\ln\hat\sigma+{1\over \hat\sigma^2}\sum_{i=1}^N \left(Y_i-\hat\beta\right)^2\right)$$
</LI>
<LI>Derive the derivative of the objective with respect to $\hat\beta$ and $\hat\sigma$:
    $$\eqalign{
    {\partial \ln L \over \partial \hat\beta} &= {-2\over \hat\sigma^2}\sum_{i=1}^N \left(Y_i-\hat\beta\right)\cr
    {\partial \ln L \over \partial \hat\sigma} &= {2N\over\hat\sigma} - {2\over \hat\sigma^3}\sum_{i=1}^N \left(Y_i-\hat\beta\right)^2\cr}$$
    </LI>
<LI>Impose the FONC to implicitly define the MLE of the two parameters (with some simplifying)
    $$\eqalign{
    \sum_{i=1}^N \left(Y_i-{\hat\beta^{mle}}\right) &= 0\cr
    N - {1\over {\hat\sigma^{mle}}^2}\sum_{i=1}^N \left(Y_i-\hat\beta^{mle}\right)^2 &= 0\cr}$$
</LI>
</OL>


<a name="Fig14"></a>
<div class="alg">
<h4>Definition 14.  One Variable Normal MLE </h4>
Let $Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\  {\cal N}(\beta,\sigma^2)$
<OL class="compact">
<LI>$\hat\beta^{mle}\ =\ \sum_{i=1}^N Y_i/N = {\bar Y}$</LI>
<LI>$\hat\sigma^{mle} = \sqrt{{1\over N} {\sum_{i=1}^N \bigl(Y_i-\hat\beta^{mle}\bigr)^2}}$</LI>
<LI>Putting the MLEs together in a vector (and rewriting):
$$\hat\theta^{mle}_U = \pmatrix{\hat\beta^{mle}\cr \hat\sigma^{mle}}= \pmatrix{ {\bar Y} &\cr&\cr \sqrt{{\sum_{i=1}^N \bigl(Y_i-{\bar Y}\bigr)^2}/N}} $$
</LI>
<LI>Since the parameter space for $\beta$ is unbounded, its MLE is always an interior solution. </LI>
<LI>The lower bound for $\sigma$ is  0.0 and $\hat\sigma^{mle}=0.0$ only if <em>all $Y_i$ are the same value.</em></LI>
<LI>Substituting $\hat\theta^{mle}_U$ into the log-likelihood results in a unrestricted maximum:
$$\ln L^U = -{N\over 2}\left(\ln2\pi + 2\ln\hat\sigma^{mle}+1\right).$$</LI></OL>
</div>
<h3><a name="s020"><LI>What makes an estimator good?</LI></a></h3>
<blockquote class="quote"><em>If you can't get it right as N goes to infinity you shouldn't be in this business.</em><br/><b>C.W. Granger</b> [attributed]</br></blockquote>

We have defined what a probability model is.  And if the parameters of the model are $\theta$ then we defined $\hat\theta$ to be an estimator of $\theta$. The method of maximum likelihood is one approach to estimation and we call the result $\hat\theta^{mle}$.  We showed MLE is pretty simple in two models and the resulting formulas perhaps seem reasonable.  We now formalize reasonable.  We define notions of a good estimator that apply across models and estimation methods. When we talk about properties of an estimator we will be evaluating the estimator <em>ex ante</em>, not the estimates it produces <em>post</em>. We cannot judge an estimator by looking at the estimate it produces in a sample and deciding to accept it or not.  For example, suppose a coin is indeed fair but we do not know that without tossing it.  We estimate $\theta$ for it as a Bernoulli model and in a sample of 10 tosses only 1 comes up Heads.  The MLE estimate will say that the chance of a Heads on the next toss of the same coin is $1/10$.  The gods who know the value of $\theta$ is actually $1/2$ are laughing at us for drawing such a wrong conclusion.  But without more information what else could we predict?  At least one thing we could say is that 10 tosses are not a lot and even though we think the chance is 1 in 10 of a Heads on the next toss we could be wrong. </p>

The properties below pertain to what happens across possible samples and how the estimates in these samples compare to the population parameter we are trying to estimate.  In addition, once we get an estimate we want to be able to say something about how confident we are about the estimate.  If we had drawn a different sample would the estimate be similar to what we see or possibly quite different? If $X$ is a random variable, then we can use its variance, $Var[X]$, or its standard deviation, $sd[X] = \sqrt{Var[X]}$ to measure how much it varies across random experiments.  If we have data on $X$, then we can use the sample versions ($\hat{Var}[X]$ or $\hat{sd}[X]$ to measure how much it varies within the sample.  One thing that is covered in Econ 250 is that the sample variance is a good way to estimate the population variance.  But now things are a little different.  When we talk about an estimator $\hat\theta$ it is a formula that depends on random outcomes so it itself is random.  But for a given sample of data (containing variables with names like $X$, $Y$, etc.) we get a single value of $\hat\theta$, called the estimate.  There is no variation in an estimator within a single sample.  Its variation comes from how it would change values over other samples we might have drawn from the population.  So an estimator has a variance and standard deviation, but we can't estimate use the formula for sample standard deviation to estimate its (population) standard deviation.  This difference is one reason that the term <span><dfn id="stderr">standard error</dfn> is is used for the standard deviation of an estimator, $se(\hat\theta)$.</span>  We use the properties of the formula or procedure that defines $\hat\theta$ along with the information within the sample about how variable the data are to estimate standard errors.  So, technically, packages like Stata produce <em>estimated</em> standard errors, but is very common to just call the output standard errors</p>

<span><dfn id="asym">Asymptotic properties or large sample properties</dfn> of an estimator relates to what happens as the sample size $N$ increases without bound.</span>  That is, what happens to the estimator as $N\to \infty$.  Properties when $N$ is not pushed to $\infty$ are called <em>small sample properties</em>. It may seem strange that asymptotic properties are easier to derive than small sample properties.  But this is true.  One reason is because the <q>noise</q> from a small sample is washed out as $N$ increases. For example, if you toss a fair coin 20 times you may get all Heads (because the probability of 20 heads in a row is not zero).  Extreme outcomes like this may affect the estimator a great deal.  Small sample properties have to account for these extremes.  But for large $N$ the sample data start to look like the population distribution.  This tendency for large samples to approach the population they come from is encapsulated in various <em>Law of Large Number</em> results. Estimators based on large samples are often governed by a <em>Central Limit Theorem</em> (defined below). CLTs show that averages of data from all sorts of distributions start to follow the normal distribution (the Bell Curve) as the sample size gets large.  And in practice large is not very large indeed before the CLT starts.</p>

Because asymptotic properties are simpler theoretically than their small sample counterparts, they serve as a basic check on the soundness of the formula (estimator) used to estimate parameters of the model.  Hence Granger's aphorism above. Indeed, small sample properties of an estimator are only easily derived if the estimator is linear in the random data (in our case the $Y$ random variable).   Two of the MLE estimates we have derived already are indeed linear since they are simply the average of the $Y$'s.  One way to study small sample properties of non-linear estimators is to simulate the estimators using Monte Carlo methods on the computer (see next chapter).</p>

We have IID data that we wish to apply a parametric model to:
$$Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\   F(y; \theta).$$
The $\theta$ could be something you are used to, such as the population mean of $Y$, or its variance, or its median, etc. We have an estimator $\hat\theta$, which is a function of the data. This estimator may be a formula you have already study or it could be something new to you that is being proposed by someone as a way to learn about $\theta$ from  data. For a given sample (a bunch of numbers) we "plug" the data into the formula (the estimator) and get a number, which becomes an estimate. But before plugging in actual (realized) data, the estimator is a formula which depends on still random variables, $Y_{i=1,\dots,N}$. A function of random variables is itself random, so an estimator is a random variable itself (but an estimate is the realized outcome of the estimator and so is not random.</p>

As a random variable, $\hat\theta$ has a distribution over the sample space. We will write it as $\hat\theta_N$ to indicate that the properties (its distribution) will depend on the sample size $N$. A <b>good estimator</b> is a formula that has <q>good properties</q>: as random variable (before plugging in) its distribution is <q>good</q> when we want to learn about  $\theta$.</p>

<a name="Fig15"></a>
<div class="alg"><h4>Definition 15.  Good Properties of Estimators </h4>
<OL class="compact">
<LI><b>Unbiased.</b> $\hat\theta_N$ is an <b>unbiased</b> estimator of $\theta$ if
$$E[\hat\theta_N] = \theta\qquad\hbox{ for any sample size } N.$$
If an estimator is not unbiased then it is biased. </LI>
<LI><b>Asymptotically Unbiased.</b> $\hat\theta$ is an <b>asymptotically unbiased</b> estimator if
$$\lim_{N\to\infty}\ E[\hat\theta_N] = \theta.$$
Another way to put this is to say its bias goes to 0:
$$\lim_{N\to\infty}\ E[\hat\theta_N] - \theta = 0.$$
</LI>
<LI><b>More efficient.</b> One estimator, say $\hat\theta^1_N$, is <b>more efficient</b> than another estimator,
$\hat\theta^2_N$ if
$$Var(\hat\theta^1_N) < Var(\hat\theta^2_N). $$
We can say the two estimates are equally efficient if their variances are equal.</LI>
<LI>Consistent. $\hat\theta_N$ is <b>consistent</b> if
<DD>1. It is asymptotically unbiased: $\lim_{N\to\infty} E[\hat\theta_N] - \theta =0.$</DD>
<DD>2. $\lim_{N\to\infty} Var[\hat\theta_N] =0$</DD>
</LI>
<LI><b>Efficient.</b> $\hat\theta^\star$ is an efficient estimator in a <em>set</em> or <em>class</em> of estimators if it is <em>equally or more efficient</em> than all the members of the class.</LI>
</OL>
</div>

<DT>NOTES</DT>

The <em>bias</em> of an estimator is $bias(\hat\theta_N) = E[\hat\theta_N] - \theta.$ <em>Bias</em> sounds bad, and it is better to be unbiased.  But this is just one good property of an estimator.  An estimator may be biased but have other good properties and thus be worthy of use.</p>

Statistical consistency is a minimum requirement for any estimator.  It means we "stay in the business."  With enough data (big enough sample size), the estimator converges in a statistical sense to the true parameter $\theta$. The statistical procedure uncovers the true parameter value for sure (given a large enough sample).

Technically the two conditions for consistency are <em>sufficient</em> conditions for the estimator to be consistent.  The actual definition of consistency involves probability limits, which we will avoid worrying about since this is <q>applied</q> econometrics.</p>

We can easily use an estimator with 0 variance.  Just pick a number for the estimate that does not depend on the data at all.  This estimator would have 0 variance across samples. We might tradeoff less efficiency for more bias in an estimate.  In fact, you already have seen this, probably without knowing it.</p>

<a name="Fig8"></a>
<figure><h4>Exhibit 8.  Biased but Inconsistent Estimator </h4>
<div id="split">
<img src="img/consistent0.png"  width="90%"/><img src="img/consistent1.png"  width="90%"/><br/>
<img src="img/consistent2.png"  width="90%"/><img src="img/consistent3.png"  width="90%"/>
</div>
</figure>


<a name="Fig16"></a>
<div class="alg">
<h4>Definition 16.  The Asymptotic Properties of MLE </h4>
Under some technical conditions about the probability model (satisfied by our models and most others)  $\hat\theta^{mle}$ as an estimator of the population parameter $\theta$ satisfies these properties:
<UL>
<LI>$\hat\theta^{mle}{}$ is a <b>consistent</b> estimate of $\theta$</LI>
<LI><em>asymptotically</em> $\hat\theta^{mle}{}$ follows a <b>normal</b> distribution.</LI>
<LI><em>asymptotically</em> $\hat\theta^{mle}{}$  is <b>efficient</b>: no other consistent estimator has a lower asymptotic variance.</LI></UL>
</div>


<h4>Small Sample Properties of OLS</h4>

In the linear regression model (LRM) without assuming normality we can apply the method of ordinar least squares (OLS).  As derived earlier for MLE on the nLRM, the assumptions of no correlation between the error and $X_i$ imply that $\hat\beta^{ols}$ is <b>unbiased</b> for any sample size $N$, not just asymptotically unbiased and not just if $z$ is normally distributed. This is because the OLS estimator (the formula) in LRM is the same as MLE in nLRM, <em> and</em> we do not need used the assumption of normality when proving $E[\hat\beta^{mle}_{1}]=\beta_1$.  We use the assumptions about the expectation of $z_i$ conditional on $X_i$

<a name="Fig17"></a>
<div class="alg">
<h4>Definition 17.  The Gauss-Markov Theorem: OLS is BLUE </h4>
Given these assumptions in the LRM:
<OL class="compact">
<LI>The population relationship between $Y_i$ and $X_i$ is a linear stochastic one:
$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i.$$</LI>
<LI>$\epsilon_i$ has zero mean: $E[\epsilon_i] = 0$; and equal variance, $Var[\epsilon_i]=\sigma^2$.</LI>
<LI>For each $i$, $Cov[\epsilon_i,X_i]=0$.  That is, the <em>included</em> variable(s) are uncorrelated with <em>excluded variables</em>.
</LI>
</OL>
Then OLS is
<DD><b>B</b>est (i.e. lowest variance among)</DD>
<DD><b>L</b>inear (in $Y$)</DD>
<DD><b>U</b>nbiased: (i.e. $E[\hat\beta] = \beta$)</DD>
<DD><b>E</b>stimators.</DD>
<em>Any</em> other linear unbiased estimator of $\beta$ has greater variance across samples than $\hat\beta^{ols}$ The proof is not particular hard and is  a standard element of Introductory Econometrics.
</div>

<DT>Good Estimator Results</DT>
<DD><pre>
        MLE is consistent                                      OLS is BLUE
----------------------------------------------------------------------------------------------------------
    +Holds for (nearly) all models                          Holds for LRM
     Holds for large N (asymptotic)                        +Holds for any N
     Requires full probability model                       +Requires only A1-A3
    +Compared to any other consistent                       Comparison to any other linear unbiased
</pre></DD>

Properties marked with <q>+</q> makes it better by applying in more situations or making a comparison to a larger set of alternative estimators.
Both theorems gives us good reasons to believe that econometric estimates are meaningful and not easily improved upon by applying a different formula to the data. These two lists overlap completely  when applied to nLRM and estimates of $\beta$.  In this case all the "+" points hold.  So this is the benchmark econometric model. When the assumptions of the model fail in some way then there can be reasons to deviate from using the MLE/OLS estimators.  In some cases you might be better using a method based on OLS that fixes some problems.  In other cases you might use ML but based on a different probability model.

</OL>
<h2><a name="s021"><LI>Two Variable Models</LI></a></h2>

So far we have developed the main concepts need to understand and apply maximum likelihood estimation illustrated them with two models that involve only one variable.  Now we develop multivariate versions of those two models which are the workhorses of applied econometrics.  </p>

The regression and probit models relate a variable $Y$ to one or more other variables.  First we will add one variable to the regression model that we will call $X$.  This is usually called a <em>simple regression model</em>.  Then we discuss the simple probit model which generalizes the one variable Bernoulli model. After the linear regression model the probit model is probably the most common probability model in economics, as has been ubiquitous in the literature since the 1970s.   Later we generalize both probit and regression to many variables and introduce matrix notation which leads to simple formulas regardless of the model size.</p>

We motivate the regression and probit model using a simple microeconomic model.  The point here is to illustrate that the parameters you are estimating with econometrics can be parameters of an economic model covered in other classes. To interpret the results of a multivariate properly it is essential to return to one more concept from the prerequisite course: <em>conditional probability.</em>  Both models are really conditional models.  Regression is a model of the conditional mean, and probit is the model of a conditional probability.  The one-variable models are unconditional models.</p>

The probit model is over 70 years old.   After the linear regression model it is probably the most common probability model in economics, ubiquitous since the 1970s.  Like Bernoulli, the endogenous random variable is binary.  In fact, Bernoulli is a special case of probit in which the probability of observing a "1" is the same for each observation. As with nLRM2, the simple probit models the relationship between $X$ and $Y$.</p>

MLE on the probit model does not coincide with the sample average (unlike Bernoulli and nLRM1). More than that, there is <u>no closed form solution</u> for MLE.  Algorithms to climb a hill numerically (optimization an objective) were developed in the 1950s and 1960s that make estimation of a probit model almost as easy and reliable as estimators that have a closed form solution. For decades the probit model was very costly to estimate, limiting its applications compared to estimators with closed form such as linear regression. Plus there is a very similar model called <em>logit</em> which we also discussion that avoids some of this computational burden.  However, computation is no longer an issue in terms of estimating a probit. Logit-based models are still common because in more complicated environments its relative convenience is still important.  The probit fits more closely with nLRM.</p>


<h4>Conditional Probability</h4>

Recall that & probability is a function of random events, which are subsets of the sample space, the set of all possible outcomes of a random experiment. If A and B are events then $P(A\cup B)$ or $P(A\cap B)$ are well-defined.  The first is the chance that either event A or event B (or both occur in the same random experiment).  </p>

Let $P(B)\ne 0$. Then  $P(A | B)$ is the probability of A <em>given</em>B" or "<em>conditional on</em> B" and is defined as
$$P( A | B ) \equiv {P(A\cap B) \over P(B)}$$
By conditioning on B it is no longer random whether B occurs or not.  A $|$ B redefines the sample space from <u>every outcome in $\Omega$</u> to <u>outcomes in B}.</u></p>

A conditional probability is a well-defined probability for any subset on $\Omega$ not just B. So $F(y|B)$ is our notation for the cdf of a random variable conditional on event $B$:
$$F(y | B) = Prob( Y\le y | B) = {Prob(Y\le y, B) \over Prob(B)}.$$
And the conditional density or probability:
$$f(y|B) = \matrix{ {Prob(Y=y,B)\over Prob(B)} & \hbox{if Y discrete}\cr\ &\ \cr F^\prime(y|B)&\hbox{if Y continuous}\cr}$$</p>

Joint and marginal pdf's and cdf's can all be conditional.  So can expectation and variance. If $P(A)>0$ then we can apply the definition of conditional probability to $B$: $P( B | A ) = {P(A\cap B) \over P(A)}$.
Use this in the expression for conditional probability of $A$ leads to Bayes' Theorem:
$$P( A | B ) = {P(B | A) P(A) \over P(B)}.$$

<OL  type="1" class="toc3" >
<h3><a name="s022"><LI>The 2-Variable Normal Linear Regression Model</LI></a></h3>


We write $Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\   {\cal N}(\beta_0 + \beta_1 X_i, \sigma^2)$ to mean that $X_i$ is a variable in the data set paired with $Y_i$. And the observed value of $Y_i$ equals
$$\eqalign{ Y_i &= \beta_0 + \beta_1 X_i + \sigma z_i, \qquad\qquad (PRE)\cr
z_i &{\buildrel  iid \over \sim}\   {\cal N}(0,1).\cr}$$

Define $K_x\equiv 2$, where $K_x$ counts two things that both added up to 2. $K_x$ is the number of variables that vary across $i$ ($Y_i$ and $X_i$).  It also counts the number of estimated coefficients on observed variables ( a constant 1 and $X_i$).  The total number of parameters, the length of $\theta$, is  $K = K_x + 1 =3$:
$$\theta = \pmatrix{ \beta_0\cr\beta_1\cr\sigma}.$$

The 3-dimensional parameter space is: $\Theta = (-\infty,+\infty) \times (-\infty,+\infty)\times [0,+\infty).$}
A special case of this model is when $\beta_1$ is known (fixed) at $\beta_1=0.0$. This case returns the model to nLRM1.  The only difference is that the unknown population parameter $\beta$ in that model is now called $\beta_0$.</p>

<a name="Fig9"></a>
<figure><h4>Exhibit 9.  The Normal LRM </h4><img  width="60%" src="img/nLRM5.png"/><figcaption>The regression is the expected value of $Y$ given $X$.  Every
observation has an additional term, the disturbance to the linear expectation.  The disturbance is distributed as a normal random variable along the vertical dimension because it shifts the vertical location of $Y_i$.  </figcaption></figure>

<a name="Fig18"></a>
<div class="alg"><h4>Definition 18.  Alternative Definition of nLRM With Listed Assumptions </h4>
<DL>
<DT>A0: the PRE is True</DT>
<DD>$Y_i$ is generated by the population regression equation $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i.$</DD>
<DT>A1: Errors have zero mean</DT>
<DD>$E[\epsilon_i] = 0$.</DD>
<DT>A2: error terms are homoscedastic</DT>
<DD>$\hbox{Var}(\epsilon_i) = \sigma$, for all $i$</DD>
<DT>A3: error terms are uncorrelated</DT>
<DD>$\hbox{Cov}(\epsilon_i,\epsilon_j)$ for $j\ne i$.</DD>
<DT>A4: $X$ is uncorrelated with the error term:</DT>
<DD>$Cov[\epsilon_i,X_i] = 0$ for all $i$.</DD>
<DD>Note: Combined with A1 this means $E[\epsilon_i | X_i ] = 0$.</DD>
<hr/>
<DT>A5 : Error terms are normal</DT>
<DD>$\epsilon_i = {\cal N}(0,\sigma)$</DD>
</DL>
</div>

<h4>Causation, Correlation and Conditional Expectation in nLRM </h4>

The equation (PRE) assumes a random (or stochastic) relationship between $X$ and $Y$.   For a given observation any value of $Y_i$ might be paired with a given $X_i$, because the error term $z_i$ can take on any real value.  So $z_i$ perturbs or disturbs the assumed linear relation between the observables $X$ and $Y$.</p>

<dd><pre>
             NAMES FOR OUR VARIABLES
         X                            Y
 -----------------------------------------------
   exogenous variable       endogenous variable
   control variable         outcome variable
   explanatory variable     explained variable
   right-side variable      left-side variable
   regressor                regressand
</pre></dd>

Equation (PRE) is the first key assumption of the nLRM. Writing down (PRE) makes it look like $X$ <em>causes</em> changes in the expected value of $Y$.  That is: $X\ \rightarrow\ Y$.  That is the assumption and the interpretation, but that does not make it the true relationship between $X$ and $Y$.  </p>

The data do not tell us directly whether this assumption holds or not. First, the causality could be reversed:  $X\ \leftarrow\ Y$. So $Y$ could be causing the expected value of variables in $X$ to change.  This would still result in a correlation between $Y$ and $X$, but now we would better off putting $X$ on the left and $Y$ on the right.  Whether $Y$ is endogenous and $X$ variables are exogenous is <u>not</u> settled by estimating a nLRM.  </p>

For example, make a scatter plot of these 5 observations:<DD><pre>
i  Age    Monthly Income
------------------------
1   18         600
2   22        3500
3   25        6200
4   27        4025
5   35        6033</pre></DD>
Your brain detects a positive relationship between Age and Income.  It is <u>not</u> a deterministic relationship (Age and Income pairs do not fall on a line), but the greater Age is the more likely it appears that income will be higher. Which way does the causality go?  Are we justified in making Income the $Y$ variable (the endogenous variable)?  And Age the exogenous, $X$ variable?  That is, should we assume Age $\rightarrow$ Income?</p>

You probably interpret Age to mean &hellip; "age of a person in years" and Monthly Income to mean "personal income in a month in dollars."  It would be pretty safe to say a person's age is exogenous, and any relationship between it and how much money a person makes is due to causality as Age $\rightarrow$ Income. </p>

But what if the unit of observation is not a <em>person</em> but a <em>firm</em>?  And age is measured in months since start up?  And income is tens-of-dollars? Then are we so safe to assume age causes income, or do firms that earn more income survive, so we do not observe old low-income firms?  In this case, income might be causing age indirectly by affecting the survival of business start ups.  You might even expect two-way causality: better earning firms survive but if they survive their age allows them to earn more because they are better known.  Either way, it is less obvious that Age $\rightarrow$ Income, not because the data have changed but because what their meaning has changed.  The meaning is not decided by how you write (PRE). Without thinking carefully you can make the true endogenous variable look like the exogenous variable and vice versa.</p>

Economic models sometimes suggest causality is the reverse of simpler explanations or other models. For example: The human capital explanation for a relationship between wages and education says that education is like an investment in a skill and wages are the return on that investment.  This view goes back to Adam Smith.  University is hard and expensive because that is what it takes to develop the skills that pay off in the labour market.  In the early 1970s the signaling modeling of education challenged the direction of causality.  It says that university graduates earn more because university is hard and expensive not because it teaches the graduates anything.  The insight was to show how an equilibrium can exist in which university does nothing but signal the already-present ability of the student.  Putting your Queen's degree on the wall may be like a peacock's tail. For our purposes the finding that wages and education tend to be positively related to each other does not tell us why they are related, especially when two plausible explanations exist that reverse the causality.</p>

Even if X $\rightarrow$ Y is the most plausible direction of causality, the way in which $X$ causes $Y$ may not be as simple as (PRE).  Here is an example of a (PRE):
$$\hbox{Blood Pressure} = \beta_0 + \beta_1 \hbox{Coffee Cups Per Day} + \sigma z. \qquad\qquad (*)$$
Since high blood pressure has no symptoms it is difficult to see why you might think it would cause a person to drink more cups of coffee.  So perhaps we are safe in assuming the causality goes in the direction assumed by the equation.  We might be interested in knowing whether $\beta_1>0$ or not.
Namely, does drinking coffee raise blood pressure? If these are the only random variables we can measure on this sample space (people) then we can't do much more than hypothesize why it might be that coffee and blood pressure appear to be related.</p>

But what if we had a third variable which could measure job-related and/or school-related stress?  Then in this case we might be concerned that people with stressful jobs drink more coffee in order to concentrate, but it is stress itself that is causing high blood pressure. In this case the causality is going through an variable (job stress) <em>omitted </em> from (*) which happens to be related to both blood pressure and coffee consumption.</p>

The effect of job stress is picked up (accounted for) by the presence of $z$ in the model (*). $z$,includes the cumulative effect of all factors in determining blood pressure <u>not</u> included in the model. But we might mistakenly conclude coffee causes blood pressure when in fact both are caused by a third omitted variable, job stress. </p>

This possibility is crucial to the proper interpretation of the nLRM.  Let's call the job stress of person $i$ as $H_i$.  Without observing $H_i$ it is part of $z_i$.  We might write: $H\ \rightarrow\ Z$ and $Z\ \rightarrow\ Y$.  Thus $H\ \rightarrow\ Y$ (stress affects blood pressure).  This itself is not a problem. </p>

The problem arises if $H\ \rightarrow\ X$.  Since $Z$ is unobserved, it <u>appears</u> that $X\rightarrow Y$, when in fact coffee may have no effect on blood pressure ($\beta_1=0$).</p>

If we are not careful the misinterpretation of regression coefficients can be very damaging. Suppose $\beta_1$ is estimated by researchers who conclude it is positive. We might start hearing warnings to cut down on coffee.  But this might do nothing to lower blood pressure because it does not address the underlying cause (stress).  Indeed, you could imagine that people who heed the advice and cut down on coffee might actually feel more stress because they are less able to focus (assuming coffee actually does help with focus).  These people experience an <u>increase</u> in stress and possibly an increase in blood pressure all because the (PRE) was interpreted in a way that got the causality wrong.</p>

After (PRE) itself the <u>path</u> of causality in the nLRM is the other critical assumption.  A4 assumes away the possibility of an unobserved variable acting through $X$. That is, <em>A4 Rules out a Proxy Relation Between $X$ and $Y$.</em>  Here are several assumptions that are equivalent or nearly equivalent to A4:<br/>
$Cov[z_i,X_i] = 0$. That is, the included observed variable $X_i$ is uncorrelated with the excluded (unobserved) disturbance term.  This says $X$ does not affect what value we expect $z_i$ will be.<br/>
$E[z_i | X_i] = 0$. This says the conditional mean of $z_i$ equals its unconditional mean (A1).  This is an implication of A4 and is the bare essential needed for the key result of statistical unbiasedness.<br/>
$z_i$ and $X_i$ are independent. That is, their joint pdf factors into the product of their marginal pdfs.  This is stronger than A4 and is not required by most important results.  It says the observed value of $X$ tells us <u>nothing</u> about the values of (unobserved) disturbance term.  By contrast, A4 or the weaker conditional expectation assumption only say that the value of $X$ tells us nothing about the typical values of $z_i$, as measured by its expected value or a linear relationship between $z$ and $X$.<br/>
$X_i$ is fixed (non-random) across repeated samples. This is the strongest of all the versions. We are modeling $z_i$ as random across possible samples (not just the one we observe as data).  So if $X_i$ is the same value in each possible sample, it is not random.  And a non-random variable is independent of any random variable (and therefore uncorrelated).</p>

<b>Assumption A4 or its companions are THE KEY to the multivariate models.</b> Unlike some other assumptions, A4 is not a technical one that can be solved with statistical technique It is difficult and sometimes impossible to run controlled experiments to reveal the direct relationship we are interested in. To be absolutely sure that you avoid contamination, you must be able to control the value of $X_i$ independently of the random unobserved factors $z_i$.  Assumption A4 assumes the sample $X$-$Y$ correlation is uncontaminated by unobserved factors contained in $\sigma z_i$. If A4 is <u>not</u> true, then the $X$-$Y$ correlation is partly a proxy for the relationship $z \rightarrow Y$.

<h5>The nLRM as a Model of Conditional Expectation</h5>

The (PRE) links $Y_i$ to $X_i$ in a <em>stochastic</em> relationship. The assumptions imply a non-random and linear relationship in conditional expectation. Take expectations of both side of (PRE) conditional on $X_i$:
$$\eqalign{E[Y_i\ |\ X_i] = &E[ \beta_0 + \beta_1 X_i + \sigma z_i\ |\ X_i] \cr
                              = &E[\beta_0 \ |\  X_i ] + E[\beta_1 X_i \ |\  X_i] + E[\sigma z_i \ |\  X_i ].\cr
                              &=\cdots\cr}$$
$\beta_0$ and $\sigma$ are constant population parameters.  By definition $X_i$ is constant conditional on $X_i$:
$$\cdots=\beta_0 + \beta_1 X_i + \sigma E[z_i \ |\  X_i]$$
The last term is the critical one. Under assumption A4, $E[z_i\ |\ X_i] = E[z_i] = 0$ (by A1). Thus
$$E[Y_i \ |\  X_i] = \beta_0 + \beta_1 X_i.$$
Given $X_i$, everything in this equation is a non-random constant. $E[Y_i \ |\  X_i]$ is an unknown population parameter. The nLRM assumes the conditional population expectation of $Y$ is a linear function of $X$.  We estimate this population expectation by estimating $\beta_0$ and $\beta_1$.</p>

Another aspect of the distribution is the variance.
$$Var[Y_i \ |\  X_i ] = Var[\beta_0+\beta_1X_i + \sigma z_i \ |\  X_i] = Var[\sigma z_i \ |\  X_i].$$
Following the reasoning above, the first two terms are constant (conditionally).  They shift the conditional mean but have no effect on the condition variance.  And again assumption A2-A4 combine to imply
$$Var[Y_i\ |\ X_i] =  Var[\sigma z_i \ |\  X_i] = \sigma^2 Var[z_i] = \sigma^2.$$
We already hinted at this result by writing
$$Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\   {\cal N}(\beta_0+\beta_1 X_i, \sigma^2).$$
This tells us we are modeling $Y_i$ as a normal random variable with variance $\sigma^2$ and a mean that depends on  another variable $X_i$.</p>


<h4>The Normality Assumption?</h4>

In addition the (PRE) models the full distribution of $Y_i$ given $X_i$, not just the conditional expectation. The nLRM assumes $Y_i$ is distributed around its conditional mean $\beta_0+\beta_1 X_i$ as a normal random variable.  It assumes a constant conditional variance  $\sigma^2$.  We estimate how variable $Y_i$ is around its conditional mean by estimating $\sigma^2$.</p>

Is MLE applied to $Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\   {\cal N}(\beta,\sigma^2)$ of limited or general use? Earlier the transformation of a raw variable, ln(Weight) rather than Weight, was discussed in the LRM.  Such a transformation can bring the data closer to what is predicted by the model.  Transformations can also be relevant for $X_i$ so that the linear conditional expectation property fits both $X$ and its relation to $Y$.</p>

Usually $Y$ depends on various factors other than $X$ that we can talk about but not see in the data.  We need these factors to be uncorrelated with $X$ (A4), but can we justify assuming normality as well?  These excluded influences all feed into the disturbance term $\sigma z$.  Suppose these factors are additive in their impact on $Y$. That is, suppose
$$\sigma z_i = \biggl(\sum_{j=1}^J V_{ji}/J\biggr)$$
where $V_j$ is one of the unobserved factors that influence $Y_i$. If each factor $V_j$ is itself normally distributed, then the sum of normal random variables is also a normal random variable.  So  $z_i \sim {\cal N}(0,1)$ is exactly true if all other influences on $Y_i$ are both additive and normal.</p>

The Central Limit Theorem says the average of many independent random variables follows the normal distribution regardless of their distribution.  So if $J$ is "large" and $V_j$ are independent then (A5) is approximately true, even if the $V's$ are not normal. So nLRM can often be a good approximation without being exactly correct. Together, the proper specification of $X$ and $Y$ and the CLT applied to the disturbance term make $z_i \sim {\cal N}(0,1)$ less restrictive / unrealistic / problematic than you might expect.
<h3><a name="s023"><LI>The 2-Variable Probit Model</LI></a></h3>

The probit and logit are both examples of <em>binary response models</em>. We focus on probit because it is closely related to the nLRM model.  In terms of the qualitative results, the logit model is nearly equivalent to probit for the basic binary cases.   However, in case of <em>multinomial response</em> some differences become important and both versions are still used.</p>

Consider a familiar case of a binary response to an underlying real value. In most sports the outcome of the contest is a <em>binary response</em>.  Either team A wins or team B wins. Some sports such as soccer have to allow for trinary (3) response: win/draw/lose, otherwise the game may never end! What is being responded to is the <em>difference in scores</em> of the two teams.  The response is which team wins.</p>

Formally, let $s_A$ and $s_B$ be their respective scores at the end of the contest. Define $Y^\star \equiv s_A - s_B$ as the score difference between. Then in a no-draw sport, a binary variable for team A winning could be defined as:
$$\eqalign{         &1 \quad \hbox{if } \quad Y^\star>0\cr
                Y \equiv & \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad (*)\cr
                        &0 \quad \hbox{if } \quad Y^\star<0\cr}.$$
In the end all that matters is the score difference.  Definition (*) says <em>$Y_i$ is a binary response to $Y^\star$</em>.  The value 0 is a threshold value of $Y^\star$ for which $Y_i$ jumps from 0 to 1 and then stays there for any positive difference. If psychological factors are important then for future performance the win/loss outcome may not be sufficient.  If killed is worse than losing a close game then the score difference itself matters in some sense.  But usually only the win/lose outcome determines standings, prize money, etc.</p>

There is a one difference between the sport analogy and the probit model. In the probit model we presume the underlying value, $Y^\star$, is <b>unobserved</b>.  It is not a variable in our data set. All we see is the binary $Y$, but we model $Y$ as a binary response to a $Y^\star$ that can take on any real value.  If we can see $Y^\star$ then we would end up with the <a href="TwoVariableRegression.html">nLRM</a>. </p>

We could generate $Y$ from the observed values of $Y^\star$. The reason the probit model is distinct from a regression, and very important to modern economics, is that it allows us to model a binary choice as a response to an underlying value that we model but do not observe.   Both regression and probit models might be useful in modeling the winner of the game before it is played, as a function of one or more observed attributes of the teams.  The outcome is random because there are random elements to sports (the bounce of the ball) and unobserved factors determine performance (motivation, key match-ups, sickness or illness, etc.</p>

A classic example: commuters who can either drive ($Y=0$) or take a bus ($Y=1$) to work.  The decision will depend on such important factors as the cost of maintaining and parking the car, the cost of a bus pass, the reliability and relative speed of the trips.  As economists, we would model a person as choosing to ride the bus if its net utility is greater than the utility of driving.   We may measure factors such as distance, parking costs, etc.  But we would not presume to <em>measure</em> or <em>observe</em> utility directly.  We allow that people have difference preferences over time and money and we cannot measure these differences directly.  Differences in preferences show up in the disturbance term in $Y^\star$.</p>

<DT>EXAMPLES OF BINARY RESPONSE RELATIONSHIPS</DT>
<DD><pre>
Field                 i              X                 Y*              Y = 0/1
-------------------------------------------------------------------------------------------------

agriculture         insect      insecticide         toxicity            alive / dead

medicine            person      antibiotic          infection           infected / cured

finance             firm        mgmt competence     profit              survive / go bankrupt

development        household   income              child role          daughter schooled / not

poli.sci.           voter       ad budget           interest            stay home / vote

--------------------------------------------------------------------------------------------------
</pre></DD>

<h4>A Binary Response <em>Model</em></h4>

So far we have simply said that $Y$ is 1 if $Y^\star$ is greater than 0, and otherwise $Y$ is 0.  This is not a <em>model</em> yet.  It is simply a mechanical definition. A binary response model makes $Y^\star$ a function of <em>parameters</em>, <em>observables</em> and <em>unobservables</em>.</p>

Typically, binary response models assume that $Y^\star$ is determined through a linear regression:
$$Y^\star = \beta_0 + \beta_1 X + z.\qquad\qquad(**)$$
For example, a model of which team wins a game would make the <em>score difference</em> a function of some aspect of the teams ($X$) and a random unobserved factor ($z$), which would include random factors such as the <u>bounce of the ball</u> as well as unobserved strengths of the teams.</p>

You might notice one difference between the equation (**) and the usual regression equation: the standard deviation of the unobserved factor, $\sigma$, is <em>set to 1</em>.  Why this is the case is explained later.</p>

Equations (*) and (**) together create a probability model for the binary variable $Y$. If we see the value of $Y^\star$ like a regression then we would know the value of the binary response.  Equation (*) is not a mechanical relationship with nothing random.  If we do not observed $Y^\star$ then the two equations combine determine a probability that $Y$ is 0 or 1, conditional on seeing the value $X$:
$$Prob(Y=1\ |\ X) = Prob\left( \beta_0+\beta_1 X + z > 0\ |\ X \right) = Prob\left( z > -(\beta_0+\beta_1 X) \right) = 1 - F\left( -(\beta_0+\beta_1 X) \right).$$

<a name="Fig10"></a>
<figure><h4>Exhibit 10.  Binary Response Model </h4><img  width="80%" src="img/BinaryResponse.jpg" alt="Binary Response Model"/></figure>

Even if we knew the values of the parameters $\beta_0$ and $\beta_1$ along with $X$ we would not know the probability of seeing a 1 unless we also know the distribution of the unobserved factor. The Probit and Logit models assume different distributions of $z$. The probit model combines (*) and assumption that $z$ is a standard normal random variable, $Z \ {\buildrel  iid \over \sim}\  {\cal N}(0,1)$. The logit model assumes that $z$ follows a similar but different distribution, introduced below.</p>

<em>A Binary Response Model generalizes the Bernoulli Model.</em>   Recall that in the Bernoulli model each observation has the same probability of $Y=1$, $Prob(Y=1) = \theta$. A binary response model makes $\theta$ a function of an observable exogenous variable, $X$.</p>

<a name="Fig19"></a>
<div class="alg"><h4>Definition 19.  Definition of the Two-Variable Probit, version A. </h4>
<DT>We write $Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\   Probit\left(\beta_0+\beta_1 X_i\right)$ to mean:</DT>
<OL class="compact">
<LI>$Y_i$ is a binary random variable. $X_i$ is a variable (fixed or random across samples) paired with $Y_i$ in the data.</LI>
<LI>$Prob\left(Y_i=1\ |\ X_i\right) = 1-\Phi\bigl(-\left(\beta_0+\beta_1 X_i\right) \bigr)$.</LI>
<LI>The parameter vector is: $\theta = \pmatrix{\beta_0\cr\beta_1}$.  So $K_x = 2 = K$.</LI>
<LI>The parameter space: $\Theta = (-\infty,+\infty) \times (-\infty,+\infty) $.</LI>
</OL>
</div>

Note that if $\beta_1$ is a known parameter with fixed value $\beta_1=0$ then each observation has the same probability of producing $Y_i=1$.  A complete mapping of Probit back to Bernoulli is discussed later. Because $\Phi(z) =1-\Phi(-z)$, we can also write $Prob\left(Y_i=1 | X_i\right) = \Phi\bigl( \beta_0+\beta_1X_i\bigr)$.  This is an equivalent ways of writing down the same model.  The second version is obviously a bit simpler, but the first version arises more easily from another way to define the Probit model...</p>

<a name="Fig20"></a>
<div class="alg"><h4>Definition 20.  Definition of the Probit Model, version B. </h4>
<DT>$Y_{i=1,\dots,N}  \ {\buildrel  iid \over \sim}\   Probit\left(\beta_0+\beta_1 X_i\right)$ can also be interpreted as:</DT>
<OL class="compact">
<LI>$Y$ is a binary random  variable.  $X$ is a variable (random or deterministic).</LI>
<LI>There is a <em>latent</em> (unobserved) random variable $Y^\star$ whose value is determined by:
$Y^\star_i = \beta_0 + \beta_1 X_i + z_i,$ and $z_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\   {\cal N}0,1).$</LI>
<LI>$Cov[X,z]=0$.  That is, $z$ is <u>uncorrelated</u> with the observed variable $X$.</LI>
<LI>$Y_i$ is a binary response to $X_i$ through the latent value of $Y^\star_i$:
$$ Y = \cases{  1 &if  $Y^\star>0$ \cr\cr
                0 &if $Y^\star<0$\cr}\qquad (*)$$</LI>
</OL></div>

The latent variable $Y_i$  varies with $X_i$ smoothly, but it invokes a response in $Y_i$ only if it surpasses a threshold value of 0.

<DT>Deriving the Probit log-likelihood function</DT>
The probit likelihood is the same as the Bernoulli but $\theta$ is replaced by the Probit probability for $Y_i= 1$. We can simply substitute that value into the formula we already have from the Bernoulli model:
$$f\left(Y_{i=1,\dots,N}\ ;\ X_{i=1,\dots,N}, \beta_0,\beta_1\right) =
\prod_{i=1}^N \biggl[1-\Phi\bigl(-\left(\beta_0+\beta_1 X_i\right) \bigr)\biggr]^{Y_i} \biggl[\Phi\bigl(-\left(\beta_0+\beta_1 X_i\right) \bigr)\biggr]^{1-Y_i}$$

However, we can also use symmetry of the normal density around 0 to eliminate the minus sign that appears in both $\Phi()$ expressions.  That is, the probability of a standard normal random variable being less that $x$ is exactly the same as the probability of being greater than -x.  So $\Phi(-x) = 1-\Phi(x)$.  And $1-\Phi(-x) = \Phi(x)$.  So the two components can be rewritten more simply:
$$=
\prod_{i=1}^N \biggl[\Phi\bigl(\beta_0+\beta_1 X_i\bigr)\biggr]^{Y_i} \biggl[1-\Phi\bigl(\beta_0+\beta_1 X_i\bigr)\biggr]^{1-Y_i}$$

As always, take the log probability:
$$\eqalign{
\ln f\left(Y_{i=1,\dots,N}\,;\,X_{i=1,\dots,N},\beta_0,\beta_1\right) &= \cr
\sum_{i=1}^N  &{Y_i} \ln\biggl[\Phi\left(\beta_0+\beta_1 X_i\right)\biggr] +\bigl(1-Y_i\bigr)\ln\biggl[1-\Phi\left(\beta_0+\beta_1 X_i\right)\biggr].\cr}$$
Finally, in the likelihood Greeks wear hats:
$$\eqalign{
\ln L\left(\hat\beta_0,\hat\beta_1\,;\,Y_{i=1,\dots,N},X_{i=1,\dots,N}\right) &=\cr
\sum_{i=1}^N &{Y_i}\ln\biggl[\Phi\left(\hat\beta_0+\hat\beta_1 X_i\right)\biggr] + \bigl(1-Y_i\bigr)\ln\biggl[1-\Phi\left(\hat\beta_0+\hat\beta_1 X_i\right)\biggr].\cr
}$$

<a name="Fig21"></a>
<div class="alg">
<h4>Definition 21.  The log-likelihood function and ML estimates on the 2-Variable Probit </h4>
<DT>Given $Y_{i=1,...,N} \ {\buildrel  iid \over \sim}\   Probit\left(\beta_0+\beta_1 X_i\right)$, and $\hat\theta^{mle}{} = \pmatrix{\hat\beta^{mle}_{0}\cr\hat\beta^{mle}_{1}}$ is the MLE.</DT>
<OL class="compact">
<LI>The probit log-likelihood function is strictly concave, as long as $Y$ includes both some 0s <em>and</em> some 1s. Therefore,
 $\hat\theta^{mle}{}$ is <em>unique</em> and satisfy FONC.  Second order sufficient conditions are satisfied for all $\hat\theta$ and need not be checked.</LI>
<LI>There is no closed form solution to FONC for $\hat\theta^{mle}{}$ in part because they involve $\Phi(z)$ which has no closed form expression.</LI>
<LI>Despite this negative result, computer algorithms exist that will quickly find $\hat\theta^{mle}{}$ by maximizing $\ln L$ starting from any arbitrary vector $\hat\theta{}$.</LI>
<LI>$StdErr[\hat\theta^{mle}{}]$ is derived as discussed elsewhere: using the inverse of the Hessian matrix of the log-likelihood evaluated at $\hat\theta^{mle}{}$. This $2\times 2$ matrix can approximated very well and very easily using numerical derivatives.</LI>
</OL>
</div>



<h3><a name="s024"><LI>Economics &amp; Econometrics: An Example</LI></a></h3>

This briefly reviews a basic microeconomic model which generates either a linear regression or a probit depending on what aspect of the choice made in the model is observed in the data.

<h4>The Model</h4>

A person has a utility, $u(h,m)$, defined over time and money. Time $h$, is spent outside the labour market (leisure and home production).  Indifference curves represent the tradeoff between time and money, and the slope of the indifference curve is the instantaneous rate of tradeoff, the Marginal Rate of Substitution:
$$MRS(h,m) = -{u_h(h,m) \over u_m(h,m)},$$
where $u_h$ and $u_m$ are marginal utilities. The person is endowed with $T$ units of time. Leisure is restricted to be $h\le T$. Suppose this a weekly model and time is in hours, with a fixed 8  hours / day reserved for sleeping. Then $T= (24-8)7 = 112$ are the hours available for the labour market and the household.  If $h$ hours are spent at home then $T-h$ are the hours spend at work.</p>

If the person works positive hours ($h < T$) we say the person is <em>in the labour force</em>. If $h=T$ this person is <em>out of the labour force.</em> Money comes from two sources: (endowed) non-labour income $I\ge 0$ and Labour income which equals $w(T-h)$.  The person's wage $w$ is given and is also the slope of the budget line when $h$ is the x axis and $m$ the y axis. So
$$m = I + w(T-h).$$
Why "female"? The model is called the "female" labour supply model because in past generations there was much greater specialization in the division of labour.  Husbands worked in the labour market.  Wives may work or not depending on whether they had small children or other household duties.  So the amount of labour women supplied to the labour market varied much more than for men. A simple labour supply model is not a model of unemployment.  If the person wants to work they can at a wage $w$ per hour. In addition, a person might want to buy time: convert $I$ into more leisure.  But in this model this is not possible. In the old female labour supply model this would be the husband's income, and his wage and work hours were assumed to be determined prior to the decision of his wife.  But $I$ can also include any other income from government transfers, interest from assets, pensions, etc.</p>

<h4>Solutions</h4>

We can substitute for $m$ in utility to make the choice over one variable: choose the optimal hours to spend in the home:
$$v(w,I,T) = \max_{h\in [0,T]} u(h,I+w(T-h)).$$
Here $v()$ is the indirect utility afforded by the budget defined by $w$, $I$, and $T$. $h^\star(w; T,I)$ is "leisure demand". An <em>interior solution</em> satisfies
$$u_1(h^\star,I+w(T-h^\star)) - w u_2(h^\star,I+w(T-h^\star)) = 0.$$
Rearrange this to get a standard result:
$$MRS(h^\star,I+w(T-h^\star)) = w.$$
The limit on the time endowment means the optimal choice may not be an interior solution.  Instead $h^\star$ may be a corner solution: $h^\star(w; T,I)=T$.  If $I$ is large enough the person prefers not to work. Or if $w$ is small enough the person prefer not to work.  The MRS at the endowment point is called the <em>reservation wage</em>: $w^\star(I) = MRS(T,I)$.  Optimally the person will work if  $w> w^\star(I)$.  Otherwise, they stay home and out of the labour force.</p>

<a name="Fig11"></a>
<figure><h4>Exhibit 11.  Labour Supply and Labour Force Participation </h4><img  width="80%" src="img/lfs2.gif" /></figure>

<h4>The Empirical Version of the Labour Supply Model Can Be a nLRM2</h4>

Recall a model of work hours as a function of preferences, market wage and outside income. Assume we are modeling behaviour of people with $I$ not very large relative to the wage.  This will imply that they all work hours.   Consider
$$u(h,m) = h^{\alpha} m^(1-\alpha).$$
That is, nice and simple Cobb-Douglas preferences.  Then labour supply is $h^\star = \alpha T+.$  That is, full income is $w T$, and $\alpha$ is the share of income that is optimal to devote to leisure. So a Cobb-Douglas worker will work a constant fraction $\alpha$ of the time endowment independent of the wage.  This is a very strong implication.  More generally we might expect that hours would vary with wages:
$Y_i \equiv \ln h^\star_i$ and $X_i = \ln w_i$.  That is, regress log-hours on log wage. And let $\sigma z_i = \ln{\alpha_i}$ be the person's unobserved preferences for leisure.  </p>

If we assume that time endowments are the same across people, then $\beta_0 = \ln T$.  Notice that in that Cobb-Douglas specification we can write $Y_i = \beta_0 + 1\times X_i + \sigma z_i$.  So the coefficient on $X_i$ is not free.  Since Cobb-Douglas generates a price elasticity of 1 then the coefficient in this log-log specification is predicted to be 1.  But we might run the regression:
$$Y_i = \beta_0 + \beta_1 X_i + \sigma z_i.$$
Now a special case of this regression is the theory (or hypothesis) that preferences over time and money are <DD>Cobb-Douglas: $H_0: \beta_1 = 1$.  </DD>
In addition we could test the assumption that people act as if they have, say, $16\times 7 = T$ hours per week to allocate to work and leisure.  Then if the model is estimated on weekly hours of work and hourly wage that hypothesis is the restriction $\beta_0 = \ln(102)$.  The estimated variance $\hat\sigma^{mle}$ would determine how variable people in the sample are in their preferences over time and money. </p>

<h4>The LFP can produce a Probit</h4>

Consider a sample of $i=1,\dots,N$ people whose labour market activity has been collected. In the LFP model $w^\star(T,I)$ and $h^\star(w;T,I)$ are <em>endogenous</em> to the  person's preferences and budget. Let's assume $u()$ and $T$ are the same for everyone in our sample (so they don't have a <em>i</em> subscript.  People differ in $I$ and $w$. Assume that $w_i$ is unobserved (not a variable in the sample). We observe non-labor income $I_i$. We observe labour force status: $LFP_i =0$ when  person $i$ says they do not work (or at home or in school or &hellip;), and $=1$ if  <em>i</em> is working.</p>

And suppose person $i's$ wage is $\ln w_i = b_0 + b_1z_i$. Assume the MRS for the utility function $u()$ is such that  $\ln w^\star(I) = b^\star_0 + b^\star_1 I$.  We might expect $b^\star_1<0$(??).   The person works if $w_i > w^\star(I)$.  Since $\ln$ is a monotonic transformation (and both sides are positive), this is the same condition as $\ln w_i \gt \ln w^\star(I)$.</p>

Plugging in the model we get $LFP_i=1$ if $b_0 + b_1 z_i > b^\star_0 + b^\star_1 I_i$.  Or,
$$z_i>{b^\star_0-b_0\over b_1}+{b^\star_1\over b_1}I_i.$$
Notice this is starting to look like a probit model for who works and who stays home. We can map this version of the LFP model directly into the latent variable (binary response) version of the probit model:
<DD>$Y_{i=1,\dots, N} \ {\buildrel  iid \over \sim}\  Probit(\beta_0+\beta_1 X_i)$.</DD>
<DD>$X_i \qquad \mapsto  \qquad  I_i$</DD>
<DD>$\beta_0     \qquad \mapsto  \qquad  {b^\star_0-b_0\over b_1}$</DD>
<DD>$\beta_1     \qquad \mapsto  \qquad  {b^\star_1\over b_1}$</DD>
<DD>$Y^\star_i   \qquad \mapsto  \qquad  w_i - w^\star(I_i)$</DD>
<DD>$Y_i         \qquad \mapsto  \qquad  LFP_i$</DD>

<h3><a name="s025"><LI>MLE &amp; OLS on Two-Variable nLRM</LI></a></h3>

When applied to the normal linear regression model the method of maximum likelihood results in closed form solutions. That is, we get explicit formulas for the ML estimates.  And these formulas might look familiar</p>

With $Y_{i=1,\dots,N} \ {\buildrel  iid \over \sim}\   {\cal N}(\beta_0+\beta_1X_i,\sigma^2)$ define
$$\eqalign{s^2_X &\equiv \sum_{i=1}^N \bigl(X_i-{\bar X}\bigr)^2\cr
         s^2_{XY} &\equiv \sum_{i=1}^N \bigl(X_i-{\bar X}\bigr)\bigl(Y_i-{\bar Y}\bigr)\cr}$$
The first is sum of the squared deviations of the X variable from its mean.  It would be the sample variance of X except we are not dividing by $N-1$ to make it an average squared deviation.  The second term is numerator of the sample covariance between X and Y (again not dividing by the number of observations).  These two definitions will fall out of the first order conditions for MLE.</p>

The log-likelihood is the same as nLRM1 except the constant (over $i$) $\hat\beta$ is replaced by $\hat\beta_0 + \hat\beta_1X_i$, which is ${\hat E}[Y_i | X_i]$:
$$\ln L\biggl(\hat\beta_0,\hat\beta_1,\hat\sigma\ ;\ (Y_i,X_i)_{i=1,\dots,N}\ \biggr) = -{1\over 2}\left(N\left(\ln2\pi + 2\ln\hat\sigma\right)+{1\over \hat\sigma^2}\sum_{i=1}^N \left(Y_i-(\hat\beta_0+\hat\beta_1X_i)\right)^2\right).$$
To maximize lnL by choosing the parameter estimates we take the partial derivatives of this function.  The result is 3 equations in the three estimates.  We derive the ML estimators by solving the 3 FONCs.  It is left to the student to verify:
$$\eqalign{
\hat\beta^{mle}_{1} \quad&=\quad {\sum_i (X_i-\bar{X})(Y_i-\bar{Y}) \over \sum_i (X_i-\bar{X})^2}\quad =\quad
{ s^2_{XY} \over s^2_X}.\cr
\hat\beta^{mle}_0 \quad&=\quad {\bar Y} - \hat\beta^{mle}_{1} \bar X\quad =\quad {1\over N}\bigl(\sum_i Y_i + (\sum_i X_i) {s^2_{XY}\over s^2_X}\bigr)
\cr
\hat\sigma^{mle} \quad &=\quad   {1\over N}\sum_i \Bigl(Y_i - \bigl({\bar Y} + \hat\beta^{mle}_{1}(X_i-{\bar X})\bigr)\Bigr)^2  \cr
}$$

Then if we plug these formulas back in to the objective to get the express for the maximum log-likelihood.  This function is exactly the same as with nLRM1:
$$\ln L^U = -{N\over 2}\bigl(\ln2\pi + 2\ln\hat\sigma^{mle}+1\bigr).$$

<h5>OLS on the LRM2</h5>

The formulas above should look familiar!  The are (nearly) identical to the formulas for OLS estimates that tortured you in Econ 351.  How did this happen?
Recall that OLS stands for <em>ordinary least squares</em>.  This in term is short/technical talk for <q>choose estimates to minimize (least) the unweighted (ordinary) squared distance of $Y_i$ from the regression line.  This is the error in predicting $Y_i$ using the estimated regression:
$$e_i \equiv Y_i - \hat Y_i = Y_i - (\hat\beta_0 + \hat\beta_1 X_i).$$
In turn, OLS says to minimize the sum of squares of this, which we will call RSS (for Residual Sum of Squares):
$$RSS = \sum_{i=1}^N  e_i^2 = \sum_{i=1}^N \biggl( Y_i - \bigl(\hat\beta_0 + \hat\beta_1 X_i\bigr) \biggr)^2.$$
We can look at the log-likelihood function when we add the normality assumption (so that we have a complete probability model for the density of $Y_i$):
$$\ln L(\cdots) = -{1\over 2}\left(\cdots + {1\over \hat\sigma^2}\underbrace{\sum_{i=1}^N \left(Y_i-(\hat\beta_0+\hat\beta_1X_i)\right)^2 \atop \hbox{RSS}}\right).$$
There are other some additive terms and factors that enter the log-likelihood but when taking first order conditions in terms of the $\hat\beta's$ these components drop out.  It is important that a <em>negative</em> sign appears on RSS in the log-likelihood, because in MLE we are <em>maximizing</em> and in OLS we are <em>minimizing</em>.  In words, <em>minimize RSS</em> is the same problem as <em>maximize lnL</em> in the the normal Linear Regression Model.</p>


<h5>Properties of MLE (and OLS) on nLRM2</h5>

Note that if $\sum_i v_i = 0$, then $\sum_i v_i H_i = \sum_i v_i (H_i-\bar H)$.  That is, taking a weighted sum of a variable $H_i$ is the same as taking the weighted sum of $H_i-\bar H$ as long as the weights sum to 0. Then we can rewrite
 $$\hat\beta^{mle}_{1} = {\sum_i(X_i-\bar X)(Y_i -\bar Y) \over \sum_j(X_j-\bar X)^2} = \sum_i\biggl[ {{X_i-\bar X \over \sum_j (X_i-\bar X)^2}} (Y_i-\bar Y)\biggr].$$
This divides the denominator of the original expression into each term in the numerator. </p>

Then ${X_i-\bar X \over \sum_j (X_j - \bar X)^2}$ is like $v_i$ above, in that the terms sum to 0 across $i$.  This means that we can also write this as:
$$\eqalign{
\hat\beta^{mle}_{1} &= \sum_i{ { {X_i-\bar X \over \sum_j (X_j - \bar X)^2} } (Y_i-\bar Y)}\cr
                    &= \sum_i{ {{X_i-\bar X \over \sum_j (X_j - \bar X)^2} } Y_i}\cr
                    &= \sum_i \omega_i Y_i,\cr
where\qquad\qquad&\cr
\omega_i &\equiv {X_i-\bar X \over \sum_j (X_j - \bar X)^2}.\cr}$$
You can verify (or look up a derivation) that $\sum_i \omega_i = 0$ and $\sum_i \omega_i X_i = 1$.   These two properties of the weights play critical roles in the statistical properties of MLE on nLRM2.</p>

The last point means that $\hat\beta^{mle}_{1}$ is a weighted average of the $Y_i$ observations, and the weights $\omega_i$ depends on the X's (and are non-linear functions of X's). We can use linearity of expectation to compute the expected value and variance of $\hat\beta^{mle}_{1}$, <em>conditional on the values of $X_{i=1,\dots,N}$</em>:
$$\eqalign{ E[\hat\beta^{mle}_{1}\ |\ X_{i=1,\dots,N}]  &= E\biggl[\sum_i{ \omega_i Y_i \ |\  X_{i=1,\dots,N}}\biggr]  \cr
                                                        &= \sum_i \omega_i E[Y_i \ |\  X_{i=1,\dots,N}] \cr
                                                        &= \sum_i \omega_i \bigl(\beta_0+\beta_1 X_i \bigr) \cr
                                                        &= \sum_i \beta_0 \omega_i\ +\ \beta_1 \sum_i \omega_i X_i\cr
                                                        & = \beta_0 0 + \beta_1 1 = \beta_1.\cr}$$
The second line uses (PRE) stated in A1.  So it uses the probability model for $Y$ to write the conditional expectation of one MLE parameter as a function of the true population parameters.  Then the last line uses the facts above about the weights to show that in the nLRM2 model $\hat\beta^{mle}_{1}$ is unbiased. This immediately gives us unbiasedness in the ML estimate of the intercept:
$$\eqalign{
E[\hat\beta^{mle}_{0} \ |\  X_{i=1,\dots,N}] &= E[ \bar Y - \hat\beta^{mle}_{1}X_i  \ |\  X_{i=1,\dots,N}] \cr
                                             &= \beta_0 + \beta_1 {\bar X} - E[\hat\beta^{mle}_{1} \bar X \ |\  X_{i=1,\dots,N}]\cr
                                             &= \beta_0 + \beta_1 {\bar X} - \beta_1 {\bar X}
                                             &= \beta_0.\cr}$$
Thus, the estimates of the coefficients in the nLRM2 are <em>unbiased</em> estimators.  This generalizes the result that $\hat\beta^{mle}_{}$ is unbiased in nLRM1 and extends the very general result that MLE is consistent (hence asymptotically unbiased).  MLE applied to nLRM is unbiased in any sample size $N$ not just asymptotically for large $N$.</p>

We can use linearity in $Y_i$ to derive the variance and the standard error of the coefficient:
$$\eqalign{
Var[\hat\beta^{mle}_{1} \ |\  X_{i=1,\dots,N}] &= Var[\sum_i{ {X_i-\bar X \over \sum_j (X_j - \bar X)^2} Y_i} \ |\  X_{i=1,\dots,N}] \cr
&= \sum_i \omega_i^2 \sigma^2  = \sigma^2 \sum_i\left[{X_i-\bar X \over \sum_j (X_j-\bar X)^2}\right]^2 =
 {\sigma^2 \over \sum_j (X_j-\bar X)^2} = \sigma^2 / s^2_X \cr
 se[\hat\beta^{mle}_1] &= \sqrt{Var[\hat\beta^{mle}_1]}\cr}$$
The last two steps are left for the student to verify. We leave the rest of the results unproved.   Later matrix notation will allow us to derive all the variances and covariances of the coefficient estimates in a few steps.</p>

Because ML estimates of the coefficients are linear functions of the endogenous variable $Y$ we can derive their full distribution across possible samples.
<OL class="compact">
<LI>$\hat\beta^{mle}_{1} \sim {\cal N}(\beta_1,\sigma^2/ s^2_X)$</LI>
<LI>$\hat\beta^{mle}_{0} \sim {\cal N}\biggl(\beta_0,{\sigma^2 \sum_i X_i^2\over {Ns^2_X}}\biggr)$.</LI>
<LI>$Cov[\hat\beta^{mle}_0,\hat\beta^{mle}_{1}] = -\sigma^2{\bar X\over s^2_X}$.</LI>
<LI>$\left(N\hat\sigma^{mle}/\sigma\right)^2 \sim \chi^2_{N-2}$.</LI>
<LI>$Cov[\hat\sigma^{mle},\hat\beta^{mle}_0] = 0.$</LI>
<LI>The Hessian method for computing an estimate of the variance of MLE produces the same results, <b>after substituting MLE for the true parameters.</b></LI>
</OL>
</OL>
<h2><a name="s026"><LI>Multivariate Models</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s027"><LI>Matrix Essentials</LI></a></h3>
Here is a 3 x 2 matrix:
$$\pmatrix{1 & 2\cr  3 & 4\cr 5 &6}.\qquad (*)$$
We don't always have to write out a matrix in a grid like that. If there is a pattern in the entries then that means they follow some sort of formula.  So we can define a matrix by giving a formula for each element.  So
$A_{n\times m} = [ a_{ij} ]$ means <q>$A$ is the $n\times m$ matrix with elements $a_{ij}$,</q>.  This is the compact way of writing
$$A = \pmatrix{ a_{11} & a_{12} & \cdots &a_{1m} \cr a_{21} & a_{22} & \cdots & a_{2m}\cr
\vdots & \ddots & \cdots & \cr a_{n1}& a_{n2} &\cdots& a_{nm}}.$$
The matrix in (*) could be defined as $A_{3\times 2} = [j+2*(i-1)]$.

<DT>Matrix Operators</DT>
Given $A_{n\times m} = [a_{ij}]$ and $B_{r\times s} = [b_{ij}]$
<OL class="compact">
<LI>$A' =[a_{ji}]$.  (sometimes $A^T$)</LI>
<LI>$A+B = \left[a_{ij}+b_{ij}\right]$ (only if $r=n$ and $s=m$)</LI>
<LI>$AB = \left[\sum_{k=1}^m a_{ik}b_{kj}\right]$ (only if $m=r$) </LI>
<LI>With $n=m$, $A^{-1}$ is the matrix: $A^{-1}A = AA^{-1} = I_n$.</LI>
<LI><b>Determinant</b>: $\det\{A\}$ (also written $|A|$) is a function $\Re^{n\times n}\to \Re$} defined as:</LI>
<UL>
<LI>$n=1 \rightarrow |A| = a_{11}$</LI>
<LI>$n=2 \rightarrow |A| =  a_{11}a_{22}-a_{21}a_{12}$</LI>
<LI>$n=3 \rightarrow  |A| = +/-$ 6 products of 3 elements.</LI>
<LI>... $|A| = $ sum of products of $n$ elements of $A$.</LI>
</UL>
</OL>

<DT>Matrix Taxonomy</DT>
Given $A_{n\times m} = [ a_{ij} ]$.
<OL class="compact">
<LI>"$A$ is square" means $n=m$</LI>
<LI>When $n=1$ we say "$A$ is a row vector"</LI>
<LI>When $m=1$ we say "$A$ is a column vector"</LI>
<LI>When $i\lt j \rightarrow a_{ij}=0$ we say "$A$ is <em>lower triangular</em>"</LI>
<LI>"$A$ is diagonal" means $i\ne j \rightarrow a_{ij}=0$</LI>
<LI>When $a_{ij}=a_{ji}$ we say "$A$ is <em>symmetric</em>"</LI>
<LI>"$A$ is the identity matrix" means $n=m\ \&\ a_{ij}=\cases{ 1 & i=j \cr 0 & i<>j\cr}$</LI>
</OL>

<DT>Square Matrix Taxonomy</DT>
Given $A_{n\times n} = [ a_{ij} ]$.
<OL class="compact">
<LI>"$A^{-1}$" is the matrix such that $A^{-1}A = AA^{-1} = I$.</LI>
<LI>"$A$ is idempotent" means $AA = A$.</LI>
<LI>"$A$ is positive semi-definite" means for all $z$, $z'Az \ge 0$.</LI>
<LI>"$A$ is positive definite" means for all $z \ne 0$, $z'Az > 0$.</LI>
</OL>

<DT>Matrix Rank</DT>
Given $A_{n\times m} = [ a_{ij} ]$.
<OL class="compact">
<LI>Column $k$ is linearly dependent if there are scalars $c_j$ such that:
$c_k=-1$ and $\sum_{j=1}^m c_j a_{\circ j}$ = $\pmatrix{0 \cr \vdots \cr 0}$.  That is, the elements of column $k$ are each the same linear combination of corresponding elements of the other $m-1$ columns.  Otherwise, $k$ is linearly independent of the other columns.</LI>
<LI>$rank\{A\}$ is the <em>number of linearly independent rows</em> of $A$, which always equals <em>the number of independent columns</em>! If $rank\{A\}$ equals the minimum of n and m, then we say $A$ is <em>full rank</em>.</LI>
</OL>

<DT>Matrix Theorems</DT>
Given $A_{n\times m} = [ a_{ij} ]$.
<OL class="compact">
<LI>If $n=m$, then $A$ is <em>full rank</em> if and only if $\det\{A\}\ne 0$.</LI>
<LI>If $n=m$, then $A^{-1}$ exists if and only if $det\{A\}\ne 0$.</LI>
<LI>If $A$ is idempotent then $trace\{A\} = rank\{A\}$. (The trace of a matrix is the sum of the diagonal elements, $\sum_i a_{ii}$.)</LI>
<LI>If $n>m$ and $rank\{A\}=m$, then $\det\{A'A\} \ne 0$.</LI>
<LI>If $A$ is positive semi-definite then determinants of all of submatrices of $A$ are <em>non-negative</em>.</LI>
<LI>If $A$ is negative semi-definite then the determinants of submatrices of $A$ alternate in sign starting with <em>non-positive</em>.</LI>
</OL>


<DT>Random Vectors and Matrices</DT>

The formulas for OLS estimates of a simple two-variable regression model (with PRE
$Y_i= \beta_0+\beta_1X_i + \sigma z_i.$  are fairly simple and have an intuitive interpretation.   The estimate of the coefficient $\beta_1$ equals the sample covariance of $X$ and $Y$ divided by the variance of $X$.  (Both the numerator and denominator are divided by $N$ and this cancels to give the expression taught in Econ 351.)  The estimate of the intercept term is the sample mean of $Y$ minus the estimate of $\beta_1$ times the sample mean of $X$.</p>

On the other hand,the formulas for OLS estimates of a multivariate regression models are very complicated when expressed in terms of individual variables and summation operations ($\sum$). You may find matrix algebra dry or abstract, but matrix and linear algebra and powerful tools for expressing regression models. The notation restores simple and interpretable formulas for OLS results in the two-variable case.  To use this notation we need to use the idea of putting random variables inside matrices.

Let $A$ be a $n\times m$ matrix.  <span> $A$ is a <dfn id="ranmat">random  matrix</dfn> if  its are random variables.</span>  We have already seen random vectors: $\hat\theta^{mle}$ is an estimator and hence a random variable (across possible samples). If $\theta$ is a vector, like in nLRM1, then $\hat\theta^{mle}$ is a random vector. The <em>expectation of a random matrix</em> is the matrix of expectations:
$$E[A]\ \equiv\ \mu_A \equiv \pmatrix{ E[a_{11}] & E[a_{12}] &\cdots&E[a_{1m}]\cr
                                   E[a_{21]} & E[a_{22}] &\cdots&E[a_{2m}]\cr
                                   \vdots&\cdots&\ddots&\cdots\cr
                                    E[a_{n1}]& \cdots & \cdots & E[a_{nm}]}.$$
Let $v$ be a $N\times 1$ random vector.  The variance of $v$, is the $N\times N$ matrix of variances and covariances:
$$\mathop{Var[v]}\limits_{N\times N}\ \equiv\ \pmatrix{ Var[v_1] & Cov[v_1,v_2] & \cdots & Cov[v_1,v_N]\cr
                            Cov[v_2,v_1] & Var[v_2] & \cdots & Cov[v_2,v_N]\cr
                            \vdots & \vdots &\ddots &\vdots\cr
                            Cov[v_N,z_1] & Cov[v_N,z_2] &\cdots & Var[v_N]\cr
                            }$$
Matrix operations make it very easy to create this matrix as the expectation of a particular <em>outer product</em>:
$$\mathop{Var[v]}\limits_{N\times N}\
                \ =\ E\biggl[\ \bigl(v-\mu_v\bigr)\bigl(v-\mu_v\bigr)'\ \biggr]$$
We write $z \sim N(\overrightarrow{0},\mathop{I}\limits_K)$ to mean that
$z$ is a $K\times 1$ vector, each element of which is standard normal, $N(0,1)$ and the elements of $z$ are uncorrelated with each other.


<DT>The 2-variable Models in Matrix Form</DT>
You are used to writing the 2-variable linear regression in scalar notation:
$$Y_i = \beta_0 + \beta_1 X_i + \sigma z_i.$$
We can re-formulate this using matrix notation.  We associate <em>columns with variables</em> and <em>rows with observations</em>. That is, we want to be able to name and number variables.  Let $y$ be the $N\times 1$ vector with elements $Y_i$,
$$y_{N\times 1} \equiv [Y_{i}].$$
In the two-variable case there are two coefficients which we can put in a vector:
 $$\beta \equiv \pmatrix{\beta_0\cr\beta_1}.$$
In the two variable case there is only one variable on the right hand side so we can keep calling it $X$.
So we can write the observation $i$ right side variables of PRE as $x_{\circ i} = \pmatrix{1 & X_i \cr}$. We will use this notation later, but for the linear regression model we can capture all observations at once using matrix notation.  The right-hand-side data from the whole sample is a a $N\times 2$ matrix:
$$\mathop{X}\limits_{N\times 2} = \pmatrix{ 1 & X_1\cr 1 & X_2\cr\vdots&\vdots\cr 1&X_N}.$$
The disturbance term $z_i$ can be put into a column vector, $\mathop{z}\limits_{N \times 1}$.  Assuming normality, $z \sim N(\overrightarrow{0},\mathop{I}\limits_N).$
So the two-variable PRE for the whole sample can now be written:
$$y = X\beta + \sigma z.$$


<h3><a name="s028"><LI>Linear Expectation Models in Matrix Form</LI></a></h3>

In the two variable case we use $X$ for the variable (random or not) on the right hand side.  We used $X_i$ for the $i^{th}$ observation on $X$. Now that we are moving to matrix notation $X$ alone is the matrix of observations for all observations.  Individual columns of $X$ would be denoted $X_j$ and represent variables.
So $X_1$ will be the first variable on the right hand side, what we called $X$ in the two-variable model. Since a column is a variable, we switch the order of indices in usual matrix notation.  So  $X_{ij}$ is the value in the $i$ column and $j$ row, because we think of variables first. $X_1$ or maybe "Income" should be thought of as column headers in the browser window of a Stata data set.</p>

Multivariate models relate $Y$ to a set of $K_x-1$ variables, $X_1$ to $X_{K_x-1}$.  We will continue to focus on models of linear expectation.  That is a fancy way of saying that we will assume that <em>something</em> about $Y$ is a linear combination of the exogenous variables and the disturbance term:
$$\eqalign{
\hbox{Regression:}\qquad Y_i &= \beta_0 + X_{1i}\beta_1 + \cdots + X_{K_x-1\,i}\beta_{K_x-1} + \sigma z_i\cr
\hbox{Probit:}\qquad Y^\star_i &= \beta_0 + X_{1i}\beta_1 + \cdots + X_{K_x-1\,i}\beta_{K_x-1} + z_i
\cr}$$
The coefficients and observed variables combine to produce a $N\times 1$ vector of values across observations:
$$\pmatrix{
1\beta_0 + X_{11}\beta_1 + \cdots + X_{K_x-1\, 1}\beta_{K_x-1}\cr
1\beta_0 + X_{12}\beta_1 + \cdots + X_{K_x-1\, 2}\beta_{K_x-1}\cr
\vdots\cr
1\beta_0 + X_{1 N}\beta_1 + \cdots + X_{K_x-1\, N}\beta_{K_x-1}\cr}$$
We can write this as $X\beta$ if we define the $N\times K_x$ matrix $X$ as the concatenation of the variables as columns:
$$X \equiv \pmatrix{1 & X_1 & \cdots & X_{K_x-1}}$$
So the regression model for the whole sample
$$\pmatrix{Y_1 \cr Y_2 \cr \vdots \cr Y_N}
= \pmatrix{ 1 & X_{11} & \dots & X_{K_x-1\, 1}\cr
            1 & X_{12} & \dots & X_{K_x-1\, 2}\cr
            \vdots & \vdots & \dots & \vdots \cr
            1 & X_{1N} & \dots & X_{K_x-1\, N}\cr}
    \pmatrix{\beta_0\cr \beta_1 \cr \vdots \cr  \beta_{K_x-1}}
    +
    \sigma\pmatrix{z_1 \cr z_2 \cr \vdots \cr  z_N}
    $$
becomes simply
$$y = X\beta + \sigma z.$$

<a name="Fig22"></a>
<div class="alg"><h4>Definition 22.  Definition of $K_x$-variable Normal Linear Regression Model. </h4>
$y \ {\buildrel  iid \over \sim}\   N\left(\mathop{X}\limits_{N\times K_x}\beta,\sigma^2I\right)$
means:
<OL class="compact">
<LI>$y$ is a $N\times 1$ random vector. $X$ is a $N\times K_x$ matrix.  Each column of $X$ is a variable paired with $y$. $N>K_x$ and <b>$rank(X)=K_x$</b>.  Usually the first column of $X$ is a vector of $1$s.</LI>
<LI>$y = X\beta + \sigma z,\quad z \ {\buildrel  iid \over \sim}\  {\cal N}(\overrightarrow{0}_N,I)$, and the population parameters are $\beta$, a $K_x\times 1$ vector and $\sigma$, a scalar.</LI>
<LI>For all $i$, $Cov[x_{\circ i},z_i] = \overrightarrow{0}_{K_x}$.  That is, the <em>unobserved</em> term $z_i$ is <em>uncorrelated</em> with the vector of <em>observed (included) variables</em> for all $i$.</LI>
<LI>The parameter vector $\theta$ has length $K= K_x+1$ and is the coefficient vector with $\sigma$ appended:$\theta \equiv \pmatrix{\beta\cr\sigma}$.  Coefficients are not restricted: $\Theta = (-\infty,+\infty) \times (-\infty,+\infty)\times\dots\times (-\infty,+\infty)\times [0,+\infty)$.</LI>
<LI>The likelihood function is identical to the two variable case except $\hat\beta_0+\hat\beta_1X_i$ is replaced by the $i^{th}$ row of $X\hat\beta$, which equals $x_{\circ i}\hat\beta$.</LI>
</OL>
</div>

<a name="Fig23"></a>
<div class="alg"><h4>Definition 23.  The $K_x$-variable Probit Model (binary response version). </h4>
$y \ {\buildrel  iid \over \sim}\  Probit\left(\mathop{X}\limits_{N\times K_x}\beta\right)$ means:
<OL class="compact">
<LI>$y$ is a $N\times 1$ binary random vector. $X$ is a $N\times K_x$ matrix where each column represents a variable paired with $y$. $N>K_x$ and $rank(X)=K_x$.  Usually the first column of $X$ is a vector of $1$s.</LI>
<LI>$y^\star = X\beta + z,\quad z \ {\buildrel  iid \over \sim}\  {\cal N}(\overrightarrow{0}_N,I); \qquad
           Y_i  = \cases{ 0 & if $Y^\star_i \le 0$\cr 1 & if $Y^\star_i>0$.}\quad Y = Y^\star .> 0
           $</LI>
<LI>For all $i$, $Cov[x_{\circ i},z_i] = \overrightarrow{0}_{K_x}$.  That is, the unobserved term $z_i$ is uncorrelated with the vector of observed (included) variables.</LI>
<LI>The parameter vector $\theta$ has length $K= K_x$ and is simply $\beta$: $\theta\equiv \beta$. Coefficients are not restricted: $\Theta = (-\infty,+\infty) \times (-\infty,+\infty)\times\dots\times (-\infty,+\infty)$.</LI>
<LI>The likelihood function is identical to the two-variable case except $\hat\beta_0+\hat\beta_1X_i$ is replaced by the $i^{th}$ row of $X\hat\beta$, which equals $x_{\circ i}\hat\beta$.</LI>
</OL></div>


<h4>A Few Reasons for Multivariate Models</h4>

Perhaps it is obvious why we can't just stick with explaining $Y$ with $X$, but if not here are some reasons:
<UL>
<LI>To separate (disentangle) <em>the effects of two or more variables that are correlated with each other.</em></LI>
For example, suppose $Y$ is really related to $X$ and $V$ like this:
$$Y = \beta_0 + \beta_1 X + \beta_2 V + \cdots + \sigma z$$
Often economic policy advice or other insights into the world rely on knowing how $X$ directly effects $E[Y]$ holding $V$ constant. If we include only $X$, then the influence of the excluded variable ($V$) would be mixed in the the disturbance term.  If the new disturbance term $\epsilon^\star = \beta_2 V + \sigma z$ still satisfies the no-correlation with $X$ requirement we still get an unbiased estimate of $\beta_1$.
And if all we care about is $\beta_1$ then we don't really need to run a multivariate regression.  </p>

But if $X$ and $V$ are correlated with each other (and $\beta_2$ is not 0), then $\epsilon^\star$ is correlated with $X$.  And in most cases endogenous variables are correlated with each other. That means that $\hat\beta_1$ includes the effect of $V$ on $Y$.  We cannot disentangle the effect of excluded variables from the included ones (if there is correlation between them). So any statement (prediction, conclusion) about the effect of $X$ on $Y$ holding $V$ constant would require estimating a multivariate regression model.</p>

<LI>To estimate differences across <em>multiple groups</em> (such as across majors).  </LI>
If we include only one dummy variable:
 $$Y = \beta_0 + \beta_1 Econ + \sigma z$$
 we can't say anything about differences between other majors, only difference $E[Y]$ for Econ majors and non-Econ majors.  Instead, we could include a <em>set</em> of dummy variables for all (but one!) major:
 $$Y = \beta_0 + \beta_1 Econ + \beta_2 Psych + \beta_3 Phys + \cdots + \sigma z$$
where Psych is a column vector of 0s and 1s.  A 1 in row $i$ means the person is a psychology major.   If row $i$ has zeros for Econ, Psych and Phys then it means the person is a sociology major.  We can compare Psychology to Physics to Sociology, etc.</p>

<LI>To allow for <em>a more flexible relationship between a $X$ and $Y$</em> by including (non-linear) transformations of $X$ in the regression.  </LI>
That is, suppose in reality:
$$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \cdots + \sigma z.$$
$X$ and its square are not linearly related to each other, but they are almost always going to be correlated with each other. If $X$ is positive then more $X$ means more $X^2$ but not one-for-one (otherwise they would be linearly dependent).   The only way to allow for this non-linear relationship is to include both $X$ and $X^2$ in the regression.
</UL>

<h4>The rank condition.</h4>

<q>X has full rank</q> means that the columns of $X$ are linearly independent of each other.  They would be linearly dependent if one of the columns could be written as a weighted sum of the others.   Using $X_j$ to indicate the column vector for column $j$, linear dependence means that for some $j$ there are numbers $a_0, ..., a_{K_x-1}$ (skipping $a_j$) such that:
$$X_j = a_0 1 + a_1X_1 +\cdots+a_{j-1}X_{j-1} + a_{j+1}X_{j+1} +\cdots+a_{K_x-1}X_{K_x-1}.\qquad\hbox{(DEP)}$$
Notice this uses scalar multiplication of a vector. </p>

If (DEP) holds exactly, it creates computational problems (that reflect conceptual problems) for any statistical procedure attempting to estimate $\beta$.  Instead of $K_x$ separate variables there are really only $K_x-1$ or fewer.  Trying to infer (estimate) $K_x$ coefficients when there are really fewer that $K_x$ variables results in an <em>indeterminacy</em>. </p>

This problem is exactly the same as trying to solve $n-1$ linear equations in $n$ unknowns. If I wrote down
$$\eqalign{
3x + 2y &= 1\cr
6x + 4y &= 2\cr
}$$
I should expect to have trouble finding values of $x$ and $y$ that satisfy those equations, because it is really one equation written twice (slightly obscured by multiplying both sides by 2).</p>

While linear dependency of include X variables is a fundamental no-no, non-linear dependency between included X variables is <em>not</em> a problem. Return to the labour force participation example. We assumed everything was the same for all $i$ except income and a random (unobserved) wage:
$$Y^\star_i   = w_i - w^\star(I) = \beta_0 + \beta_1 I + z_i.$$
With a MV model we can include other variables. We now write $w_i$ as
$$w_i = b_0 + b_1 Age_i + b_2 Age^2_i + z_i$$
because we also observe the person's age.  This allows wages to rise with age but not at a constant rate.  To allow for changes in wage growth we include both Age and its square.  To run this probit we generate AgeSq = Age*Age and the probit is
$$LFP^\star_i = w_i - w^\star_i = \beta_0 + \beta_1 Age_i + \beta_2 AgeSq_i + \beta_3 I_i + z_i$$</p>

There is a dependency between two columns X: one is the square of the other.  However, this is not a <em> linear</em> dependency. A multiple of Age does not equal Age*Age (unless Age only takes on the values 0 and 1!).  Thus, multivariate models can capture the simultaneous relationship between $Y$ and several explanatory variables and non-linear relationships between $Y$ and a single variable.</p>

To see why no exact linear dependency among included X variables is a fundamental problem, suppose $Y_i=$ whether person $i$ suffers a heart attack in a year's time. A researcher looks at the data he has in an Excel spreadsheet and writes down a probit model. So $X\beta$ might represent a latent (unobserved) measure of cardiac disease perhaps corresponding to the blockage in the arteries near the heart.  We add the random element $z$ to capture whether the person has unusual stress or other factors that would combine with their arteries to cause an attack. $\cdots$ means that $X$ might contain many other variables we can observe about the person.
$$Y^\star = \beta_0 + Female\beta_1 + WeightKG\beta_2 + WeightLB\beta_3 + \cdots + z.\qquad\hbox{(*)}$$
What is wrong with (*)?  Maybe this is a Canadian researcher who has one foot in metric and one foot in English, so their spreadsheet includes weight <em>in both kilograms (KG) and pounds (LB)</em>.  Could we ever separate out the effect of weight in kg on heart attacks from an effect in pounds?  No, because they move in lock step together so that there is no way to disentangle (separate) their influences.  This exact linear dependence between columns means <em>$rank(X)\lt K_x$.</em></p>

This dependency means that (*) is identical to a model with one less free population parameter:
$$\eqalign{
Y^\star &= \beta_0 + Female\beta_1 + 2.2*WeightLB\beta_2 + WeightLB\beta_3 + \cdots + z\cr
        &= \beta_0 + Female\beta_1 + WeightLB\bigl(2.2*\beta_2 + \beta_3\bigr) + \cdots + z\cr
        &= \beta_0 + Female\beta_1 + WeightLB\beta_2^\star + \cdots + z\cr
}$$
Where there were two free parameters, $\beta_2$ and $\beta_3$, now there is just one, $\beta^\star_2$.  If the researcher does not notice this and tries to estimate $\beta_2$ and $\beta_3$ the software package will say (in one way or the other) that it is impossible.</p>

<h4>MLE on the nLRM in Matrix Notation.</h4>

Define: $\hat{y} \equiv X\hat\beta$ and $e \equiv y - \hat{y}$ as the $N\times 1$ vectors of predicted values and prediction errors for coefficient estimates $\hat\beta$.
<UL>
<LI>Then using matrix operations:
$$\eqalign{
\ln lk\bigl(\ \hat\beta,\hat\sigma\ ;\ y,X\ \bigr) &= -{1\over 2}\left(N\ln2\pi + 2\ln\hat\sigma+{1\over \hat\sigma^2}\sum_{i=1}^N \left(Y_i-x_{\circ i}\hat\beta\right)^2\right)\cr
&=-{1\over 2}\left(N\ln2\pi + 2\ln\hat\sigma+{1\over \hat\sigma^2}\left(y-X\hat\beta\right)'\left(y-X\hat\beta\right)\right)\cr
&=-{1\over 2}\left(N\ln2\pi + 2\ln\hat\sigma+{e'e \over \hat\sigma^2}\right)\cr
}$$</LI>
<LI>Maximizing over $\hat\beta$ results in $K_x$ first order conditions: $(X'X)\hat\beta^{mle} = X'y.$ When $X$ satisfies the full-rank condition ensures that $X'X$ is <em>invertible</em>:
$$\hat\beta^{mle} = \left(X'X\right)^{-1} X'y.$$</LI>
<LI>Therefore
$\hat y^{mle} = X \left(X'X\right)^{-1} X'y \qquad$ and $e^{mle} = \bigl(I-X \left(X'X\right)^{-1} X'\bigr)y$.
</LI>
<LI>As usual $\hat\sigma^{mle} = \sqrt{ {e^{mle}}'e^{mle} / N }$, and $\ln lk_U$ is identical to the formula given earlier for nLRM2.</LI>
</UL>


<h4>OLS in Matrix Notation.</h4>

To derive OLS estimate of $\beta$ it is not necessary to assume that $z$ is a <em>normal</em> random vector.  Instead, the assumptions of the Gauss-Markov Theorem only require the other assumptions above: that $z$ is uncorrelated with $X$: $E[z | X] = E [ X ] = \overrightarrow{0}$, and $Var[z] = I_N$, so that $Var[\sigma z] = \sigma^2 I.$  In words, the disturbance terms have no serial correlation across observations and are homoscedastic (constant variance).</p>

OLS is the solution to the problem of minimizing $e'e$ where $e$ is the vector of prediction errors defined above. Because of the exact way $e'e$ shows up in the log-likelihood of function for the <em>normal</em> LRM the OLS estimates are identical to the MLE estimates of $\beta$:
$$\hat\beta^{ols} = \left(X'X\right)^{-1} X'y.$$
Predictions and errors are also the same:
$$\eqalign{
    \hat y^{ols} &= \hat y^{mle} = X \left(X'X\right)^{-1} X'y \equiv M y\cr
    e^{ols} &= e^{mle} = \bigl(I-X \left(X'X\right)^{-1} X'\bigr)y = (I-M)y.\cr}$$
Note that $M$ is defined as $X(X'X)^{-1}X'$. That matrix only depends on the right-hand-side values $X$.  And although complicated looking it is still simply a $N\times N$ matrix.  It is the matrix that takes observed values of the $Y$ variable and maps them into the corresponding predicted values, the values on the regression <q>line</q>, although with multiple explanatory variables it is not really a line in two dimensions anymore. </p>

The OLS estimate of $\sigma$ is not based on first order conditions like the MLE estimate.  That estimate is a <em>consistent</em> estimate of $\sigma$  but it is biased.  The OLS estimate is defined as the formula that results in an unbiased estimate (of $\sigma^2$ not its square root):
$$\hat\sigma^{ols} = \sqrt{ {e^{ols}}'e^{ols} / (N-K_x) }.$$
The form of the estimates means this expression can be further manipulated:
$$\eqalign{
e'e &= \left(y-X(X'X)^{-1}X'y\right)'\left(y-X(X'X)^{-1}X'y\right)\cr
     &= y'\left(I-M\right)' \left(I-M\right)y \cr
     &= y'\left(I-M\right)y \cr}$$
It is left to the student to verify that the matrix $I-X(X'X)^{-1}X'$ is symmetric and idempotent so that the last stop follows. Thus,
$$\hat\sigma^{ols} = \sqrt{ {y'\left(I-M\right)y \over N-K_x} }.$$
As a random vector $\beta^{ols}$ has a symmetric variance matrix containing the variances and covariances of all the estimates.
$$\eqalign{
Var[\hat\beta^{ols})] &= Var\left[ \left(X'X\right)^{-1} X'y \right]\cr
 &= Var\left[\left(X'X\right)^{-1} X'(X\beta+\sigma z)\right]\cr
 &= Var\left[ \beta + \left(X'X\right)^{-1}\sigma z\right]\cr
 &= Var\left[ \left(X'X\right)^{-1}X' (\sigma z)\right]\cr} $$
Since (X'X) gets wipes by its inverse we see that the variance of the estimates is the variance of the linear transformation of the errors $z$. The next step uses the <em>outer product</em> definition of a variance (see exercise above).  In particular, if $v$ is a random vector and $A$ is a matrix then $Var[Av] = AVar[v]A'.$  So
$$\cdots = (X'X)^{-1}X'\ Var[\sigma z]\ \left\{ (X'X)^{-1}X'\right\}'.$$
Under the assumptions of the Gauss-Markov Theorem, $Var[\sigma z] = \sigma^2I$.  Since $\sigma$ is a scalar its position in the product can move around.  So the $\sigma^2$ can moved to the front.  Further, the identity matrix simply gets absorbed in the product (i.e. $X'IX = X'X$), so
$$ \cdots = \sigma^2 (X'X)^{-1}X'\ X (X'X)^{-1}.$$
Again, another inverse cancels out the inside $X'X$ and the result is:
$$Var[\hat\beta^{ols}] = \sigma^2 (X'X)^{-1}.$$
We then estimate the variance over samples by replacing the parameter $\sigma$ with our estimate:
$$\hat{Var}[ \hat\beta^{ols}] = \hat\sigma^2 (X'X)^{-1}.$$
Remember this is a $K_x \times K_x $ matrix of variances and covariances.  The vector of standard errors is the square root of the diagonal of this matrix.

<h3><a name="s029"><LI>Likelihood Ratio Tests</LI></a></h3>

Previous classes discussed the main ideas behind a statistical test of a hypothesis.  The particular test you have focussed on most are t-tests within the linear regression model, but Z tests, F tests and possibly $\chi^2$ tests have also shown up. The intuition behind the (two-sided) t-test is fairly simple: Estimate the parameter from the data ignoring the null hypothesis; Compute how far the estimated parameter is from the hypothesized value, or in the case of the F-test how much different the unexplained variation is. If it is far enough away, reject the null hypothesis.  Otherwise fail to reject the null.  The standard error in the t-test is needed to control for how far estimates of the parameter would likely be from the hypothesized value even when the null hypothesis is true. t-tests can be used in MLE just as in procedures covered in Econ 250 and Econ 351. The difference is the formula for the denominator in the t-test, the estimated standard error of the estimate, but in practice read off the estimated standard error from Stata output just as usual. </p>

What is new here?  We discuss a different test and how to conduct when using MLE.  The idea is different from the t-test in two respects (and is more like the F-tests for multiple hypotheses covered in Econ 351).   The first difference is that the hypothesis can involve any number of parameters (and need not be linear combinations of parameters as in the F test).  The second is that in this approach you still ignore the null hypothesis in one stage, but we don't just see how far the estimates are from the hypothesis.  Instead, we estimate the model a second way assuming (or requiring) the null hypothesis holds.  Then, instead of seeing if the parameter values are "big enough" to reject the null, we look to see if the match to the data is "different enough" to reject the null. </p>

A <span><dfn id="restriction">restriction or restricted probability model</dfn> is a subset of the parameter space, ${\cal R}\subset \Theta$.</span> For example, in the Bernoulli model with the single parameter $\theta$, one restriction might be that the coin be "fair", which we can get all fancy about and write ${\cal R} = \{ 1/2 \}$. In the two variable regression model with parameter vector $(\beta_0,\beta_1,\sigma)$, a theory that says $E[Y|X]$ is positively related to $X$ is the restriction
$${\cal R} = \{ (\beta_0,\beta_1,\sigma) : \beta_1 > 0\}.$$The fewer points in the restriction the tighter or more restrictive it is.

<a name="Fig12"></a>
<figure><h4>Exhibit 12.  Restriction on the Parameter Space </h4><img width="80%" src="img/RestrictedParameterSpace.png" alt="A Restriction on the Parameter Space"/>
<figcaption>In this example, the model has two parameters, $\theta_1$ and $\theta_2$. The set of parameter values that make up the parameter space, $\Theta$ is shown as a parallelogram, just to illustrate the idea that $\Theta$ is not necessarily an interval or some other simple set.  The curved line might be a restriction implied by some <q>theory</q>.</figcaption></figure>

The parameter space is determined by which values of $\theta$ produce a well-defined probability model, and in this case the combinations of $\theta_1$ and $\theta_2$ that work depend on each other. This means the parameter space has two dimensions ($K=2$).  To know where I am inside $\Theta$ I need to know the value of both $\theta_1$ and $\theta_2$.  The true parameter vector has to be a point in $\Theta$, but some crazy theory says that it is actually within  the subset ${\cal R} \subset \Theta$. The subset is a one-dimensional ($R=1$) because there is really only one free parameter within $\cal R$.  If you give me $\theta_1$ then to be in $\cal R$ I have no choice about $\theta_2$.   This is a subtle point because $\cal R$ is a set inside two dimensions but is only one dimensional because there is only one "free" parameter to determine where in $\cal R$ the element is.</p>

A <span><dfn id="rmle">restricted maximum likelihood estimator</dfn> is the value of $\hat\theta$ that maximizes the likelihood over a restriction ${\cal R}$.</span> Mathematically, a Restricted MLE solves:
$$\max_{\hat\theta \in {\cal R}}\qquad \ln L (\hat\theta).$$
Restricted MLE can be called $\hat\theta^{mle}_R$, and the maximum of the log-likelihood on the set ${\cal R}$ called $\ln L_R \equiv \ln L(\hat\theta^{mle}_R)$.If ${\cal R}$ is a null hypothesis to be tested, then <b>the restricted MLE is the estimate as if $H_0$ is TRUE.</b>.</p>

We know that the maximum unrestricted likelihood must be greater than the restricted likelihood: $\ln L_U \ge \ln L_R$.  Why? The maximum over  subset of choices can be no bigger than the unrestricted maximum.  The best case for the restriction is when the data are "free to choose" any estimate in the whole space yet they still choose a parameter in the restricted set: if $\hat\theta^{mle}_{U} \in {\cal R}$ then $$\hat\theta^{mle}_{U}=\hat\theta^{mle}_{R} \hbox{ and } \ln L_R=\ln L_U.$$
Otherwise, when given the choice maximizing the likelihood the data will do better in the unrestricted version then the restricted version.  And in this case the restricted likelihood is less than than the unrestricted likelihood.</p>

Even if ${\cal R}$ is TRUE (so $\theta \in {\cal R}$) it may be that $\hat\theta^{mle}_{U}\not\in{\cal R}$. This is like making a Type I error. This discrepancy between the estimate and the true parameter might be attributed to <q>sampling variation</q>: The data in sample do not look exactly like the population, and that difference can lead MLE to values that don't satisfy the restriction even though the restriction is true in the population.</p>

Statistical Consistency of MLE as a procedure means that with a large enough sample size this sampling variation disappears (although the $N$ that does this can be very large compared to the size of the sample, just as with other tests you have seen). On the other hand, if the restriction is FALSE it is still possible $\hat\theta^{mle}_{U}\in {\cal R}$ This is like making a Type II error. So the restriction does not matter in the sample we have, even though in the population the restriction is violated. Again, statistical consistency of MLE means a large enough sample will eliminate this kind of error (but the required $N$ may be astronomical).</p>

If $\theta$ is far enough from ${\cal R}$ in the parameter space or if we just happen to draw a really unusual sample, we will see a "big" difference between the restricted and unrestricted models.  If the difference is "big enough" we should abandon the restriction as a theory and say that the <q>data reject the hypothesis</q>
$H_0:\theta\in{\cal R}$.</p>

How to summarize the difference between the restricted and unrestricted models? The key is computing a measure of the difference between the restricted and unrestricted models' agreement with the data. Then we need a theory for how big the difference might be even if the restriction is true (i.e. we need to control for the probability of a Type I error just as you learned to do with t-tests). Below we first define the measure of agreement with the data then we state the theory for its distribution if the theory is true.  And we give some intuition for why this makes sense.</p>

<a name="Fig24"></a>
<div class="alg"><h4>Definition 24.  The (strangely named) Likelihood Ratio Statistic </h4>
Given a probability model, the unrestricted MLE $\hat\theta^{mle}_{U}$, and the corresponding likelihood $\ln L_U$ of the probability model. And given a restriction $R$ on the parameter space, which can also be stated as a hypothesis:
 $$H_0 : \theta \in {\cal R}.$$
For this hypothesis we have computed the restricted MLE $\hat\theta^{mle}_{R}$ and corresponding likelihood $\ln L_R = \ln L(\hat\theta^{mle}_{R})$ for the hypothesis/restriction.  Then the <em>likelihood ratio</em> is defined as
$$LR \equiv 2\bigl(\ln L_U - \ln L_R\bigr)\qquad\qquad (LR)$$
</div>

Why call a <u>difference</u> a <u>ratio</u>? LR starts with the ratio of the likelihoods squared ($(L_U/L_R)^2$).   Taking the natural logarithm of this squared ratio brings the 2 down from the exponent, and the ratio becomes a difference in logs. That is,
$$\ln\bigl(\left({L_U/L_R}\right)^2\bigr) = 2\times\bigl(\ln L_U - \ln L_R\bigr).$$
LR is never negative. It is zero when $\hat\theta^{mle}_{U}=\hat\theta^{mle}_{R}$. Otherwise, LR is positive, and <u>the farther from zero the more the data seem to go against the restriction being true</u>. We use knowledge from statistical theory of how much variation in LR we should expect if the restriction is true.  If the computed LR is <q>big enough</q> we conclude the restriction/hypothesis is not true ($\theta\not\in {\cal R}$). To know what is big enough we have to know the distribution of LR when the null is true.</DD>

In the LR test, DoF is a difference in the number of free parameters.
<DD>$K \equiv$ Number of Parameters in the Unconstrained Model.</DD>
<DD>$R \equiv$ Number of (free) Parameters in the Constrained Model.</DD>
<DD>So $DoF = K-R$ is <u>the number of restrictions the hypothesis (theory) puts on the likelihood maximization.</u></DD>

A Contradiction? That sounds strange: <q>more restriction</q> $\Rightarrow$ <q>more freedom</q> But DoF is the <u>extra freedom</u> gained by <u>NOT imposing the theory</u>.  All else constant, <q>greater DoF</q> = <q>a more restrictive theory</q>. </p>

For example, in the Bernoulli model: $K=1$. Since  $R\le K$, DoF is either 1 or 0. We will <u>not</u> discuss tests involving DoF=0.  So in a one-parameter model like Bernoulli we only consider $R=0$, which means that the restriction is <u>a single point</u>:
$${\cal R} =\{\theta_R\}$$
That is, in this restricted model there is no choice in the matter. So a LR test in the Bernoulli model would be summarized as:
$$\hat\theta^{mle}_{R}\ =\ \theta_R \qquad \ln L_R\ =\ \ln L(\theta_R) \qquad LR = 2(\ln L_U - \ln L(\theta_R)).$$

<a name="Fig25"></a>
<div class="alg"><h4>Definition 25.  Distribution of the Likelihood Ratio </h4>
<DT>Let $\theta$ have $K$ parameters and suppose the null hypothesis is $H_0: \theta\in {\cal R}$ where ${\cal R}$ has $R$ free parameters. If $H_0$ is true, which means $\theta\in\theta_R$, then</DT>
$$LR\ \ {\buildrel D \over \to}\ \ \chi^2_{K-R}.$$
</div>

That is, <em>the asymptotic distribution</em> of LR is $\chi^2_{K-R}$ when the <u>restriction is true</u>. Although LR is not guaranteed to be exactly chi-squared for finite values of N, you can have some confidence that in most models it is as a good approximation.  This result rests on the earlier result that $\hat\theta^{mle}$ is asymptotically normally distributed.

<a name="Fig26"></a>
<div class="alg">
<h4>Definition 26.  Recipe for Conducting a Likelihood Ratio Test </h4>
<DT>Given a probability model with parameter vector $\theta$ that has $K$ free parameters.</DT>
<OL class="steps">
<LI>Compute $\ln L_U = \ln L(\hat\theta^{mle}_{U})$, the unrestricted log-likelihood over the whole parameter space $\Theta$.</LI>
<LI>State a null as a restriction: $H_0: \theta \in {\cal R}$.  Usually this can be expressed as one or more equations that the parameters satisfy, although the equations do not have to be linear in the parameters. </LI>
<LI>Set DoF = $K-R$ where $R$ is the number of free dimensions under the null. Estimate $\hat\theta^{mle}_{R}$ by imposing the restriction on the parameters and compute $\ln L_R = \ln L(\hat\theta^{mle}_{R})$.  (This could be difficult if the restrictions are not simple because it would require solving a constrained optimization problem numerically.) </LI>
<LI>The alternative is everything else: $H_A = \theta \not\in {\cal R}$.  Given your $\alpha$, find the critical chi-squared value, $X^\star$: $Prob\bigl( \chi^2_{K-R} > X^\star \bigr) = \alpha$.</LI>
<LI>Compute the test statistic <b>LR = $2(\ln L_U - \ln L_R)$</b>  <em>Reject</em> $H_0$ if $LR> X^\star$.  Otherwise <em>fail to reject</em> $H_0$.</LI>
</OL>
</div>
<h3><a name="s030"><LI>Tests of Overall Significance</LI></a></h3>

<H4>Likelihood Ratio Test of Overall Significance</H4>

A reminder: when we write down and then estimate a multivariate model for a variable $Y$ we are typically trying to <em>explain</em> some of the variation in $Y$ that we see in the data.  If $Y$ did not vary, if $Y_i = \bar Y$ for all observations then there would be no variation to explain.  If there is variation in $Y$ then the estimated model will explain some of the variation as coming from the relation between $Y$ and the chosen variables on the right-hand side.  That is, as the X's vary with $i$ their variation is correlated with how $Y$ is varying across $i$.  If all of the estimated coefficients were exactly 0, if $\hat\beta_k=0$ for all $k$ except the $0$ (the constant term), then the estimated model is saying that the X variables do nothing to explain $Y$.  If some or all of the estimate coefficients are not exactly zero then the model is saying there is some relationship between $Y$ and the $X$ variables in the sample.</p>

With multiple variables on the right hand side to explain $Y$ it becomes important to determine the overall power of your model to explain variation in $Y$. The absence of explanatory power is capture by this hypothesis:
$$H_0^\dagger: \beta_1 = \beta_2 = \cdots = \beta_{K_x-1} = 0.$$
$\dagger$ is used to denote this special null (but this is not general notation). Under $H_0^\dagger$ the left hand side of the model becomes $\beta_0 + z_i$ (for probit) or $\beta_0+\sigma z_i$ (for nLRM).  Either way, the probability model under $H_0^\dagger$, that is if the null hypothesis is the true, becomes the one-variable version we started with. The expectation of $Y_i$ conditional on $x_{\circ i}$ is not a function of $x_{\circ i}$.  The hypothesis excludes $\beta_0$, which is still unrestricted. It is the coefficient on the constant term, the right-hand side variable that does not vary.  So $\beta_0$ picks up or explains the mean of $Y$ in the data.  (It is possible that someone might be interested in the hypothesis that combines $H_0^\dagger$ and $\beta_0 = 0$, but typically $Y$ is a variable for which there is no reason the intercept term should also be 0.)</p>

The alternative hypothesis to test is any set of parameters that does not satisfy $H_0^\dagger$.   In words, the alternative is that at least one of the parameters $\beta_{k, k>0}$ is not equal to zero.
$$H_A^\dagger: \beta_k \ne 0, \hbox{for one or more coefficients k,} k>0.$$
The alternative includes any point in the parameter space where some combination of the parameters are non-zero. </p>

When using MLE to estimate the parameters, the restricted log-likelihood under $H_0^\dagger$ is the unrestricted MLE for the one-variable model.  For either probit (Bernoulli) or regression (1-variable nLRM) the log-likelihood is a simple formula.
$$\ln L^\dagger_{R} = \cases{  S \ln\bigl({\bar Y}\bigr) + (N-S)\ln\bigl(1-{\bar Y}\bigr) & Probit-to-Bernoulli\cr
\cr
    -{N\over 2}(\ln 2\pi + 2\ln \hat\sigma_Y +1) & nLRM-to-nLRM1\cr }$$
The degrees of freedom for a test of $H_0$ against $H_A$ is determined as usual for a likelihood ratio test. The restricted model has $K_x-1$ fewer free parameters that the unrestricted model.  So the degrees of freedom of a test of $H_0^\dagger$ is $K_x-1$. Therefore,
$${LR}^\dagger = 2*(\ln L_U - \ln L_R^\dagger)$$
will follow the $\chi^2_{K_x-1}$ distribution if $H_0^\dagger$ is true. If we can reject $H_0^\dagger$ in favor of $H_A^\dagger$ then we conclude that collectively the $X$ variables do help explain the values of $Y$.

<H4>F Test of Overall Significance in OLS</H4>

The logic of the F test when using OLS is explained in any econometrics text book, and should be included in the text used in Econ 351 or equivalent.

</OL>
<h2><a name="s031"><LI>Beyond IID Sampling &amp; Regression/Probit</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s032"><LI>Heteroscedasticity &amp; Robust Standard Errors &amp; Clustering </LI></a></h3>

Some of the assumptions in the Gauss-Markov Theorem (GMT) are illustrated in this diagram:
<img src="nLRM5.png"/>
One assumption is the linear relationship between $X$ variables the <em>conditional mean</em> of $Y$, implied by the regression equation.  Another assumption is that the actual value of $Y$ includes the conditional mean and a disturbance to the linear relationship, which we have been denoting $\sigma z$ and is often called $\epsilon = \sigma z$.  The GMT does not require that the disturbance terms are normally distributed as the rotated bell-curve on the diagram suggests.  What is important to the GMT is first, that the mean of the disturbance is zero: \medskip
[MN] $E[\epsilon | X]=0$;\medskip
second, the variance of the disturbance term is the same for all observations, 
[VAR] $Var[\epsilon] = \sigma^2$;\medskip
 and third 
[COV] $Cov(\epsilon_i,\epsilon_j)=0$ for $i\ne j$.  \medskip
That is, there is a single number $\sigma$ which is the standard deviation of the true but unobserved disturbance terms.  In words, the GMT assumes that we expect observed values of $Y$ to be the same vertical distance from the regression line for all observations; the disturbances are on average zero: positive disturbances are expected to be balanced with negative disturbances; and the value of one disturbance of observation $i$ has no influence on other disturbances $j$.</p>

However, these two assumptions [MN] and [Var] on the disturbance terms have very different roles in the GMT.  In particular, you might remember the following partial result from the two-variable model, so $X$ is a single variable with coefficient $\beta_1$:
$$Two Variable Model:\qquad E[\hat\beta^{ols}_1] = \beta_1 + { Cov[\epsilon,X]\over Var[X]}.$$
If [MN] is true then for all observations then the covariance of the disturbance term and $X$ is zero.  The last term disappears and we get the result that OLS is unbiased.  As with other OLS formulas there is a matrix version that is reminiscent of the the two-variable version:
$$Multivariate Model:\qquad E[\hat\beta^{ols}] = \beta + \left(X'X\right)^{-1}X' E[\epsilon|X].$$
Here we can see that $1/Var[X]$ is analogous to $(X'X)^{-1}$.  And $Cov[\epsilon,X]$ is closely related to $E[\epsilon | X]$.</p>

Either way, this result that OLS is unbiased does not rely on [Var] or [COV]  at all.  The role those assumptions is not in the U part of "BLUE" but in the "B" part through the variance of the estimator over samples.  We already derived the formula under the GMT assumptions.
$$\eqalign{
Var[\hat\beta^{ols})] &= Var\left[ \left(X'X\right)^{-1} X' \epsilon \right]\cr 
                      &=  \left(X'X\right)^{-1} X' Var[\epsilon] X \left(X'X\right)^{-1}\cr
                      &=  \left(X'X\right)^{-1} X' \sigma^2 I X \left(X'X\right)^{-1}\qquad\quad \hbox{Using [VAR] and [COV]}\cr
                      &=  \sigma^2\left(X'X\right)^{-1}\cr}$$
That expression is the smallest variance that is possible for any linear unbiased estimator under the assumptions.  We can estimate the variance matrix by replacing the one parameter $\sigma$ that appears in the expression with an estimate.  For OLS,
$$\hat\sigma^2 = {e'e \over N-K}.$$
So the formula is correct only if $Var[\epsilon] = Var_{hom}[\epsilon] = \sigma^2 I$, which is the matrix expression that corresponds to [VAR] and [COV]. Heteroscedasticity is the fancy term for disturbance terms that have different variances.  An example is the figure below where the rotated bell curves have different spreads for different values of $X$.
<img src="img/nlRMhetero.png"/>
If the expected distance from the regression line is not the same for each observations then OLS is still unbiased but the formula for the estimated $Var[\hat\beta^{ols}]$ is misleading.  If [VAR] and [COV} do not hold then our deviation of the formula gets stopped at this step:
$$Var_{sand}[\hat\beta^{ols})] = \underbrace{ \left(X'X\right)^{-1} X' \atop Top Bun} \underbrace{ Var[\epsilon] \atop Filling} \underbrace{ X \left(X'X\right)^{-1}\atop Bottom Bun}.$$
This became known as the <q>sandwich</q> expression because the true variance matrix of the disturbance terms is sandwiched by a matrix on the left and its transpose on the right. </p>

Two related ways to relax the assumptions of [VAR] and [COV] rely on the sandwich formula.  The first is called <em>robust standard errors</em>.  The idea is pretty simple.   Let's continue to maintain [COV] so that the off-diagonal elements of $Var[\epsilon]$ are 0 and each disturbance is uncorrelated with all other disturbances.  What's left in the matrix are the diagonal elements.  Under [Var] all the diagonal elements equal $\sigma^2$.  The most general alternative is that each element is a (potentially) different parameter, $\sigma^2_i$:
$$Var_{het}[\epsilon] =  \pmatrix{ \sigma^2_1 & 0 & 0 &\cdots & 0\cr
                            0 &\sigma^2_2 &0 & \cdots &0\cr
                            0 & 0 &\ddots &\cdots &0 \cr
                            0 & 0 &0 &\ddots &\cdots \cr
                            0 & 0 &0 &\cdots &\sigma^2_N \cr}. $$
If we had estimates of each of these observation-specific variances we could plug them into the sandwich formula.  Then the estimate of how the OLS parameters would vary across samples would not rely on any assumption about the disturbance variances. </p>
What information do we have to estimate $\sigma^2_i$.  Recall that is the expected square distance of the $Y_i$ observation from its expected mean, $\epsilon_i = Y_i - E[Y_i | x_{\dot i} ] = x_{\dot i}\beta $.  By definition, $E[\epsilon^2_i] = \sigma^2_i$.  We estimate this difference with $e_i = Y_i - \hat Y_i.$  We know that on average $e_i$ is zero so $(e_i-0)(e_i -0) = e_i^2$ is the sample analog and can serve as an estimate of $\sigma^2_i$.  Notice that both the estimate of the constant variance and the individual variances uses the values of $e_i^2$.   In the first case the values are averaged in order to get as precise an estimate  of $\sigma^2$ as the data allow us.  In the second case each $e_i^2$ is a single <q>observation</q> of each $\sigma^2_i$.
$$\hat{Var}_{het}[\epsilon] =  \pmatrix{ e^2_1 & 0 & 0 &\cdots & 0\cr
                            0 &e^2_2 &0 & \cdots &0\cr
                            0 & 0 &0 &\ddots &\cdots \cr
                            0 & 0 &0 & \cdots &e^2_N \cr}. $$
If this heteroscedastic estimate is put into the sandwich formula the result is are <em>robust</em> estimates of $Var[\hat\beta^{ols}]$.  

<h3><a name="s033"><LI>Panel Data Models: Fixed &amp; Random Effects</LI></a></h3>
<div class="break"></div><blockquote class="upshot"><h5>Panel Data Models: Fixed &amp; Random Effects</h5> is not ready. This page is left blank to provide some room for taking notes.</blockquote><div class="break"></div>
<h3><a name="s034"><LI>Instrumental Variables</LI></a></h3>
One of the most commonly used techniques in econometrics currently is IV estimation, which involves the use of <em>instrumental variables</em>.  The first thing to note is that an instrumental variable is just a variable in the data set.  It plays a special role in, say, a regression model because the researcher claims  it is a special kind of exogenous explanatory variable.  Many people see IV as a technique that does not rely on economic theory, but indeed a theory is required to turn an ordinary variable into an instrument.</p>

Second, the reason IV is commonly used is because it attempts to solve to fundamental assumption in standard models, that included $X$ variables are uncorrelated with excluded factors (disturbance terms, unobserved variables).  
$$Y = \beta_0 + \beta_1 X + \epsilon.$$
Now suppose the Gauss-Markov assumption that  $Cov[X,\epsilon]=0$  does not hold.  The effect on OLS estimates is that they are no longer unbiased and no longer consistent:
$$[BIAS?]\qquad E[\hat\beta_1^{ols} ] = \beta_1 + {Cov[X,\epsilon]\over Var[X]}.$$
What this equation says is that the influence of $X$ on $Y$ is both direct (through $\beta_1$) and indirect (as a <q>proxy</q> for the error term it is correlated with).   Why does this matter?  The key is what we <em>infer</em> about the relationship between $Y$ and $X$ based on our estimation results.
We know that
$${\partial E[Y | X] \over \partial X} = \beta_1.$$
So if we hold all else constant and can make a unit change in $X$ the expected value of $Y$ will increase by $\beta_1$.  If we use $\hat\beta_1^{ols}$ to estimate $\beta_1$ and it is biased as above then we will either under- or over-estimate the effect of a change in $X$ (depending on whether the covariance is positive or negative).  Alternatively if we want to test whether $\beta_1$ takes on a special value (such as 0) then our test statistic will be incorrect if the Gauss-Markov theorem does not hold.  </p>
One way to think about this problem is to be more explicit about why $X$ might be correlated with $epsilon$.  Suppose the <em>true</em> regression model is
$$Y = \beta_0 + \beta_1 X + \underbrace{ \beta^\star_2 H + \epsilon^\star \atop \epsilon}.$$
The disturbance $\epsilon$ contains the effect of a hidden variable $H$ as well as a disturbance term $\epsilon^\star$ that we assume is truly uncorrelated with everything else.  So if we could observe $H$ in our data then we would be back to our standard assumptions and OLS estimates of the regression would be unbiased (and consistent).  If $H$ is not observable but it turns out that is also uncorrelated with $X$ then we are still okay.  What matters is when an included variable like $X$ is correlated with an observed (or excluded) variable like $H$.  Then we get a special form of [BIAS?]: 
$$[EXCLUDED?]\qquad E[\hat\beta_1^{ols} ] = \beta_1 + \beta^\star_2{Cov[X,H]\over Var[X]}.$$
Regardless of whether we think $X$ is correlated with a variable like $H$ which we can imagine observing but actually can't or we simply think that $X$ is correlated with the disturbance term $\epsilon$ without pinning it on a particular channel, we would say that $X$ is <em>endogenous</em>.  In other words, its value is not independent of the unobserved factors that determine the value of $Y$.  </p>

An <em>instrument</em> for $X$ is an observed variable $Z$ that can overcome this endogeneity bias.    Let's call this variable $Z$.  Unlike $H$ it is <em>observed</em> (otherwise it can't be an instrument).  Second, like $H$, $Z$ it is <em>correlated with $X$</em> (otherwise it can't be an instrument).  Finally, unlike $H$, $Z$ has <em>no effect of its own on $Y$</em> (otherwise it can't be a <em>valid</em> instrument).  
$Z$ observed
$Cov(X,Z) \noteq 0 $
$Cov(Z,Y) = 0 $
The idea of IV is to use only the variation in $X$ explained by $Z$ to estimate $X's$ effect on $Y$.  



<h3><a name="s035"><LI>Ordered Probit, Multinomial Probit &amp; Tobit</LI></a></h3>
<div class="break"></div><blockquote class="upshot"><h5>Ordered Probit, Multinomial Probit &amp; Tobit</h5> is not ready. This page is left blank to provide some room for taking notes.</blockquote><div class="break"></div>
<h3><a name="ex007"><LI>Exercises for <em>What To Do<br/>Definitions, results, procedures</em></LI></a></h3>
<OL class="exer">
<LI>Based on the postulates, what is the probability of the empty set, $P(\emptyset)$?  What is the interpretation?</LI>
<LI>Why does it turn out that $\hat\theta^{mle}_U\ =\ {\bar Y}$? How does this compare to your past study of 0 and 1 outcomes (e.g. the binomial distribution)?</LI>
<LI>In the nLRM1, solve for $\hat\beta^{mle}$ and $\hat\sigma^{mle}$. Confirm second order conditions.</LI>
<LI>In the nLRM1, solve for $\hat\beta^{mle}$ and $\hat\sigma^{mle}$. Confirm second order conditions.</LI>
<LI>In the nLRM1, there is a small difference between $\hat\sigma^{mle}$ and $\hat\sigma^{ols}$.  In particular, the former divides by $N$ the latter by $N-1$. Relate this difference to the tradeoff between bias and efficiency of estimators. </LI>
<LI>Matrix versus Scalar Notation. Let $W = \sum_{j=1}^M V_j^2$.  And define
$$v_{M\times 1} \equiv \pmatrix{V_1\cr V_2\cr \vdots\cr V_M}$$
How could I write a formula for $W$ using $v$ and not individual elements $V_j$?
<OL class="compact">
<LI>Cannot be done.</LI>
<LI>$v*v$</LI>
<LI>$vv$</LI>
<LI>$v'v$</LI>
<LI>$vv'$</LI>
<LI>I don't understand.</LI>
</OL></LI>
<li>Suppose $v$ is a random (column) vector and $\mu_v \equiv E[v] = \pmatrix{E[v_1]\cr\vdots\cr E[v_n]}$
is the vector of expected values of each element of v. The variances and covariances of elements in v are constructed by:
$$Var[v] = E[ (v-\mu_v)(v-\mu_v)' ].$$
Let B be a matrix of constants and $w \equiv Bv$. That is, $w$ is a vector of linear combination(s) of the elements of v.  In terms of B and $\mu_v$ , what is E[w]?
<OL class="compact">
<LI>$ B $</LI>
<LI>$ B\mu_v$</LI>
<LI>$ B\mu_vB'$</LI>
<LI>$ \mu_v $</LI>
<LI>$ B^{-1}\mu_v$</LI>
<LI>Not determined by the information provided.</LI>
<LI>No idea how to answer this question.</LI>
</OL>
Hint: E[] is a linear operator, so it can move through multiplication by constant matrices.</li>

<LI>In terms of B and V, what is $Var[w]$?}
<OL class="compact">
<li>$BV$</li>
<li>$BVB'$</li>
<li>$BB'$</li>
<li>$V$</li>
<li>$VV'$</li>
<li>Not determined by the information provided.</li>
<li>No idea how to answer this question.</li>
</OL></LI>
<li>Show that $X'X$ is <em>symmetric</em> (and thus its inverse is also symmetric).</li>
<LI>Show directly that X(X'X)^{-1}X' is <em>idempotent</em>.</LI>
<LI>Show directly that if $M$ is an idempotent matrix then $I-M$ is also idempotent</LI>
</OL>
</OL>
</OL>
<h1><a name="s036"><LI>How To Do It<br/>Data &amp; Stata</LI></a></h1>
<blockquote class="toc"><h4>Contents</h4>
<OL type="A" class="toc2">
<LI><a href="s038.html" target="contentx">Data: Variables, Observations, Data Sets</a></LI>
<LI><a href="s039.html" target="contentx">Basic Stata</a></LI>
<LI><a href="s044.html" target="contentx">Computers Help Humans Understand Statistics<br/>Simulation &amp; Monte Carlo</a></LI>
<LI><a href="s045.html" target="contentx">More Stata</a></LI>
</OL>
</blockquote>
<OL  type="A" class="toc2" >
<h2><a name="s038"><LI>Data: Variables, Observations, Data Sets</LI></a></h2>


Econometrics analyzes data organized by observation and variable.  For our purposes, an observation corresponds to the outcome of a single random experiment.  Variable contents are interpreted as the realized value of a random variable defined on the experiment. For the most part we assume that different observations are <em>independent random experiments</em>.  For a single observation variables will be dependent on each other.  In terms of a spreadsheet, an observation would usually be a single row.  A variable is a single column.  Spreadsheets are able to store and manipulate more than one sheet at a time.  A database is essentially a number spreadsheets that are associated with each other through shared identifiers.  We will in effect assume that if the data we start with began in a database that a <em>query</em> was already run that dumped the information we need to a single spreadsheet (matrix), although later we discuss merging two data sets.</p>

<h4>Types of Variables</h4>

A key property of a variable is its  <em>unit of measurement</em>.  Some kinds of variables do not have units. If this is the case, then they must be treated differently in econometrics than variables that do have units.

<UL>

<LI>Discrete</LI>
A discrete variable corresponds to a discrete random variable.  It takes on a finite number of distinct values.
<OL class="compact">Kinds of discrete variables

<LI><em>Binary</em> variables take on values 0 and 1, but often raw data will use other values which must be recoded to 0 and 1 to enter an estimation sample.
    <DD>Examples:<br/> Sex (Male/ Female), Status (Citizen/ Non-citizen, Married/ Single) etc.</DD></LI>

<LI><em>Ordered</em> variables are discrete values that have a natural (meaningful) ordering, but the magnitudes of the codes are not meaningful.
    <DD>Examples:<br/>
        Letter Grades (A/F), Win / Draw / Lose a Game, Degree: HS / BA / MA / PhD, etc.</DD>
</LI>

<LI><em>Categorical</em> variables code unordered discrete values.  They have no natural ordering so the numeric values should not be mistaken for values that have units.
    <DD>Examples:<br/> Province, Occupation, Industry, Major. </DD>
    <DD>Care must be taken not to have the statistical procedure implicitly order categorical data.  Usually they must be replaced by a set of <em>indicator</em> or <em>dummy</em> variables (see the How section of the notes).  It is convenient to store as single variable with categories in one variable and then generate the indicator variables only when preparing the estimation sample.</DD></LI>

<LI><em>Hierarchical</em> variables are nested categories.
    <DD><em>Examples</em>: Industry or Occupation coded by a number of digits</DD>
    Usually the nesting would be flattened before entering an estimation sample.  For example, a person decides to control for two-industry.</LI>
</OL>

<LI>Continuous</LI>

Continuous variables can (ideally) take on any value within an interval or intervals. However, data stored on a digital computer is never continuous.
<OL class="compact">Kinds of continuous variables

<LI><em>Rounded</em> variables take on discrete values, but will be treated as if it were continuous.  For example, age has been coded into years, income is in dollars, weight is in kg.  These sound continuous, but of course we could measure age down to the second not just the year.  And we might measure weight down to the microgram, so nearly any continuous variable includes some rounding.  If the rounding is coarse enough we might start thinking of the variable as an ordered discrete variable.  </LI>

<LI><em>Censored</em> variables are a mixture of continuous and discrete.  One example is the  amount spent on car purchase in a year.  If a person made a purchase, the price is entered (continuous).  But many observed values will be 0.  This is code for "no purchase made" not "got a car for free."
    When a variable is censored some values are treated as continuous, others discrete. In the example, a purchase amount of \$0 is coding a discrete outcome (no car purchased).  But the amount 	\$23,325 is a continuous outcome. Another example comes from many data sets that <q>top code</q> income variables.  For example, when people are asked their income they give an answer like 	\$36,000 or 	\$152,000.  But answers above, say, \$100,000 are converted to simply 	\$100,000 or above.   A third example is how long a currently unemployed person has been unemployed.  If what we care about is the complete duration of unemployment spells then currently unemployed people can tell us that.  So if they have been unemployed 6 weeks we have treat 6 as censored because the true outcome is "6 weeks or more."</LI>

<LI><em>Truncated</em> variables go beyond the case of censored variables.  For example, if a person is working we can ask them what their wage is and get a continuous answer.  But if they are not working we do not want to treat their wage as \$0.  Instead, many models will say that person has a wage they could earn if they chose to work but we don't know what that is unless they are actually working. </LI>
</OL>
</UL>


<h5>VARIABLE AND SAMPLE TYPES</h5>
<DD><pre>
                   Discrete Data                     Continuous Data
           | binary   ordered   categorical    continuous  rounded  censored
    ----------------------------------------  --------------------------------
    obs 1  |    0        2         35             23.55      22       00.00
    obs 2  |    1        3         24            -18.01      18     3065.22
    obs 3  |    0        6         50             00.00      65     1250.00
    &hellip;
</pre></DD>

<h4>Types of Data Sets</h4>

The key feature of a data set is its <em>unit of observation.</em>
<UL>
<LI>Cross Section:  A set of distinct individual observations with no inherent order.
    The observation number in a cross section contains no information.  Often $i$ is used as the observation index in a cross section.  $i=1,2,3,\dots,N$.
    <DD>Examples:<br/> a set of households in a telephone survey;  a set of Queen's students who walk past the JDUC during the noon hour.</DD></LI>

<LI>Time Series: Data that is ordered, usually by time, but other concepts may induce ordering.

    <DD>Examples: <br/> National Income and Product Accounts by Quarter, Vital Statistics (births, deaths, etc) by Year</DD>

    In this case the observation number, often denoted $t$, codes the ordering.  $t$ comes before $t+1$, etc.  Seasonal data has indices to indicate and track more than one unit of time.  So monthly data may have a variable $t$ which is the number of months since January, 1970.  But it will also might have $y$, $q$, and $m$ coding the year, $q$ coding the quarter of the year (1-4), and $m$ the month (1-12).  But each observation is still a single month and one month follows the other.</LI>

<LI>Pooled Time Series Cross-Section:  A combination and generalization of  cross section and time series. Each observation has a time (ordered) quality and an unordered (cross-section) quality.     We use a pair of indices $(i,t)$ to track the individual and time dimensions of an observation.  The same $i$ may be used for different individuals at different times.  For example, observation $i=1$ in the year 2001 data and observation $i=1$ in the year $2009$ data are not necessarily the same person.</LI>

<LI>Panel (Longitudinal):     A special case of pooled data in which an observation (a person, a household, a firm, a country) is observed repeatedly in an ordered pattern.    Again $(i,t)$ captures the two dimensions, but in a panel a given $i$ represents the same person at different time periods.  So in a panel person $(1,2001)$ is the same person as $(1,2009)$.</LI>

<LI>Weighted Sample.
    When each individual in the population is not equally likely to be in the sample. If you live in PEI you are much more likely to be contacted by the <em>Labour Force Survey of Canada</em> than if you live in Ontario.  Weighted sampling is designed to improve precision of sample statistics within groups.  A person in the LFS from Ontario stands in for more people than someone from PEI. For purposes of introductory econometrics weighted but random sampling is not a major concern. If weights exist they can safely be ignored. Weights matter greatly in other contexts.</LI>

<LI>Selected Sample. A data set in which being in the sample depends on an outcome to be studied with the sample. Example: Studying community health using a survey of people who visit the ER in a week.  Such people are not represented of the population as a whole in terms of health. Drawing a conclusion based on this sample would likely be a distorted view of community health.    A selected sample is like a weighted sample but the weights are not chosen by design.  Special techniques beyond this class are required to analyze selected samples in order to overcome the selection effect.</LI>
</UL>

<h5>TYPES OF DATA SETS</h5>
<DD><pre>
               Cross-Section                                           Time Series
         |   id   age   province  income                      |  t   UERate   GDP    Recession
  -----------------------------------------           ------------------------------------------
  obs 1  |    1    16      35     00.00                obs 1  | 1995   8.5    210.01     0
  obs 2  |    2    35      24     3065.22              obs 2  | 1996   8.1    222.35     0
  obs 3  |    3    22      50     1250.00              obs 3  | 1997   5.2    218.00     1
  ...                                                    ...

                          LONGITUDINAL DATA CAN BE STORED "LONG" OR "WIDE"

 Long Format                                   Wide Format
        |   id  yr age   province  income              | id y1 y2 y3 a1 a2 a3 p1 p2 p3 inc1   inc2 inc3
 -------------------------------------------    ---------------------------------------------------------
 obs 1  |    1  95  16      35     00.00        obs 1  |  1 95 96 97 16 17 18 35 35 35  0.0    0.0  0.0
 obs 2  |    1  96  17      35     00.00        obs 2  |  2 01 02 03 35 36 37 24 36 24  3065. 3805. 0.0
 obs 3  |    1  97  18      35     00.00        ...
 obs 4  |    2  01  35      24     3065.22
 obs 5  |    2  02  36      36     3805.22
 obs 6  |    2  03  37      24     00.00
</pre></DD>
<h2><a name="s039"><LI>Basic Stata</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s040"><LI>Learn how to use Stata effectively</LI></a></h3>

<h4>What you shall/should know about Stata by the end of this course</h4>

<span><DT><dfn id="basic">Basic Stata Commands</dfn></DT><DD><pre>log/close   use/save   merge   save   do/doedit<br/>display   summarize   describe   list tabulate<br/>generate   replace   drop/keep   drop/keep if<br/>mvencode/mvdecode  regress   probit   test   lrtest<br/>xtsetxtreg   xtprobit   ivregress   ivprobit<br/>scatter   line   twoway   hist</pre></DD></span>

<DT>functions</DT>
<DD><pre>
rnormal()      normal()    normalden()    invnormal()
</pre></DD>

<DT>Syntax<DD><pre>
by :
in  if
[ ]
i. #
</pre></DD>

<DT>Kinds of files</DT>
<DD><pre>
 Type               Extension
 ------------------------------
 Data file              .dta
 Log  file              .log
 Command file           .do

 *Graph file            .gph
 *Program file          .ado
 *Dictionary file       .dct
 </pre>
*= may not be involved in coursework or the final project.</DD>

<a name="Fig13"></a>
<figure><h4>Exhibit 13.  Bird's-eye View of a Session using Stata </h4><img  width="80%" src="img/Stata1.png" /></figure>


<h4>Logging Your Stata Commands and the Results</h4>

Often you will interact with Stata by typing, clicking and seeing results to the screen. However, to use Stata effectively you need it to interact with external files.


You may never look at any particular log file after you close it, and when you are just "looking" at data or trying to understand something done in class you might not open a log. But your final project may require a large number of steps that will have to be re-done when you find mistakes or change your mind.  At that point, having a log file to refer to can save you a great deal of time and possibly find and correct errors that we will detect in marking that will hurt your grade.</p>

A do file is simply a list of Stata commands in a file as you might type them in the command window in Stata. The file extension <code>.do</code> lets Stata know it is a list of commands.
<DD>Suppose the file <code>myproject.do</code> is in your Stata working directory and you:
 <pre>File &#10148; Help &#10148; Do  choose <em>myproject.do</em>               &bull;  do myproject</pre>
Then Stata will run the commands in the file as if you typed them in the command window. It is important to be able to re-do work that goes into your final project and to demonstrate to a grader/reviewer of your work that you work is correct.  So building up do files from logs of interactive Stata work and the running the file to produce your final results is very important.</p>
<DD>You can create and edit do files using the do-file editor:
<pre>&bull; doedit       Window &#10148; Do-file Editor  (Ctrl-9)</pre>
</DD>

<h3><a name="s041"><LI>Get Data In &amp; Out of Stata</LI></a></h3>

The schematic of Stata session in the previous section showed various commands sandwiched between starting Stata and opening a log file and closing the log and exiting. For your project you will
<UL>
<LI>Find a data set (or possibly combine two or more data sets)</LI>
<LI>Import and Load the data into Stata</LI>
<LI>Discover and report patterns in the data (means, tabulations, etc.)</LI>
<LI>Process the data and apply one or more econometric models to it.</LI>
<LI>Do these steps over days or weeks, saving/storing the changed data between sessions so that you can start where you left off.</LI>
</UL>
So the next layer of the Stata sandwich is getting data in and out of a session.

<a name="Fig14"></a>
<figure><h4>Exhibit 14.  Level 2 of the Stata Sandwich </h4><img  width="80%" src="img/Stata2.png"/></figure>

Stata has its own format for storing data (<code>.dta</code> files). Most of the practice data we use will already be in that format. Many data sets that might be used for the Course Project are available in Stata format already. However, data you might want to use may come in a different format. Stata can <code>import</code> data in many other formats. Imported data or data that you process using Stata should then be saved on your computer (or cloud storage) in Stata format so that next time it can be loaded into Stata directly.  Some tasks during the term may require uploading <code>.dta</code> files you have created or manipulated.

<DT>Loading a .dta file</DT>
<DD>To load a Stata data set to work with it you <b>use</b> it:
<pre>File &#10148;  Open &laquo;file&raquo;.dta             &bull; use &laquo;file&raquo;</pre></DD>
<DD>As you work with data in Stata the changes are not saved permanently until you <b>save</b>:
<pre>File &#10148; Save           &bull; save , replace       Ctrl-S</pre>
This form works if the data in memory are already associated with a file on disk.  The <code>replace</code> option will wipe out the contents of the data file and replace it with the data currently in memory. </p>

There is no <code>undo</code> button or command in Stata. As you change the data in memory you lose what it looked like before the change.  Stata is not set up to let you <em>undo</em> your previous command.  However, you do not have to continually <code>save</code> your data to undo a command.  The <code>snapshot</code> command is a quick way to save the current data set so that if subsequent changes you make are a mistake you <code>snapshot restore</code> to that point.  The big difference between <code>save</code> and <code>snapshot</code> is that snapshots disappear (are deleted) once you exit Stata. (Another  related command which may come in handy is <code>preserve</code>.)

<DD>To save to a new file :
<pre>
File &#10148; Save As &laquo;newfilename&raquo;.dta             &bull; save &laquo;newfilename&raquo;
                                                         &bull; save &laquo;oldfile&raquo;, replace</pre>
The format of Stata data sets changes over time.  Any version of Stata can <code>use</code> data saved from earlier versions, but an older version of Stata may not be able to read data sets saved by newer versions. So if you are sharing data with someone using an older version you may have to use <b>saveold</b> instead of <b>save</b>.</DD>

If you need to import non-<code>dta</code> data there are two strong recommendations.  First, use drop-down menus to import data because reading files in other formats can be complicated and is not something you do repeatedly:
<DD><pre>
File &#10148; Import
</pre></DD>
Most likely your data are in one of the formats Stata can read.  You may have to set some options in the <code>Import</code> dialog for it to work. In some cases, external data may be hard to read into Stata.  In some cases, getting data ready for econometric work requires knowing a fair amount about how Stata wants data organized.  So the second recommendation is to simply ask me to help you import data.  This is not a skill that is important for you to figure out on your own at this point.  </p>

<h4>If you are used to Excel and spreadsheets, how is Stata like it in handling data?</h4>

Stata is LIKE Excel (and other spreadsheets) in that &hellip;
<DD>A Stata data set is like Excel in that it organizes data in rows and columns.</DD>
<DD>Because of this, you can view or work with data in Stata as if it were spreadsheet:
<pre>Data &#10148; Data Editor
             &#10148; Data Editor        &bull; edit
             &#10148; Data Browser       &bull; browse</pre>
</DD>
Both store different kinds of data.  All information on a computer is stored using binary values (strings of 0/1 values).  To interpret a list of 0s and 1s as kinds of data require <code>codes</code> that map each pattern of bits into something that makes sense to us.
<pre>
&bull;  help data types
</pre></dd>

Stata is UNLIKE Excel in that &hellip;
<DD>In Stata rows and columns are treated very differently in Stata (but are treated almost symmetrically in Excel.)  </DD>
<DD>In Stata, a row is associated with an <em>observation</em>, which is a unit of statistical analysis.  Conceptually, an observation is a realized outcome of a random experiment.</DD>
<DD>A column is associated with a <em>variable</em>, which conceptually is a random variable.</DD>
<DD>Stata handles only one data set at a time.  In Excel a spreadsheet file can have multiple sheets, but Stata always handles one data set (observations and variables) at
a time.
<DD>Stata is able to combine data from two or more data sets using <code>merge</code> and <code>append</code>.</DD>
<DD>Both those commands modify the data set in memory by bringing data on the disk <em>already stored as a <code>.dta</code> file</em>. </DD></DD>
<dd>A Stata variable can only store one kind of data.  An Excel column could have lots of different kinds of data.</dd>

<h4>Example</h4>

Stata comes with example data sets. These notes we use one of these data sets to illustrate basic uses of Stata.   These data sets are easily accessible in Stata:
<dd><pre>
&bull;  sysuse nlsw88
</pre></dd>
This command loads the data set into memory so that Stata can work on it.  If you try to do this you may get an error message:
<dd><pre>
&bull;  sysuse nlsw88.dta
no; data in memory would be lost
r(4);</pre></dd>
(If you did not get an error message, then do this and you will see it:
<dd><pre>
&bull;  drop grade
&bull;  sysuse nlsw88.dta
</pre></dd>
Now you should see the error message above.  You must get used to the idea that Stata commands you think will work won't necessarily.  You must learn to read the error message that Stata prints out to understand what went wrong.  It takes time to learn how to do this, but you won't learn if you do not <em>read the error messages and think about what they (might) mean</em>.  This error is an example.  We tried to <em>use</em> a data set, which means copy it from disk into memory.  But if there is already a data set in memory, and if it is different than its file on disk, Stata will not let you wipe it out by using a different data set.   In this case we don't care about losing the change (dropping the variable <code>grade</code>), so we can tell Stata it's okay:
<dd><pre>
&bull;  sysuse nlsw88.dta , clear
</pre></dd>
What we have done is specified an <em>option</em> that <code>sysuse</code> accepts.  Namely, we are saying it's okay for Stata to clear (wipe out) the data in memory and replace it with <code>nlsw88.dta</code>.


<h3><a name="s042"><LI>Understand Your Data</LI></a></h3>

So far we've learned that Stata variables can store integers (5), decimal numbers (+35.232) and strings of characters (<q>Canada</q>).  And you can load data from some external source in Stata with the plan to use them in an econometric analysis. Now What? Do you know what is in the data?  <mark>Do you know what the data mean?</mark></p>

By the end of Econ 452 you are responsible for knowing/understanding what <em>your</em> data mean.  I say <em>your</em> because you will choose a data set and analyze it, and your report will be read and marked seeing if you do understand what your data mean.  If you do not understand your data you are very likely to make mistakes that an experienced economist (even a mediocre one like yours truly) will discover.  </p>

You can <code>browse</code> as a spreadsheet, but browsing the data does not give you an overview, especially when the data include thousands of observations and hundreds of variables. And the numbers themselves do not say what the numbers mean.  Consider the number 5 in a data set.  It could <em>mean</em> just about anything in the real world, such as the number of people in a household or the fact that a person is serving a sentence of 5 years.  It mean also have nothing to do with numbers.  It could mean a company is in the service industry (which happens to be coded as 5 in the data set.</p>

That is, a datum does not contain in itself the information about what the datum means.  The fancy word for information about the data is <em>meta-data</em>.  In the old days statistical software stored the values and did the computations.  All the meta data was stored externally in code books, user guides, etc.   Now Stata can store this information inside the data set itself, although reference to external documentation is often still necessary. The commands below give you an overview of a data set. You should use them to be looking for aspects/issues of the data:
<UL>
<LI>Which variables are in the data set and do you have a general idea of their meaning from the variable names and the variable labels?</LI>
<LI>Do variables take on a range of values that match what you think the variables mean?</LI>
<LI>Are there unusual values of variables that may be <em>mistakes</em> or <em>outliers</em> or <em>special codes</em>?</LI>
<LI>Are there <em>missing values</em> that need to be understood and processed before the data are used in an econometric analysis?</LI>
<LI>Do you understand the <em>units</em> variables are expressed in and do you understand the <em>codes</em> used to store discrete categorical data?</LI>
</UL>

<h4>Descriptive commands</h4>

<DT><code>describe</code></DT>
<code>describe</code> lists the variables in the current data set like a table of contents. Hopefully the names of variables are relevant.  If not you should use <code>rename</code> to make them relevant. In textbooks we talk of <var>X</var> and <var>Y</var> and <var>Z</var>.  But in a particular case <var>X</var> is really <q>income of the household</q> or <q>population of the city in 2005</q> or &hellip;.   The name of a variable in both the data set and in the report should say what it is.  So rather than storing income in a Stata variable named <code>X</code>, why not call the variable <code>inc</code> or <code>income</code>? And in referring to summary statistics and other aspects of the variable in your report, use  <var>Income</var> rather than <var>X</var> or <var>I</var>.</p>

Variable <em>names</em> should be descriptive but &hellip; they should be short, and in Stata they must be single words. (No spaces and other special characters are allowed because this would confuse Stata.  Stata interprets "X Y" as a list of two variables not as a name of a single variable.)  Stata can also store <em>labels</em> (descriptions) of data sets, variables, and values of variables. Labels can be very long and descriptive.  To make it clear, you must put labels inside quotes when typing them (unless you are filling in a box in a menu entry).  This may not seem very important.  Isn't econometrics all about the numbers itself?  However, labels are used to explain and document what the numbers mean.  Many of the external data sets you will be looking to use in the final project will use labels to tell you how to interpret the results.</p>

<DT><code>summarize</code></DT>
<code>summarize</code> produces a table of summary statistics that describe the data. Use it to look for missing observations (different counts), unusual or unexplained minimum and maximum values.  Check that the sample means match your understanding of units  (i.e. dollars or thousands of dollars).</p>

<DT><code>tabulate</code> and <em>value</em> labels</DT>
<code>tabulate</code> produces a tabulation of values that a variable takes on.  This is useful for categorical variables.  You can also cross-tabulate two variables using <code>tab2</code>.  And <code>tab</code> can produce a set of indicator variables for each value (using the <code>generate()</code> option).</p>
<Dd><pre>
&bull; sysuse nlsw88
&bull; summ race
&bull; tab race
</pre>Results in this output:
<pre>
    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
        race |      2246    1.282725    .4754413          1          3

       race |      Freq.     Percent        Cum.
------------+-----------------------------------
      white |      1,637       72.89       72.89
      black |        583       25.96       98.84
      other |         26        1.16      100.00
------------+-----------------------------------
      Total |      2,246      100.00
</pre></Dd>
<mark>Inspect this output</mark> before reading on.  The summary of <code>race</code> says it takes on values between 1 and 3 with a mean of 1.28.  But the tabulation says that <code>race</code> takes on values <q>white</q>, <q>black</q> and <q>other</q>.  You might guess, and you would be right, that these correspond to the numeric values 1, 2, and 3, respectively.  You can check that guess and Stata's math: <code>Average Race = 1*.73 + 2*.26 + 3*.01 = 1.28</code>. </p>

We have already discussed that variables can have labels, but the <em>values</em> that variables take on in your data set can also have labels.  Apparently, the variable <code>race</code> is <b>coded</b> and the code is defined by the labels you see in the <code>tabulate</code> output.  Stata will replace the numeric values of a labeled/coded variable with the labels when it thinks it makes sense to do so.  You can find out about value labels in several ways:  Look at the <code>describe</code> output; use help to learn about the <code>label</code> command; look at the properties of the variable (in the properties window on the screen).</p>


<DT>Other descriptive commands</DT>
<dd><code>list</code></dd>
<DD><code>count</code>, e.g. <code>count if income&lt; 1000 </code></DD>
<DD><code>codebook</code>:  this produces an combination of <code>summary, describe and tabulate</code></DD>

<h4>Example</h4>

All Statistics Canada surveys of individuals as the respondent their <em>gender</em>.  Here is one variable for that response in the Stata data from <a href="http://odesia.ca">ODESI.ca</a>
<DD><pre>
. describe GENDER

              storage   display    value
variable name   type    format     label      variable label
-----------------------------------------------------------------
GENDER          byte    %1.0f      GENDER     Sex of respondent
</pre></DD>
At this point, the main piece of information to look at is <code>variable label</code>, which is <q>Sex of respondent</q>.  So far so good.  And we see that in this data set the values of <code>GENDER</code> are labeled with the value label that also happens to have the same name, presumably <q>male</q> and <q>female</q> (because Statistics Canada is still a <q>sys-normative</q> institution, although your instructor is old-fashioned and finds those concepts <q>problematic</q>.)  What do the other columns of information mean?</P>

<DT>storage type</DT>
The output says GENDER is stored as a <code>byte</code>, which can store up to 128 different integer values (a byte is 8 binary digits so a total of 2<sup>8</sup> = 128 different values, which may not be enough soon!).  That is the smallest storage bin that Stata allows, and since GENDER is (or was traditionally) a binary value that is more than enough space to store it. What do you think would happen if you tried to store something that takes on more than 128 values in a byte?  For example, a code for the countries of the world?  </p>

<DT>display format</DT>
The variable will be displayed using the <code>%1.0f</code> format.  This means when values of GENDER are printed out on the screen are to a file they will take up one space (1) and zero (0) spaces will be devoted to the decimal part of the number.  The <code>f</code> says that the display is of <b>f</b>ixed length.  This way of specifying how to print numbers out goes back to at least the 1970s (see <em>wikipedia</em> on <code>Printf_format_string</code>).</p>

Let's <code>summarize</code> it:
<dd><pre>
. summ GENDER

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
      GENDER |     16081    1.585722    .4926122          1          2
</pre></dd>
You might think that the data set would use 0 and 1 for GENDER, but it does not.  It uses <code>1</code> and <code>2</code>, apparently, because those are the min and max values in the data and a <code>byte</code> variable can't store decimal numbers.  So which is which?  Is <code>1</code> code for male or female?
Using the <code>label</code> command mentioned earlier here is what I found out about the GENDER label:
<DD><pre>. label list GENDER
GENDER:
           1 Male
           2 Female
</pre></DD>
We see from the describe output that the label has the same name as the variable, which need not be the case.
So now we know that <code>1</code> codes a male respondent and <code>2</code> codes a female.  </p>

Remember that value labels are just that.  They are <em>not</em> the value of the variable itself.  For example, you could get Stata to tell you how many males there are by typing <code> count if GENDER==1</code> but it will not work to type <code> count if GENDER=="male"</code> because the values stored in GENDER are 1 and 2.  The labels for those values are not the same thing and can't be used instead of the value in calculations.</p>



<h3><a name="s043"><LI>Manipulate Data</LI></a></h3>

<h5>Getting Rid of Data</h5>

<UL>
<LI>Variables (columns)</LI>

<DT><code>drop &lt;varlist&gt;</code></DT>
<DD>Removes variables (columns) listed</DD>

<DT><code>keep &lt;varlist&gt;</code></DT>
<DD>Removes all variables <em>except</em> the ones listed</DD>
<DD><pre>
MENU PATH                      OR               COMMAND <hr class="plain"/>
Data &#10148; Variable Manager                      &bull; drop
                                              &bull; keep
</pre></DD>

<LI>Observations (rows) </LI>
<DT><code>drop if &lt;condition&gt; </code></DT>
<DT><code>keep if &lt;condition&gt; </code></code></DT>
<DD><pre>
MENU PATH
Data &#10148; Create or Change Data
   &#10148; Drop or keep observations
</pre></DD>
<DD>There is a fundamental difference between <code>drop</code> and <code>drop if</code>.  The latter will selectively drop observations (rows) if the data in the row meet the condition after <code>if</code>.</DD>
</UL>

<DT>Example</DT>
<DD>Drop observations for which a variable <code>x</code> equals 3 (keeping all other observations):
<pre>&bull; drop if x==3</pre></DD>

<DT>The <code>if</code> modifier</DT>
<DD>Most Stata commands will accept the <code>if &lt;condition&gt;</code> moderator</DD>
<DD>This will limit the effect of the command to rows that satisfy the condition.  Rows that do not satisfy the condition are unaffected or are not used in the calculation.</DD>
<DD>See help on Stata's <em>logical operators</em></DD>

<h5>Replace Values and Generate Variables</h5>

<DD><pre>
MENU PATH                            OR               COMMAND <hr class="plain">
Data &#10148; Create or Change Data             &bull; replace &lt;var&gt; = expression
   &#10148; Change Contents of Variables

Data &#10148; Create or Change Data             &bull; gen &lt;newvar&gt; = expression
   &#10148; Create New Variable
</pre></DD>

<h5>Functions</h5>

Stata has many built-in functions which can be used when replacing data, generating new variables, deciding which observations should be affected (the <code>if</code> moderator), etc.
<dd><pre>&bull; help functions</pre></dd>

<h5>Recode</h5>

</OL>
<h2><a name="s044"><LI>Computers Help Humans Understand Statistics<br/>Simulation &amp; Monte Carlo</LI></a></h2>

What does <em>random</em> mean? The word random is used to mean different things.  So far in our class we only ascribe the word random to <em>events on a sample space</em>.  We say that some mechanism (God, Zeus, quantum physics, etc.) selects which outcome will occur based on the <em>ex ante</em> probability of the events.   In this view there is no way to repeat a truly random event.  According to the story, the next time you draw an event from the sample space the mechanism will again somehow select from all possible events according to the probability.  It may be the same event as before or not.</p>

There is nothing random about the operation of a computer, except to the extent that cosmic rays and products of truly random radioactive decay might <a href="https://en.wikipedia.org/wiki/Soft_error">scramble memory</a>. Yet computers appear to generate random events.  Doesn't your phone shuffle your playlist?  Yet, there is nothing random about the operation of the microprocessor on the phone.  If your computer/phone/tablet starts to act in a truly random fashion it is time to get a new one.  </p>

In the modern world we are surrounded by <em>simulated randomness</em>. One of the most important uses of computers in econometrics is to simulate randomness.  What is meant by this is something quite specific.   What is really important to  simulate is <em>independent sampling</em> from a distribution.  That is, computer algorithms have been developed to <u>fake</u> IID sampling.</p>

One of the best uses of simulated computer randomness is to study <em>small sample properties of estimators</em>.  Large sample properties of statistical estimators are typically derived mathematically (symbolically). Small sample properties are difficult or impossible to derive this way, unless the estimator is <em>linear</em> and/or there is something very special about the sample space (usually normality).  The best way to understand small sample properties in general is to <em>simulate randomness across possible samples</em>.</p>

<span>A single simulated sample is called a <dfn id="replication">replication</dfn></span>.  We will use $R$ to denote the number of replications. This is different than the sample size of each replicated sample, still called $N$. For each replicated/simulated sample, we again use the computer to carry out the estimation procedure and then collect information about estimates across the replications.  The use of  simulated random sampling to learn about statistical properties is called <b>Monte Carlo</b>.</p>

<a name="Fig27"></a>
<div class="alg"><h4>Definition 27.  The Standard Uniform Distribution </h4>
<DT>We write $V\sim U(0,1)$ to mean</DT>
<OL class="compact">
<LI>$V$ is a continuous random variable that takes values strictly between 0 and 1</LI>
<LI>$V$ has a uniform (constant) pdf on (0,1). To make the total probability integrate to 1 the height of the density is 1:
$$f(v) = \cases{ 1 & for $0\le v\le 1$\cr
                 0 & elsewhere.\cr}$$</LI>
<LI>The cdf of $V=\int_{0}^{v}f(u)du$ is simply $v$:
$$F(v) = \cases{ 0 & $v\lt 0$\cr
                 v & $0\lt v\lt$\cr
                 1 & $v\gt 1$.\cr}$$</LI>
</OL></div>

Equal density means that the probability $v$ lands in any interval $(a,b)$ inside $(0,1)$ is simply the size of the interval  $b-a$. The uniform distribution is the key to simulating randomness.   It turns out that simulating $U(0,1)$ is all you need to be able to do in order to simulate any distribution $F(x)$.   The key insight that allows us to do this is not obvious:</p>

Let $X\sim F(x)$ be a continuous random variable with the CDF $F(x)$.  Define a new random variable $G= F(X)$.  That is, $G$ is the value of the cdf of $X$ evaluated at the <em>realized</em> value of the random variable. Then,
$$G \sim U(0,1)$$
That is, the cdf evaluated at a random outcome drawn from that cdf is uniformly distributed <em>regardless of the (continuous) distribution we started with.</em></p>

If $X$ is a discrete random variable that takes on values $x^1,\dots,x^k$ with corresponding pdf's $f(x^k)$ then $G$ is also discrete and takes on the values $F(x^k)$ each with probability $f(x^k)$.</p>

<h4>Example</h4>

The table below shows 5 IID observations from a sample space that has two random variables defined on it.  $Z \sim N(0,1)$ and $Y \sim Bernoulli(0.3)$.  And there is another random variable which is a function of $Z$:  $G(z) = \Phi(z)$ is the cumulative normal distribution evaluated at $z$ Finally there is a fourth, $H = F(y)$, is the cumulative Bernoulli evaluated at $y$.   The table shows what a realized data set might look like.
<DD><pre>
i    z       g             y       h
-----------------------------------------
1   0.0     0.5            0      0.6
2  -0.3     0.382089       1      1.0
3   0.3     0.617911       1      1.0
4   1.96    0.975          0      0.6
5  -0.001   0.499601       0      0.6
</pre></DD>
The theorems above tells us what the distribution of $G$ and $H$ are given that they are the cumulative distribution at the realized values of $Z$ and $Y$.</p>

The theorems about the distribution of the CDF tells us we can generate a realized value of any continuous random variable if the following two things can be done. First, draw a realized value from $U(0,1)$.  Call this value $u$. Then <em>invert</em> the mathematical function for the CDF of $X$ at the value of $u$.</p>

Below is a CDF of a random variable $X$ that looks like the cumulative normal cdf, but does not need to be that function. Like all CDFs, it takes on values between 0 and 1 along the vertical axis. The Uniform random number generator will produce a number between 0 and 1 with equal probability everywhere in that range.  The realized value is marked $u$ and appears to be around 0.75 in this case. This value is inverted to find the value of $x$ for which $F(x) = u$. The theorem says the value of $x$ produced by this procedure will follow the distribution of $X$. More specifically, if we repeated this procedure over-and-over again and the Uniform random number generator was working properly the histogram of values of $x$ would pile up like the pdf $f(x)$.  As the number of replications went to infinity the histogram would converge to $f(x)$ exactly.</p>

<a name="Fig15"></a>
<figure><h4>Exhibit 15.  Inverting the CDF to Simulate a Random Experiment </h4><img  width="60%" src="img/InvertCDF.png"/></figure>

What does this mean? The problem of simulating draws from any distribution has been reduced to a pair of much simpler problems: Come up with a way to simulate draws from the Uniform distribution. Have the ability to <em>invert</em> the CDF of the random variable you want to simulate.  The Inversion task  requires a formula (if one exists) <u>or</u> a numerical algorithms for computing functions and inverses of functions. The procedures for all common random variable CDFs have been known for decades and are built into the library of Stata and any other statistical package. We will rely on these routines and not discuss how they work. The generating uniform random numbers task is also very well established. An applied econometrician should have a basic understanding of what random number generators (RNGs) do and what they don't do.</p>

The key to a Uniform RNG is  not to just to produce numbers between 0 and 1 that are equally likely, but also to produce sequences of numbers that look like independent draws from the uniform distribution. To assess this aspect of RNGs we need to understand the notion of autocorrelation.</p>

<h4>Autocorrelation and Independence</h4>

Many observations of a random variable are ordered in some natural way, with ordering in time being the most obvious one. A <em>time series</em> is a sequence of values of a random variable. A key aspect of time series is the notion that the values of the random variable are realized one at a time following the order.  So $Y_0$ is realized, then $Y_1$, etc.  Because of this, the observations in a time series are often not independently distributed across time. For example, if we measure a person's weight at the end of each month the values are not independent draws from some distribution.  As many people come to realize, there is a great deal of persistence in body weight.  (For that matter there is even more persistence in body height and even more persistence in age!). Of course, economic time series display a great deal of persistence as well.  Stock prices are persistent as is GDP, the unemployment rate, interest rates, etc.</p>

If the observations in a time series are <em>independent</em> then the marginal distribution of the next realization (at time $t+1$) is not a function of the values of the observations realized up until $t$. Remember the joint distribution of independent random variables factors into the product of the marginals, so knowing the previous values does not influence the distribution of the next value. If, instead, the observations are not independent then there may be information in the previous values that helps predict the future values (and thus is launched many brief but lucrative careers on Wall Street).  We can look for evidence of dependence (including persistence) from the values of the time series  using <em>autocorrelation</em> in the series.</p>

<a name="Fig28"></a>
<div class="alg"><h4>Definition 28.  The Lag operator $Ly$ </h4>
Consider a time series $y = (Y_1,Y_2,\dots,Y_t,\dots)$ that may go on indefinitely or may stop at some biggest value $t=T$.  The the first <em>lag</em> of the time series is another series where the previous value of $y_t$ now appears in the $t$ place.
$$Ly \equiv (\cdot, Y_1, Y_2,  \dots, Y_{t-1}, \dots)$$
$Ly$ is the lagged values of $y$, and $Ly$ can be thought of as a time series itself. We have now created paired values of $Y_t$ with the previous value of $Y_{t-1}$.  $Y_2$ is paired with $Y_1$, $Y_3$ with $Y_2$, etc.  $Y_1$ has no partner, which is represented as a dot (missing). We can lag the lagged values and create a new list to pair up with $y$ (all but the first two observations):
$$LLy = (\cdot, \cdot, Y_1, Y_2,  \dots, Y_{t-2}, \dots).$$
We can be clever with the notation and define $L^2y \equiv LLy$. In fact, $L^ky$ can be defined for any integer $k\ge 0$.  With this notation, $L^0y = y$ is the original series.  </div>

Negative lags such as $L^{-1}y$ would be the $k^{th}$ lead value of $y$, $L^{-1}y = (Y_2,Y_3,...,Y_{t+1})$, but we will not need to worry about both leads and lags in this class.

<DT>Example: Lags of a time series</DT>
<DD><pre>
t            y       Ly     LLy
-------------------------------------
1           8.5      •       •
2           6.3     8.5      •
3           2.5     6.3     8.5
4           5.0     2.5     6.3
5           1.1     5.0     2.5
correlation
with y      1.0    0.023   -0.00014
</pre></DD>

A reminder that the correlation $\rho$ between two random variables is defined in as the covariance divided by the product of the standard deviations:</summary>
$$\rho \equiv corr(X,Y) \equiv {Cov(X,Y)\over StDev(X)StDev(Y)}.$$
We can then use this to define the autocorrelation within a time series, $y = (Y_1,Y_2,\dots)$ of $T$ observations on a random variable $Y$, and the sample observations are not necessarily independent.
The <em>first order autocorrelation</em> of $y$ is the <em>correlation between $y$ and $Ly$.</em> In general the $k^{th}$ order autocorrelation is the correlation between values in $y$ and the $k^{th}$ lag of $y$:
$$\rho_k \equiv \rho(y,L^ky) = \hbox{"k order autocorrelation of $y$"}$$
Note that $\rho_0 = \rho(y,L^0y) = Cov(y,y)/(StDev(y)StDev(y)) = Var(y)/Var(y) = 1$.</p>

For a given time series the autocorrelation coefficients ordered by the lag, $\rho_0, \rho_1, \dots$ is called the <em>autocorrelation function</em> of $y$. Graphed as a function of the lag $k$ it is called the <em>correlogram</em>. If $y_{t=1,\dots} \ {\buildrel  iid \over \sim}\   F(y)$, that is the observations in the time series are independent.  Then $\rho_k=0$ for all $k>0$.  That is, a <b>necessary</b> but not sufficient condition for IID sampling is a zero autocorrelation function.</p>

<h4>Randomness that is anything but random</h4>

In many situations, when people talk about something being random they have in mind IID sampling and the implication of zero correlation.  For example the ball drawn in the lottery is random if the value is uncorrelated with the previous ball.  It is not important the draws are deeply random in some metaphysical sense, just unpredictable given previous draws and any other information someone might possess. Since we know that zero covariance does not imply independence, then zero autocorrelation for all lags does not imply IID sampling. That is, it is not a sufficient condition.  That is because covariance (and correlation) are measures of linear dependence.  There can be non-linear dependencies between the values in $y$ that are masked by a zero autocorrelation function. </p>

So this is the way a complex calculating machine like a computer, using an algorithm developed by a much more clever but much less efficient calculator called a human, can mimic randomness.   It is <u>not</u> mimicking randomness at all but it is mimicking independence by doing a great job of <u>mimicking zero autocorrelation.</u> Computers mimic zero linear correlation by using very non-linear functions.  In fact, notions of independence, predictability and non-linearity are all entangled with each other in the field of <em>chaos theory</em> and <em>non-linear dynamics</em>.  The brief summary of this is that some people see correlated time series and see randomness at work. Others see non-linear dynamics that looks like randomness but is not.</p>

<a name="Fig29"></a>
<div class="alg"><h4>Definition 29.  Features of a Pseudo-Random Number Generator (pRNG) </h4>
A pRNG is an algorithm that produces a list of numbers that have these features:
<OL class="compact">
<LI>The list is <em>recursive</em>: the algorithm uses where it is in the list to produce the next item (and then moves to that spot in the list). A pRNG has one input, called its <em>seed</em>. Setting the seed picks where to start. </LI>
<LI>The list is <em>circular:</em> starting from any point eventually to a value that produces the starting point. A good pRNG is a very long list; it starts repeating itself only after a lot of draws.  The length of the list is called the pRNG period. </LI>
<LI>The numbers all lie between 0 and 1 and are evenly distributed.  They look like the uniform distribution, so we will write $U \ {\buildrel  pseudo \over \sim}\  U(0,1)$ if  $U$ is a number produced using a pRNG.</LI>
<LI>The sample autocorrelation function of the list is <em>very</em> close to 0 for lags $k>1$ and less than the period.</LI>
</OL></div>

<a name="Fig16"></a>
<figure><h4>Exhibit 16.  The Autocorrelation Function </h4>
<img  width="80%" src="img/ACF.png" alt="ACF"/><figcaption>
<DD>The autocorrelation for two time series for the first 30 lags.  </DD>
<DD>The left is 10,000 draws from a pRNG, $u_{t=1,\dots,10000} \ {\buildrel  pseudo \over \sim}\  U(0,1)$. The very small spikes show that these numbers display essentially zero autocorrelation, a necessary but not sufficient condition for serial independence.  </DD>
<DD>The right graph is the autocorrelation of $y_t =0.2u_t-0.1u_{t-1}+0.4u_{t-2}$.  The correlations dampen down as the lag increases.</DD>
</figcaption>
</figure>

You can use $U$ to simulate a Bernoulli random variable:
$$Y = \cases{ 0  & if $U\lt 1-\theta$\cr
              1  & otherwise.\cr}$$
If $U \ {\buildrel  pseudo \over \sim}\  U(0,1)$ then $Y \ {\buildrel  pseudo \over \sim}\  Bernoulli(\theta)$. More generally, if you can compute $F^{-1}(x)$ you can simulate IID samples from $F(x)$.

<a name="Fig30"></a>
<div class="alg"><h4>Definition 30.  How to Simulate an IID Sample (continuous) </h4>
Given $X\sim F(x)$.  Set $r=1$ (the first observation).
<OL class="steps">
<LI>Draw $u^r \ {\buildrel  pseudo \over \sim}\  U(0,1)$.  This is one pseudo-random replication of a IID uniform random variable.</LI>
<LI>Find the value $x^r$ such that $F(x^r)=u^r$. That is, compute $x^r = F^{-1}(u^r)$. Based on the theorem above, $x^r$ is a simulated draw from the distribution $F(x)$:
    $$x^r \ {\buildrel  pseudo \over \sim}\  F(x).$$
    The inverse of the cdf can be computed either as a closed form for the inverse of the cdf or it can be computed $x^i$ numerically using computer algorithms.</LI>
<LI>Increment $r$ and repeat.  A pseudo IID sample is then $X_{r=1,\dots,R} \ {\buildrel  pseudo \over \sim}\  F(x)$.</LI>
</OL>
</div>

<a name="Fig17"></a>
<figure><h4>Exhibit 17.  Simulating a Normal Random Variable using Pseudo $U$.</h4>
<img  width="80%" src="img/pseudoN.png" alt="pseudo Z"/></figure>


<a name="Fig31"></a>
<div class="alg"><h4>Definition 31.  A Monte Carlo Experiment </h4>
Given a probability model and some statistic(s) based on the model to study (e.g. $\bar Y$, $\hat\theta^{mle}$ or LR for a $H_0$).
<OL class="steps">
<LI>Pick the value of the <em>true</em> parameter vector $\theta$ that will generate the simulated data. Choose the sample size $N$.  Set $R$, the number of replicated samples of size $N$.</LI>
<LI>Set the initial seed for the pRNG. Initialize $r=1$.</LI>
<LI>Use pRNG to simulate a sample of size $N$ from the probability model, compute the statistics and store them for analysis when the experiment is complete.</LI>
<LI>Set $r = r+1$.  Return to the previous step and repeat until $r>R$.</LI>
<LI>Graph and/or summarize the statistics: report them as $R$ replications of samples of size $N$ on the probability model with true parameters $\theta$.</LI>
</OL></div>

Many references call the true parameters in a Monte Carlo experiment $\theta_0$.  Then plain old $\theta$ can be any possible parameter value but not the particular one that is the true value.  There are good reasons to use either notation, but in these notes I try to use $\theta$ only for the true value and $\hat\theta$ or some other symbol for a parameter vector that is not necessarily the true value.  The probability model that is used to generate the simulated sample is called the data generating process (DGP).  A full understanding of small sample properties of a statistical procedure may require different experiments that vary $N$ and $\theta$.  $R$ is simply set big enough so that the distribution of the statistics is accurately filled in.  For example, R=10,000 would be more than enough replications for nearly anything you might want to simulate.  It will not take Stata or a similar program very long to carry out this many replications.</p>
<h2><a name="s045"><LI>More Stata</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s046"><LI>Must I Paint You a Picture?<br/>Graphing Data and Prediction</LI></a></h3>
<div class="break"></div><blockquote class="upshot"><h5>Must I Paint You a Picture?<br/>Graphing Data and Prediction</h5> is not ready. This page is left blank to provide some room for taking notes.</blockquote><div class="break"></div>
<h3><a name="s047"><LI>Something Stands for Nothing<br/>Handling Missing Values and Survey Data</LI></a></h3>

A data set will often not be complete in the sense that the value of some variables for some observations are missing. A piece of data may be missing because a person refused to answer the question or had not responded in time to have it included in this.  It could be missing because the concept did not make sense in this particular case and no numerical value fits.   Often Statistics Canada will release data sets where observations have been recoded as missing to protect confidentiality.  They do this based on models that determine whether a combination of variables could somehow reveal the respondents identity.  Finally, the data may have existed but just been lost or was not collected (asked) one time.</p>

In the Old Days, and still today sometimes, missing data is often marked with a special numerical value, like -99 or 999.99.  These kinds of values are often not valid measurements so they could be used to code missing data.
<DD><pre>
Obs    Age   Working   Wage
-------------------------------
 1      25     1        12.25
 2      17     0       -99
 3      32     1       -98
 4      22     1       -97
-------------------------------

-99 = N/A
-98 = Salaried worker, no hourly wage
-97 = Refused to Answer
 </pre></DD>
The Danger and Hassle of -99 as code for missing is that if the researcher forgets to do something about the missing values the statistical package may treat the code as an ordinary number.  Perplexing (and in some cases falsely interesting) can be generated by forgetting to deal with data where missing is coded as a number.  Since data sets still sometimes use numerical codes for missing values it is essential to read the data documentation and look at summaries of the data so that you do not accidentally treat a value like -99 as valid when it is not.  Remember, it is your job to know your data.</p>

Since 1985 or so, <span>computer hardware has special codes treated as "missing".  One such code is <dfn id="NaN">NaN</dfn>, which stands for Not A Number</span>.  A variable or matrix entry that contains <code>.NaN</code> will not be treated like any ordinary value (like -99). The period is put in front of NaN so that you don't confuse this with a variable that just happens to be named <code>NaN</code>.  As a user of Stata, you will not see <code>.NaN</code>.  Data or results may actually equal <code>.NaN</code>, but Stata will recode these to other values before you see them on the screen.</p>

Any arithmetic operation involving <code>.NaN</code> results in <code>.NaN</code>.  For example, <code>4 + .NaN = .NaN</code>.  So if your estimation sample contains .NaNs your econometric results will include <code>.NaN</code>s.  You should then realize that something is wrong.  For example, here is a researcher who calculated the logarithm of Wage and forgot that negative values mean missing.  Now average log wages are undefined:
<DD><pre>
Obs    Age   Working     Wage       lnWage
----------------------------------------------
 1      25     1          12.25     2.5055259
 2      17     0         -99        .NaN
 3      32     1         -98        .NaN
 4      22     1         -97        .NaN
-----------------------------------------------
. average(lnWage) = .NaN
 </pre></DD>
There are different reasons why data is missing, and <code>.NaN</code> cannot distinguish between these reasons.  (Above, <code>-98</code> and <code>-99</code> kept track of the reason data was missing, but exclusive use of <code>.NaN</code> loses this information. In the example we may want to know the average hourly wage for those who are paid a wage.  </p>

In its early versions Stata used a period <code>.</code> to mean <q>missing</q>.   This is like <code>.NaN</code>, in that any operation  involving missing results in missing.  But Stata will try to be smart about whether the operation should proceed with the non-missing values.
<DD> <pre>
Obs    Age   Working   Wage
-------------------------------
 1      25     1        12.25
 2      17     0       .
 3      32     1       .
 4      22     1       .
-------------------------------
average(Wage) = 12.25 because Stata will usually perform calculations without using missing values.
 </pre></DD>

Not all Missings are the same. <span>Current versions of Stata continue to use <code>.</code> for missing, but now multiple values of <dfn id="missing">Stata missing codes</dfn> are available as <code>.a</code>, <code>.b</code>, etc.</span>. These special missing values allow the data to store the information for why data is missing (like -99 versus -98) but at the same time safeguard arithmetic operations from using the codes like ordinary numbers.
<DD><pre>
Obs    Age   Working   Wage
-------------------------------
 1      25     1        12.25
 2      17     0       .a
 3      32     1       .b
 4      22     1       .
-------------------------------
 </pre></DD>
Here, <code>.a</code> has been used to mean "missing because the concept of wage does not mean anything when the person is not working."  And <code>.b</code> means "missing because the person is not paid by the hour."   Finally, plain vanilla <code>.</code> means missing because the information really is just missing because the person did not answer.

<DT>Valid and Invalid Skips</DT>

Statistical models do <b>not</b> work with missing values.  They are mathematical formula, and <em>any</em> arithmetic operation that involves a missing value will itself be missing.  You must either: <em>eliminate</em> or drop observations (rows) that contain missing values before sending them to a model. In fact, <code>Stata</code> can sometimes be too helpful in this regard as it will drop observations that include missing values for you.  Sometimes this is helpful, but other times it can lead to confusion.  Or, you must <em>recode</em> missing values to a numerical quantity that makes sense before sending data to a model. </p>

In a survey conducted by phone, someone is asked "Do you have children" and they reply "No".  The survey software will then automatically skip  questions about children because the questions make no sense for this household. The answers should be considered <em>valid</em> skips for that person.  <span>In survey data, a <dfn id="valid">Valid Skip</dfn> creates missing information because the question was skipped for a valid reason, because, based on other questions, we know the answer or the question is meaningless</span>. So if the next question to be asked is "How many" the survey software will recode a missing value because the person did not answer.  But it is a valid skip because we know the answer based on the previous question.  </p>

In the same survey a person is asked, "How much money did you earn last year" and they reply <q>None of your business</q> and then hang up. The survey software will record a missing value, but this is not a valid skip.  <span>In survey data, a <dfn id="invalid">Invalid Skip</dfn> creates missing information because the question was not asked and we cannot infer the answer from other questions.</span> All questions to be answered after the person refused that required answer will also be coded as invalid skips.</p>

A third reason variables may be missing is censoring for privacy reasons, something Statistics Canada is very concerned (paranoid) about.  So even if the value was not missing in the master files they may be missing in the file that you have access to. 

<DD><code>mvencode</code> and <code>mvdecode</code> are commands that allow you to convert special codes such as -99 to Stata missing values and back.</DD>

<mark>
<DT>Rule of Thumb:</DT>
 <blockquote>Drop <em>observations</em> that have <em>Invalid</em> Skips for any Variables <em>used</em> in your econometric analysis.</blockquote></mark>

If an observation contains missing values in a variable you plan to use and it is due to an invalid skip then this observation cannot be sent to the estimation procedure.  You should drop this observation from the data set before estimation.  Sometimes it is easier to describe what is a valid observation and then drop any observations that do not satisfy that criterion.  So, getting rid of invalid skips can involve either <code>drop if</code> or <code>keep if</code>. </p>

<h3><a name="s048"><LI>Including Dummy/Indicator/Factor Variables in a Model</LI></a></h3>

Discrete variables in data sets often code categories of things, such as sex, race, industry, etc.  This kind of categorical information does not have <em>units</em>.  For example, if sex is coded 0 for male and 1 for female this does not mean that females are more than males. The values 0 and 1 do not mean anything themselves.  And, suppose we are coding countries within NAFTA, with USA, Mexico and Canada coded as 0, 1 and 2, respectively.  This does not mean that Mexico is somehow half-way between USA and Canada in any sense.  It just happens to have the code 1 which is halfway between the codes for the other countries.  </p>

Note that if we call a variable <code>sex</code> then we also have to tell the reader which way it is coded (could be 0=male or 0=female).  One way to avoid this is to use names of variables that tell you how they are coded.  So instead of using the variable name <code>sex</code> we could use the name <code>female</code> and this would in itself tell the reader that a value of 1 means the person is female and 0 means male.</p>

Someone writes does the regression:
$$Y = \beta_0 + \beta_1 Female +  \sigma z.$$
How do we interpret the coefficient $\beta_1$?  We can't say it is the difference between a man and woman, because each observation has their own disturbance term, $z$.  So some men in the data will have a greater value of $Y$ in the data than some women, and vice versa.  What $\beta_1$ tells is the <em>expected</em> difference in $Y$ between men and women:
$$E[Y | Female] = E[\beta_0 + \beta_1 Female +  \sigma z\  |\ Female] = \beta_0 + \beta_1 Female + 0.$$
So there are two expected values:
$$\eqalign{
    E[Y | Female=0 ] &= \beta_0 + \beta_1 0 = \beta_0\cr
    E[Y | Female=1 ] &= \beta_0 + \beta_1 1 = \beta_0+\beta_1\cr
    }$$
We can then use these equations to say what the regression says about the differences between men and women:
<DD>Average Difference Between Women and men =<br/>
$E[Y | Female=1] - E[Y | Female =0]$<br/>
$= \beta_0+\beta_0 - \beta_0$<br/>
$= \beta_1.$</DD>
So the coefficient on the Female dummy variable is the population difference in average outcomes.  Why don't we include a dummy variable for <em>both</em> men and women?  As in:
$$ Y = \beta_0 + \beta_1 Female + \beta_2 Male + \sigma z .$$
The problem is that this specification creates exact linear dependence in a $X$ matrix, because for every observation $Female + Male$ equals either $0 + 1$ or $1 + 0 $, so it is always equal to the constant term, 1. So with two categories we would include an indicator for one of the categories but not both.  (If for some reason the constant term was excluded from the model then you could include indicator variables for both categories, but this is not typical).  </p>

Now consider the case of country with three possibilities:
<DD><pre>
Country      Means
   0         USA
   1         Mexico
   2         Canada</pre></DD>
If we ran this regression in a pooled time series:
$$RealPerCapitaGDP = \beta_0 + \beta_1 t + \beta_2 Country + \sigma z_{it}.$$
Would this make sense?  Because GDP is in units of, say, US dollars, the right hand side should be as well.  So the coefficient on time $t$ would means we are assuming that GDP is following a linear trend in time.  Maybe this is okay or not &hellip; this is not the issue.  The issue is what does including Country in the model say?  It says that we are assuming that Mexico's RealPerCapitaGDP is half-way between the USA and Canada in every year.
<DD>$E[ RPCGDP | t, Country=USA ] = \beta_0 + \beta_1t + \beta_2 0$</DD>
<DD>$E[ RPCGDP | t, Country=Mexico ] = \beta_0 + \beta_1t + \beta_2$</DD>
<DD>$E[ RPCGDP | t, Country=Canada ] = \beta_0 + \beta_1t + 2\beta_2$</DD>
Or
$$\eqalign{
{E[ RPCGDP | t, Country=USA ]+E[ RPCGDP | t, Country=Canada ]\over 2}
&= {(\beta_0 + \beta_1t + 0 + \beta_0 + \beta_1t + 2\beta_2)\over 2}\cr
&= (\beta_0 + \beta_1t + 1\beta_2)\cr
&= E[ RPCGDP | t, Country=Mexico ].\cr}$$
We know the data will not look anything like this but if we ran this regression in Stata it would impose that relationship on the estimated values, because including Country in the regression as coded assumes the relationship in the population.</p>
A way to see why this makes no sense (and might help you avoid making this mistake in other contexts) is to see that Country is a variable that has no units of measurement.  It is simply a categorical variable like Female above.  Including Female in the regression was okay because it is <em>a binary variable</em> so it only takes on values 0/1, on/off, yes/no, etc.  But Country takes on 3 values and no matter how we code it there will be an ordering implied.  One country would be in between the other two.</p>
The solution is to <em>include indicators for different categories</em> in the regression.
<DT>Indicator Variables for Country</DT>
<DD><pre>
Country      Means        MEXICO     CANADA    USA
   0         USA            0           0       1
   1         Mexico         1           0       0
   2         Canada         0           1       0 </pre></DD>
Consider this regression:
$$RealPerCapitaGDP = \beta_0 + \beta_1 t + \beta_2 MEXICO + \beta_3 CANADA + \sigma z_{it}.$$
Now we specified a regression where the only categorical variables are binary.  We have excluded one of the categories, in this case USA.   We say that USA is the default country, the base category, or the excluded value (you will see all various combinations of those words). And we can interpret the coefficients:
<DD>$\beta_2$ = Expected difference in RPCGDP in Mexico <em>relative</em> to the USA in the same year t</DD>
<DD>$\beta_3$ = Expected difference in RPCGDP in Canada <em>relative</em> to the USA in the same year t</DD>

Notice that the names of the variables tell you what category they code for.  So an econometrician could look at the specification and see how to interpret $\beta_2$ and $\beta_3$.  But if the variable <code>Country</code> were included an econometrician would not have complete information and they might worry that the specification makes the regression results meaningless. </p>

Now consider a very large example.  There are 312 different 4-digit industry categories in the North American Industry Classification System (NAICS).
<DD><pre>
 k   Code       Description
------------------------------------------------
  0    1111	    Oilseed and Grain Farming
  1    1112	    Vegetable and Melon Farming
  &vellip;      &vellip;           &vellip;
310     9271	Space Research and Technology
311     9281	National Security and International Affairs
</pre></DD>
By now you should not be tempted to simply include such a variable in a regression, because National Security is not 9 times the industry that Oilseed farming is ... their codes are just categories.  If you had a large sample of firms so that you had some firms in every 4-digit industry then could consider including an indicator variable for each except one:
$$Profit = \beta_0 + \beta_1 VegFarm + \dots + \beta_{311} NatSecurity + \sigma z.$$
However, you and your reader probably do not care about each and every coefficient and the name of the industry. There are two better ways to describe this model than naming each one or using &hellip; as above.
First, you can say that $Ind_k$ is defined as an indicator for whether a firm is in industry $k$ or not.  That is, in the data it is a column of 0s and 1s, and a 1 appears in rows where the firm is in industry $k$.  Then you can write that same regression as:
$$Profit = \beta_0 + \sum_{k=1}^{311}\beta_k Ind_k + \sigma z.$$
The use of the subscript $Ind_k$ makes it easier for the reader to see that these are all indicators for individual industries, not a potential wrong inclusion of the NAICS codes themselves.</p>
Another way to describe the same regression is to use matrix notation.  Instead of defining individual industry indicators, define a matrix of indictors.  That is, define $IndCat$ as $N \times 311$ matrix of indicators for industries.  So the $k^{th}$ column of $IndCat$ is the same as $Ind_k$.  Then instead of using the summation sign, use matrix multiplication:
$$Profit = \beta_0 + IndCat\gamma + \sigma z.$$
 Here one other thing was changed: instead of calling each coefficient $\beta$ the coefficients on industry are called $\gamma$.  This lumps together all the industry coefficients and separates them from other coefficients that might be important in themselves.  If other variables were added to this regression you wouldn't have to start numbering their coefficients at 312.  Just give them a different Greek letter.</p>

<DT>To account for a categorical variable in an econometric model that takes on $K$ values in the data, include dummy or indicator variables for $K-1$ of the categories in the $X$ matrix.</DT>

<h4>Creating Indicator Variables for a Categorical Variable in Stata</h4>

Okay, so you have a data set with a variable like <code>Country</code> above.  How would you make it so that you have categorical variables instead.
<dd><pre>
    &bull;  gen MEXICO = Country==1
    &bull;  gen CANADA = Country==2
&vellip;
    &bull;  regress RPCGDP t MEXICO CANADA
</pre></dd>
Or you can use the <code>gen()</code> option to the tabulate command.
<dd><pre>
    &bull;  tab Country, gen(dum)
&vellip;
    &bull;  regress RPCGDP t dum*
</pre></dd>
There are many reasons you want not want to use either of these options for a large category like four-digit industries.  First, you do not need nor want to name each of the 312 industries.  The <code>tab , gen()</code> alternative avoids this by giving them numerical names that are analogous to $Ind_k$.  However, your data set now has 312 extra variables!  All the information had been contained in on variable (Industry) and that compact storage has be blown out to 312 variables.</p>
Stata allows you to include indicator variables in a model without having to store them in the data set.  It creates the extra columns in the $X$ matrix for you:
<dd><pre>
    &bull;   regress profit i.industry
    </pre>
</dd>


<h3><a name="ex036"><LI>Exercises for <em>How To Do It<br/>Data &amp; Stata</em></LI></a></h3>
<OL class="exer">
<LI>Use what you have learned about getting help in Stata to learn about <em>log files</em>, <em>command log files</em> and <em>viewing</em> and <em>exporting</em> log files.  </LI>
<LI>Put commands in a do file to &hellip;
<DD>Start a log file</DD>
<DD>Display the value of ${1\over \sqrt{2\pi}}e^{-2}$</DD>
<DD>Close the log file</DD></LI>
<LI>Save the file, then Do (or run) the do file</LI>
<LI>View the log file and fix any mistakes</LI>
<LI>Even if it worked the first time, do the file a second time. If it fails, modify the file so it will work regardless of whether it was run before.
<DD>Hint: you may need to learn about the <code>replace</code> option.</DD></LI>

<LI>Use <code>display</code> to display </DT><DD><pre>&quot;The answer is <var>(2&pi;)<sup>-1/2</sup> e<sup>-2</sup></var> &quot;</pre></DD>
<DD>That is, change &quot;2+2&quot; above to what is required to get Stata to compute ${1\over \sqrt{2\pi}} e^{-2}$ and display it on the screen.</DD>
<DD>This requires using help or just exploring how to do this.  It might take some time.</DD>

<li>Here is a sample of size 7:
<DD><pre>
7.1
-.4
2.2
0.88
1.11
3.6
2.4
</pre></DD>
Type the command <code>&bull;  edit</code> to open the spreadsheet-like data editor.  Enter those numbers in the first column then type exit (File Exit or click on the X in the upper-right corner of the window.)  Then use Stata help to find a command that will let you <em>a t-test of the hypothesis that the population mean equals 2.0</em>.  Run the test and draw the correct conclusion from the output (reject or fail-to-reject) given $\alpha=.1$.
</li>
<li>Here is a small data set with 3 variables and five observations.
<DD><pre>
n     x        y       z
1
2
3
4
5
    </pre></DD>
Load the data into Stata and save the data in Stata format with the name: <code>myfirst.dta</code>.  Then <code>exit</code> Stata.  Start Stata again and <code>use marist</code>.  (Do not use the Open menu command.  Learn to use commands like <code>use</code>. Confirm that all the data are the same as you entered.  If you have problems, think about these issues:
<DD>Where on my computer did <code>myfirst.dta</code> get saved?  That is, which folder?  </DD>
</li>

<li> Do this:
<dd><pre>
&bull;  CF452
&bull;  sysuse nlsw88.dta , clear
</pre></dd>
Save the data to a Stata file named <code>mysecond.dta</code>.  Exit Stata.  Start Stata again and <code>use mysecond</code>.  Confirm the data are the same. </li>
<LI>Using the information about GENDER in the data set determine how many observations in the data set are female.</LI>
<LI>Find out what industry is coded as <q>5</q> in the <code>nlsw88.dta</code>.</LI>
<LI>Use <code>codebook</code> to find at the <em>median</em> number of usual hours worked in <code>nlsw88.dta</code>.</LI>
<LI>When estimating an econometric model Stata will automatically drop observations with missing values, so why does it make sense to drop the observations myself?</LI>
X--> 

</OL>
</OL>
</OL>
<h1><a name="s049"><LI>Group Tournaments, Partner Tasks, Course Project</LI></a></h1>
<blockquote class="toc"><h4>Contents</h4>
<OL type="A" class="toc2">
<LI><a href="s051.html" target="contentx">In Class Demos and Tutorials</a></LI>
<LI><a href="s053.html" target="contentx">In Class Group Tournaments</a></LI>
<LI><a href="s058.html" target="contentx">Takehome Partner Tasks</a></LI>
<LI><a href="s062.html" target="contentx">Quizzes and Optional Stata Challenges</a></LI>
<LI><a href="s066.html" target="contentx">Course Project</a></LI>
</OL>
</blockquote>
<OL  type="A" class="toc2" >
<h2><a name="s051"><LI>In Class Demos and Tutorials</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s052"><LI>LR Tests and Robust Standard Errors</LI></a></h3>
<h4>What To Do Before the Demo</h4>
<div id="split">
<OL class="steps">
<LI><DD><pre>
. get452 Demo week7
</pre></DD>
<DD>The data come from the Joint Canada-U.S. Health Survey, 2004 <code>http://search1.odesi.ca</code></DD>
<DD>Missing observations have been dealt with.  Variables have been renamed for clarity.</DD>
<DD>view week7.smcl to see overview of the demo sample</DD>
</LI>
<LI>commands and options to look up in Stata help:
<DD><pre>
. estimates store <em>name</em>
. lrtest <em>unrestricted</em> <em>restricted</em>
. testparm  <em>factor-varlist</em>
. regress .... , robust
</pre></DD>
</LI></OL>
<h4>Part I.</h4>
<OL class="steps">
<LI value="0">Recode hasdoc so it takes on 0/1 values not 1/2 values.</LI>
<LI>Run a probit model on hasdoc that allows health status to depend on: country, age, sex, marital status
        income quintile, allowing quintile to interact with country.  </LI>
<LI>Store the estimates from 1.</LI>
<LI>See how has-doctor probability differs by country, age, sex, income holding all else constant?  Are signs and their
significance as expected?</LI>

<LI>Run a restricted probit in which income quintile still affects the probability but it does not interact with country.</LI>
<LI>Store the estimates from 4.</LI>
<LI>Use lrtest to test whether access to a (regular) doctor depends.  Explain ...</LI>
<LI>Use testparm to test whether doctor access is related to income in the restricted model.</LI>
</OL>

<h4>Part II.</h4>
<OL class="steps">
<LI value="0">Create new a variable that collapses health to a binary outcome (1=VERY GOOD or EXCELLENT)</LI>
<LI>Repeat steps 1-7 in Part I for health instead of has-doctor.  </LI>
<LI>Summarize cross-border differences in health and health-care access.</LI>
</OL>

<h4>Part III.</h4>
<OL class="steps">
<LI>Estimate a regression of BMI on the same specification as the restricted model in the previous part.</LI>
<LI>Are Americans fatter than Canadians, all else constant?  What else explains obesity?</LI>
<LI>Re-estimate with robust standard errors and compare standard errors and t tests.</LI>
</OL>    
</div>
</OL>
<h2><a name="s053"><LI>In Class Group Tournaments</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s054"><LI>Don't Look Back in Anger: Econ 250/351 Review Questions</LI></a></h3>

<!--<h5>What To Do Before the Tournament</h5>
<OL class="compact">
<LI>At onQ/452, click on Communications-&gt;Groups.  See which group number you were randomly assigned to under <q>Group Tournament A</q> (1-6).</LI>
<LI>Review Your Notes, Textbook, Exams, etc. from Econ 250/351 (or equivalent pre-requisite course material).</LI>
<LI>Skim the Review &amp; Reenforcement of Econ section of these notes.</LI>
<LI>Be ready to show what you remember from previous courses.</LI>
</OL>
<h5>What To Do During the Tournament</h5>
<OL class="compact">
<LI>No notes, phones or calculators allowed.  You will be provided scrap paper and exam booklet for your group's answers.  Your group can also use the whiteboard nearby.</LI>

<LI>Assign one group member to be the Writer in the group to write answers in the exam booklet.  </LI>

<LI>Assign a different student to be the Reader who gets the question sheet and reads them out to the group.</LI>
</OL>

<HR/>-->
<h5>Question I.</h5>

Here are latest census numbers on # of internet-connected devices in households (q).  $f(q)$ is fraction of
households with that number.  $F(q)$ is cumulative distribution of $q$.  Provide the values (A)-(G)
<DD><pre>
    q       f(q)         F(q)
    -------------------------
    -1       0            (B)
     0      .1
     1      .15
     2      .03           (C)
     3      .08
     4      .15
     5      .22           (D)
     6      .17
     7      .11
     8+     (A)           (E)
</pre></DD>
<DD>(F) What is the median # of devices?</DD>
<DD>(G) If you can compute the average # of screens what is it?  If not, why not?</DD>

<HR/><h5>Question II. Provide numbers (A)-(H) and answer (I)</h5>
Latest census results number of cars (X) and swimming pools in houses (Y).  Numbers in the table are the fraction of households with the corresponding numbers of cars and pools.  f(y) is the "marginal" distribution of Y and f(x) is the marginal distribution of X.  
<dd><pre>                        CARS (Y)
                0       1       2     f(y)

        0      0.15    0.35    0.3     (A)
POOLS
 (X)    1       0      0.12    0.08    (B)

        f(x)    (C)     (D)     (E)</pre></dd>
<DD>(F)  The probability that Y=1  conditional on X=2, $f(1 | Y=2) = $</DD>
<DD>(G)  <em>Average</em> or <em>expected</em> number of cars, $E[X] = $</DD>
<DD>(H)  The numerical expression for the expected cars conditional on Y=1, $E[X|Y=1] =$</DD>
<DD>(I)  Are Cars and Pools <em>independent</em> random variables?  Briefly explain why or why not.</DD>

<HR/><h5>Question III.</h5>
<OL class="steps">
<LI>Write down the (simple, 2 variable) linear regression model (LRM).  As a hint here are some symbols you might need/use:
$Y\quad  \epsilon \quad \beta  \quad i  \quad  0 \quad= \quad 1 \quad X \quad \sigma$</LI>
<LI>Write down the assumptions of the <em>Gauss-Markov Theorem</em> about the LRM.</LI>
<LI>The conclusion of the Gauss-Markov Theorem can be summed up as <q>OLS is BLUE</q>. Briefly explain what that means.</LI>
</OL>

<HR/><h5>Question IV. Briefly explain the difference between these pairs of concepts.</h5>

<DD><pre>A.    estimate              vs.     estimator
B.    parameter             vs.     estimate
C.    Type I error          vs.     Type II error
D.    significance          vs.     power
E.    standard deviation    vs.     standard error
F.    null hypothesis       vs.     alternative hypothesis
</pre></DD>

<HR/><h5>Question V.</h5>

<DT>Let $f(\theta) = \theta^2 \left(1-\theta\right)^3 $</DT>

<DD>(A) What value(s) of $\theta$, $\theta^\star$, maximize f?</DD>

<DD>(B) What is the max value, $f(\theta^\star)$.</DD>

<DT>Let $g(x) = {2\over 5} \ln(x) + {3\over 5}\ln(1-x).$</DT>

<DD>(C) What value(s) of $x$ maximize $g(x)$?</DD>

<DD>(D) Explain the relationship between (A) and (C).</DD>

</div>
<h3><a name="s055"><LI>Should Have Been an Engineer Tournament</LI></a></h3>

<h4>What To Do Before the Tournament</h4>
<OL class="compact">
<LI>Go through the Preliminary Analysis below</LI>
<LI>Review Stata commands mentioned below using the notes and other help resources.</LI>
<LI>Be ready to learn what the data say about graduates by answering questions provided at the Tournament.</LI>
</OL>

<h4>What To Do During the Tournament</h4>

<OL class="compact">
<LI>Assign one group member to be the Writer in the group guide the group through the "spot" questions and to write answers in the exam booklet.  This student should not try to use their notebook at the same time.  They should look on with another student.</LI>

<LI>Assign a different student to be the Uploader, the ONE student who uploads a Stata log file to the Dropbox for the group.  This student should be (relatively) comfortable with Stata, uploading, etc.  You are depending on them.</LI>

<li>Other students can/should use their own computer to access the data, try things, check results produced by the Uploader, etc.</li>


</OL>

<h4>Preliminary Analysis</h4>

<UL>
<LI>The data are a few selected variables from the National Graduate Survey, 2005:
<code>http://search1.odesi.ca/#/details?uri=%2Fodesi%2Fngs-81M0011-E-2005.xml</code></LI>

<LI>Get the package of material:<DD><pre>
    &bull;  get452 Tournament B
</pre></DD></LI>

<LI>PDF files with documentation of the survey are available as part of the Tournament B package also on your computer (installed with the lab2 package, same directory as Stata works in).</LI>

<LI>Skim the documentation to understand the population the sample is drawn from and how the survey was conducted.</LI>

<LI>Load the data:</LI>
<DD><pre> &bull; use TourB </pre></DD>

<LI>Look at the data using commands such as </LI>
<dd><pre>summary   describe   tabulate  codebook</pre>
You can also use the Variable manager and Data Browser. You need to look at <em>value labels</em> to interpret the code used for variables.</dd>

<LI>Learn about <em>missing values</em> and how they are coded in Stata from the course notes and elsewhere. In particular, read through the notions of <em>valid</em> and <em>invalid</em> skips discussed in the notes.</LI>

<LI>Look at the variables <code>SL_Q07P</code> and
<code>CNDJOBSP</code>, which <em>categorize</em> continuous dollar amounts.  That is, the person answered with a dollar amount (say 	\$16,500), but by the time the PUBLIC USE version of the data are released their answer was replaced with a coded value (for example, 8).   Your group will be asked to recode these variables so that they are in units of thousands of dollars. Think about that before the tournament.</LI>

</UL>

<h4>Questions to Answer on paper, backed up by your uploaded log file</h4>

<div class="bigfont">
<OL class="steps">
<h5>Part I</h5>
<span class="hidden">
<LI>Excluding <em>invalid</em> missing observations, what fraction of respondents borrowed money from the government for their education?</LI>

<LI>What fraction of those who did borrow from the government owed nothing on their government loans at the time of graduation?</LI>

<LI>Come up with a way to RECODE loans so that it is in units of thousands of dollars. That is, decide a way to convert the code 8 back to a dollar value that is close to the original answers as possible. (There is not one 'right' answer to this, but some recodings make more sense than others.) Briefly describe how you recode the data in your paper answer.</LI>

<LI>Deal with values of loans that are coded missing. Valid Skips should be counted as 	\$0, other should stay missing. Accounting for valid skips and excluding invalid skips, compute the average value of loans owed at graduation within the sample. Report your answer to decimal places (e.g. 4.65, which means 	\$4650).</LI>

<li>Tabulate <code>PR_Q05B</code>.  Go to PDF documentation about that variable.  Discuss in your group whether the tabulation makes sense and whether the variable should be used in, say, a regression. Write a brief summary of your discussion.</li>
</span>
<h5>Part II</h5>
Save your data in a file so you can get back to where you were before answering the next questions
<pre>
   &bull; save TourBpart1
</pre></dd>
<span class="hidden">
<DT>We want to focus on categories related to </DT>
Economics, and as a comparison, Engineering, and for people in the survey because they completed a Bachelor's degree.</DT>

<DT>Use <code>keep if </code> or <code>drop if </code> so that your data set only contains respondents
<DD>whose level of certification is
<pre>Bachelor/first prof. degree/...</pre>
and who graduated in one of 3 aggregate programs:
<pre>
    Social and behavioural Sciences, etc.
    Business, management, etc.
    Architecture, engineering, etc.
</pre></DD></DT>

<LI>Report gender ratios by program (so they will appear in your  log file) and write them on paper like this:
    <DD><pre>
                     Ratio of Women to Men Graduates
    Soc Sci                    ?.??
    Business                   ?.??
    Engineering                ?.??
        </pre>
    </LI>

<li>Report the percentage of graduates in each program making $50,000 or more in 2006 (accounting for valid skips). Do
    it for all graduates and for men and women separately. Write them on paper like this.
    <DD><pre>
                      Pct. Making over $50K
                   ALL          MEN         WOMEN
    Soc Sci        ??.?         ??.?        ??.?
    Business       ??.?         ??.?        ??.?
    Engineering    ??.?         ??.?        ??.?
</pre></DD></LI>
</span>
Save your data in a second file so you can go back and forth between Part I and Part II.
<pre>
   &bull; save TourBpart2
</pre></dd>
</OL>
<DT>Upload a log file to the Tournament B DropBox that shows how your written answers are computed. Don't worry if the log file contains mistakes, restarts, etc.</DT>
</div>
<h3><a name="s056"><LI>Levittation Tournament</LI></a></h3>

<h4>What To Do Before the Tournament</h4>
<OL class="compact">
<LI>Go through the Preliminary Analysis below</LI>
<LI>Review Stata commands mentioned below using the notes and other help resources.</LI>
</OL>

<h4>What To Do During the Tournament</h4>

<OL class="compact">
<LI>Assign one group member to be the Writer in the group guide the group through the "spot" questions and to write answers in the exam booklet.  This student should not try to use their notebook at the same time.  They should look on with another student.</LI>

<LI>Assign a different student to be the Uploader, the ONE student who uploads a Stata log file to the Dropbox for the group.  This student should be (relatively) comfortable with Stata, uploading, etc.  You are depending on them.</LI>

<li>Other students can/should use their own computer to access the data, try things, check results produced by the Uploader, etc.</li>

<li>Your group's paper submission should summarize your choices/decisions. Because you will also submit a log file, the written part does not have to be exhaustive or detailed.</li>

<LI>Both the log file and the written version can include false starts, corrections, etc.</LI>

</OL>

<h4>Preliminary Analysis</h4>
<UL>
<LI>Get the package of material:<DD><pre>
    &bull;  get452 Tournament C
</pre></DD></LI>
</UL>

<h4>Questions to consider and answer on paper, backed up by your uploaded log file</h4>
<DL>

<DT>What variables might be endogenous (Y)?</DT>
    <DD>At some point your group has to decide on at least $Y$ variable</DD>
    <DD>Your group can study multiple $Y$ values if doing so is interesting</DD>

<DT>Which variables might influence $Y$ (be included in the $X$ matrix)?</DT>
    <DD>At some point your group has to decide on $X$ variables to include</DD>
    <DD>Your group can report different X's if comparisons are interesting</DD>

<DT>What econometric model makes sense for Y?</DT>
    <DD>Essentially choose between regression- or probit-based estimation</DD>

<DT>What specification makes sense?</DT>
    <DD>Transformation of Y?  Units?  Scale?</DD>
    <DD>Transformation of X variables? Units? Scale? Interactions?</DD>
    <DD></DD>

<DT>What structure for the errors makes sense for this data?</DT>
    <DD>We have talked about different ways to correct for error terms that do not satisfy the Gauss Markov Theorem assumption. Your group might decide that these methods would make sense here.</DD>

<DT>What might we learn from the results?</DT>
    <DD>Your group should settle on one (or at most a few) questions to pose.  You might not decide the question is until you do some preliminary analysis.</DD>

<DT>What did we learn from the analysis?</DT>
    <DD>Summarize the answers/conclusions about questions you pose based on your analysis.</DD>
</DL>

<div class="break"></div>

<h4>Questions to Answer</h4>
<em>You can answer on paper and/or upload typed answers to the dropbox, backed up by your uploaded log file.  You have until 3:30pm today to upload files, but earlier timestamps on files are better.</em>
<div class="bigfont">
<OL class="steps">
<h5>Part I</h5>
<LI>State the model (or possibly models) you estimate on the data.  Do <em>not</em> use Stata syntax for this, use mathematical notation as done in the notes and class, but make sure I know how to map the variables in your equations to the data. </LI>

<LI>Describe transformations of the original data required to estimate your equation(s) in Stata.</LI>

<LI>Briefly discuss (bullet points are okay): what you expect to learn from your analysis; why $X$ variables can/should be thought of as exogenous to your $Y$ variable; why the direction of causality might be unclear for some variables; signs you expect on estimated coefficients ($\hat\beta$'s)</LI>

<LI>Some $X$ variables just make sense to include because the <q>obviously</q> should influence $Y$.  But which coefficients are worth testing whether their values take on particular values (possibly 0 or maybe equality of two coefficients).  </LI>

<LI>Estimate your model in Stata.  Briefly describe / interpret your results and whether they contained surprises and what happened to your formal tests?</LI>

</span>
<span class="">
</OL>
<DT>Upload a log file to the Tournament C DropBox that shows how your written answers are computed. Don't worry if the log file contains mistakes, restarts, etc. <em>If you upload a Stata DTA file, use <b>saveold</b> to save the file so I can read in Stata 13.</DT>
</div>
<h3><a name="s057"><LI>Dissect an Applied Econometrics Article</LI></a></h3>
<div class="break"></div><blockquote class="upshot"><h5>Dissect an Applied Econometrics Article</h5> is not ready. This page is left blank to provide some room for taking notes.</blockquote><div class="break"></div>
</OL>
<h2><a name="s058"><LI>Takehome Partner Tasks</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s059"><LI>Bill Joel Sociology</LI></a></h3>

<blockquote class="upshot"><h5>Only The Good Die Young</h5>
<pre><em>Come out, Virginia, don't let me wait
You Catholic girls start much too late
But sooner or later it comes down to fate
I might as well be the one
They showed you a statue and told you to pray
They built you a temple and locked you away
But they never told you the price that you pay
For things that you might have done...
Only the good die young</em></pre></blockquote>

<h4>Research Question</h4>

Although it is set up as a test of a hypothesis, the real purpose of this task is to
<UL>
<LI>Get you to learn how to handle survey data</LI>
<LI>Read survey data documentation carefully to understand what questions means</LI>
<LI>Use Stata commands to manipulate the data so that it matches the concept of interest</LI>
<LI>Ensure statistics you produce are representative of the population you want to study</LI>
</UL>
Was Billy Joel a good sociologist? Do (or did) Catholic girls <q>start late</q>? &hellip; nudge, nudge &hellip; this was how pop songs of the 1970s talked about sex. Specifically, do teenage girls/women raised Catholic begin to have sexual intercourse at a later age than girls raised in a different religion?</p>

<DD>Questions left open from the lyrics</DD>
<UL>
<LI>Should we compare Catholics to other Christian denominations, all other religions or everyone else?</LI>
<LI>Does Prof. Joel think this effect, if it exists, is <em>caused</em> by the faith itself or other factors that just happen to be correlated with religion?</LI>
<LI>Billy Joel was writing about teenagers in the 1960s, almost 50 years before your generation.  Was his theory applicable only to girls he knew or more broadly over time and place?</LI>
</UL>

How To Answer This Question With Statistics? At a minimum, data is need for women on their religious upbringing and first sexual experiences. The data should be representative of a population (responses to non-random surveys would not settle the matter). Since not all Catholics will not <q>start later</q> than all those in the comparison group we have to chose a way to compare the distributions.</p>

The data to use come from the U.S. <a href="https://www.nlsinfo.org/">National Longitudinal Survey of Youth 1979 (NLSY79 )</a>, <code>https://www.nlsinfo.org/</code>. The data are a representative sample of American youth age born 1957-1964 (compared to you and Billy Joel, this is m-m-m-m-my generation).There is oversampling of some groups so to make inferences about the population sampling weights must be used.
This data set is one of the most important sources of economic and sociological data in the world.  The <a href="https://nlsinfo.org/bibliography/search/cohort=NLSY79">NLYS79 bibliography</a> includes over 5200 published papers, including one by <a href="https://nlsinfo.org/bibliography/search/author=Ferrall%2C+Christopher">yours truly</a>. The NLS cohorts are one of the recommended data sources for your Course Project.</p>

<h4>What To Do</h4>

<OL class="steps">
<li>Join one of the Billy Joel Sociology Partnerships at onQ. If you are working with a partner you both join the same group.  This provides you with a "locker" where you can upload files to share with each other.  Your group also as a "dropbox" that you will use to upload your reports</li>

<LI>Get the material and load the data:</LI>
<DD><pre>
    &bull;  CF452
    &bull;  get BillyJoel
</pre></DD>
The data extracted from the NLSY site is in <code>BillyJoel.dct</code>, but the data have been read in and labeled and saved to <code>BillyJoel.dta</code>. Summaries of the data are included in files in the package:<DD><pre>
     &bull; view BillyJoel.smcl
     &bull; view BillyJoel.cdb.txt</pre>
The NLSY codebook from the extracted data is
<pre>
     &bull; view BillyJoel.NLYScdb.txt</pre>
You need to look at the file to find out the interpretation of negative values.  Those values code valid and invalid skips.</DD>

<LI>Use Stata data handling techniques discussed so far to compute accurate two aspects of first intercourse among females by religion</LI>
    <DD><em>Median</em> age of first intercourse within religious groups </DD>
    <DD><em>Fraction</em> of each group reporting first intercourse before age 17.</DD>
This requires accounting for missing values (both valid and invalid skips), cross tabulations and summarize data within groups, and handling of <em>categorical</em> data.

<LI>You will produce 12 numbers from the data, which would go in this table</LI>
<DD><pre>
                            Catholic   OtherChristian  NonChristian
UnWeighted
        % Before 17              (01)      (02)           (03)
        Median Age 1st           (04)      (05)           (06)

<em><q>Bonus</q> Calculations
Weighted
        % Before 17              (07)      (08)           (09)
        Median Age 1st           (10)      (11)           (12)</em>
</pre></DD>
<UL>
<LI><q>UnWeighted</q> Simple tabulations from the data (after accounting for skips and missing values)</LI>
<LI><q>% Before 17</q> is the fraction of the group that reports first having sexual intercourse before they were 17.</LI>
<LI><q>Median Age 1st</q> is the median age of the group at first intercourse.  (Why median? Why not mean?)</LI>
<LI><q>OtherChristian</q> means raised in a Christian religion other than Catholic.</LI>
<LI><q>NonChristian</q> means raised in other religions or in no religion</LI>
<LI>Bonus Calculations:
The first six numbers are "required". Computing the Weighted figures is not required, but you might use this as an opportunity to read up on Stata's ability to handle weighted data.  Tabulations that use the sampling weight variable included in the data set so that the numbers are estimates of the population the NLSY is drawn from.</LI></UL>

You can produce a table with your 12 numbers inserted, but that is optional (see below).

<LI>Things You Need to Deal With / Account For</LI>
<UL>
<LI>The data provided include boys.  You must <code>drop</code> these observations.</LI>
<LI>The relevant questions were asked twice, in 1983 and 1984.  Some people refuse to answer, and some give inconsistent answers.  You must read the <q>codebook</q> <code>billyjoel.cdb.txt</code> to understand the codes, and you need to decide how to handle these cases.  (There is not necessarily one right way to do this, so answers may differ, but not radically.</LI>
<LI>Hint for the bonus calculation:  The sampling weight variable should be used as a <code>frequency weight</code>.</LI>
</UL>

<DT>Stata commands and functions you might need or find useful</DT>
<DD><pre> drop if   mvencode    recode  tabulate    generate    round()</pre></DD>

<LI>Provide your answers in the right format</LI>
<UL>
<LI>A Stata data set with exactly one observation and 7 or 13 variables.</code>.
    <DD>One variable must be called <code>code</code> and contains your Econ 452 Code as a string.</DD>
    <DD>The other variables must be named <code>n01</code> to <code>n12</code> and correspond to your computed values.</DD>
    <DD>If you don't do the bonus weighted calculations you can either make <code>n07-n12</code> missing or not in your data set at all.</DD></LI>

<LI> A <code>log</code> file and a <code>do</code> file that produces the numbers in your submitted data set. (The Stata commands do not have to generate your data set ... you can do that by typing in the numbers or cutting and pasting). </LI>

<LI>A paragraph or two that concludes whether Billy Joel Sociology is consistent with the facts in the NLSY data.  This paragraph can be accompanied by the optional table shown above with your numbers inserted. (This can be a .txt file, a .doc file or a .pdf file.)</LI>
<LI>Yes, we should test his hypothesis formally but for our purposes you can just eyeball the numbers and come to a conclusion. </LI>
</UL>
</OL>

<H4>Additional Notes</H4>
Numbers you produce should show reasonable handling of the data, even if they disagree with my answer (and/or the correct ones). You and your partner are encouraged to discuss this task with groups and even work side-by-side on it.  But you and your partner should do all the work involved in the submission separate from other groups.  Do not go to the trouble of trying to mask duplicate work by making minor changes to another group's submission. </p>

What else can I do with this opportunity that may help with the Course Project? Run a <em>probit</em> model for <q>before 17</q> using indicator variables for religion.  Include in your log file. Go to the NLSY website and for the 1979 Cohort find these variables: <code>Birth Year of the Respondent's Mother</code>; <code>Primary Racial Identity of the Respondent</code>; The equivalent questions about sex asked of males. Download these variables and the respondent ID.  Read the data into Stata using <code>Import using a dictionary file</code>.  (I can help with this step). <code>Merge</code> your new data with the data set I provided based on the ID variable. Augment your analysis with the new data: Produce similar figures for males. Try using econometric models to account for more than religion, including race and whether the respondent's mother was young when she had the respondent (or other variables you wish to merge in).</p>

What <em>not</em> to apply from this task to your Course Project? This is supposed to be a somewhat interesting and light-hearted exercise that requires real skill to do properly. This is not all meant to be a model for your Course Project. Do <em>not</em> &hellip;
<UL>
<LI> pick your topic from song lyrics or other whimsical ideas</LI>
<LI> reference no published research in explaining your topic, data and methods</LI>
<LI> focus on one or two explanatory variables when other important factors are also available in the source data (that's what multivariate models are for)</LI>
<LI>eyeball sample statistics rather than test hypotheses formally</LI>
</UL> 
<h3><a name="s060"><LI>Homer Simpson Econometrics</LI></a></h3>

<blockquote><em>If something goes wrong at the plant, blame the guy who can't speak English.</em></blockquote>

<h4>Data</h4>

The data come the archive of the J. of Applied Econometrics (JAE) for the paper:
<pre>
"Optimal Response to a Shift in Regulatory Regime: The Case of the US Nuclear Power Industry,"
John Rust and Geoffrey Rothwell,
<em>Journal of Applied Econometrics 10, Special Issue: The Microeconometrics of Dynamic Decision Making</em>
(Dec. 1995), pp. S75-S118.
</pre>
The JAE archive happens to be hosted here ... in Dunning Hall: <a href="http://qed.econ.queensu.ca/jae/1995-v10.S/rust-rothwell/">http://qed.econ.queensu.ca/jae/1995-v10.S/rust-rothwell/</a>

The original data are in many separate files. I have combined those into one file (plants.raw) and read them into Stata data sets for you. The main file is <code>HomerSimpson.dta.</code> That file was created using
Stata's <code>infile </code>command, and a "dictionary" file that I wrote. The command that created HomerSimpson.dta is simply:
<DD><pre>
&bull;  infile using rr, using(plants)
&bull;   saveold HomerSimpson
</pre></DD>
<code>infile</code> command reads the in <code>rr.dct</code> which explains the structure of the "raw" file, <code>plants.raw.</code> so the data can be read in properly.</p>

The other file, <code>nukesum.dta</code> contains some other information about the plants, but you will probably find no use for that data. If you want to use it with the other data set use <code>merge</code>. Unlike the NGS and NLSY data used earlier, <code>HomerSimpson.dta</code> is fairly simple because most of the processing was already done by the authors of the paper. However, you should always look at the data and verify coding, deal with missing observations, etc.</p>

The authors' documentation of the data file is also available:
<DD>In Stata: <pre>&bull;  view readme.rr.txt</pre></DD>
Or, at the JAE archive above.

<h4>What To Do</h4>

<OL class="steps">
<li>Join one of the Homer Simpson Econometrics Partnerships at onQ. If you are working with a partner you both join the same group.  This provides you with a "locker" where you can upload files to share with each other.  Your group also as a "dropbox" that you will use to upload your reports</li>

<LI>Get the material and load the data:
<DD><pre>
    Make sure the Stata Working Directory is where you want to work
    &bull;  get452 TakeHome HomerSimpson
    &bull;  use HomerSimpson
</pre></DD>

<LI> Skim the first two sections of the paper to understand the meaning of the data.   In particular, try to re-produce the numbers in Figure on S91 and Table I on page S92.  Attempting to replicate summary statistics that someone else reports is a way to learn what's in the data, how it is coded, etc., because you will know when you get it right.  However, I have not attempted this myself and sometimes getting an exact match can be frustrating. The paper uses a sophisticated econometric analysis well beyond the scope of this class. So do not worry about understanding section 3 and beyond.</LI>

<LI>Develop an econometric model (or possibly models) for the data that reveals something about nuclear power
plants.  That is, specify an population equation where we can think of $Y$ as an outcome of the plant's operation that depends on technology, regulation, and management competence and decisions.   The $X$ variables should then be observed values that measure or proxy for those factors. The variables you include in your model might be transformations of the ones in the original data, which you will implement using Stata commands and/or syntax options. A good answer will demonstrate some of the lessons concerning <em>specification</em> that we discuss in class and which are covered in econometrics textbooks.  Feel free to consider a regression model, a probit model or something more advanced. It won't hurt to try something you don't fully understand.</LI>

<LI>Once you settle on your specification, create a <code>do file</code> with the commands that carry out your analysis, starting from the original <code>.dta</code> file.  Your do file should open a log file which you will also submit.  Write a brief explanation of what you did (variables chosen, technique used) and what you learn from it.</LI>

<LI>Submit Your Files.  Remember, you must upload the files above to the Partnership <b>DropBox</b> to complete the assignment.  Files that you have shared with your partner in your "locker" are not automatically available.</LI>
</OL>
<h3><a name="s061"><LI>Proposition Joe Prospectus</LI></a></h3>

<blockquote class="upshot">
<pre><em>
Wanna know what kills more police than bullets and liquor? Boredom.
They just can't handle that sh--.
You keep it boring, String.
You keep it dead f-ing boring.
</em></pre>
Source:  Joseph 'Proposition Joe' Stewart, "The Wire" Season 3 Episode 3</blockquote>

<h4>What To Do</h4>

<OL class="steps">
<li>Join one of the Prop Joe Partnerships at onQ. If you are working with a partner you both join the same group.  This provides you with a "locker" where you can upload files to share with each other.  Your group also as a "dropbox" that you will use to upload your reports</li>
<LI>Write up a proposal for an econometric analysis.  You have 3 (mutually exclusive) sources of your proposal (the first two having sub-options):</LI>
    <OL class="compact">
    <li>Propose to estimate a model using data from this survey:
        <pre>
        Survey of Financial Security 2005.
        http://search2.odesi.ca/#/details?uri=%2Fodesi%2Fsfs-13M0006-E-2005.xml
        </pre>
    </li>
    <li>Come up with your own data set, model, and relevant literature.  (That is, propose what you would actually do for the Course Project.  You are free to change your mind about the project. You are
        not committed to pursue what you propose.)</li>
    </OL>
<li>Template</li>
<pre>
Title:  <em>Give your project a title</em>

Topic:  <em>Give your project a one sentence explanation</em>

Data Source(s)
    <em>Describe the data you plan to use.  Source of data, titles of survey, URL if available.  </em>

Endogenous Variable(s)
    <em>Describe the variable(s) you plan to study as endogenous</em>
    
Exogenous Variables & Specification
    <em>Describe variables in the data that you will include as X variables. </em>
    
Reference(s)
    <em>Refer to a published paper(s) that relates to your plans.  Briefly
    summarize its analysis and its conclusion and how your focus is similar and/or different.</em>
    
Current Status
    <em>Briefly say where you are in terms of preparing data, finding references, etc.</em>
    
</pre>

</OL>

</OL>
<h2><a name="s062"><LI>Quizzes and Optional Stata Challenges</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s063"><LI>Stata Challenge 1</LI></a></h3>

Submit your answers by creating a thread under the Challenge 1 topic in the <em>Tutorial and Demonstration Material</em>. There is no credit associated with completion of this challenge.  But the skills demonstrated by this challenge will be valuable in the Stata Quiz and Group Tournament B.

<h4>Step-by-step Instructions </h4>

<OL class="steps">
<LI>(If not done during the tutorial: use get452 to get the week1 tutorial file.)  Follow the notes on page 8 along with the Stata material and exercises in Part II of notes to do this.</LI>

<LI>Start a <code>cmdlog</code> so that your commands are recorded but not output.  (Note the name of the log file when it is created.) </LI>

<LI>Working interactively do the following (which will require searching for variables, transforming them, etc.):
    <OL class="compact">
    <LI>Start a regular log file.  And remember advice about <code>replace</code> so that later steps will work.</LI>
    <LI><em>Use</em> the <code>cas2004</code> data set.</LI>
    <LI><em>Find</em> the variables that group age into <b>9</b> categories and the three variables  that code whether the person has used alcohol, marijuana/cannabis or cocaine in the last 12 months. Get rid of all other variables except those four from the data.
    <LI>Create 2-way cross-tabulations of age categories with each substance use variable.  Identify which age category has the highest fraction of use. (<em>Hint: look at the options such as <code>row</code> that you can add to the <code>tabulate</code> command</em>.) </LI>
    <LI>Use 2-way tabs to determine whether people who used alcohol are more or less likely to have used cannabis and cocaine. (There are other ways to do this too.)</LI>
    <LI>Once you have 4 variables in the data set then <em>save</em> the data to a new <code>.dta</code> file.  Note that <code>save</code> will not write over an existing file unless you include the <code>replace</code> option, which will be needed later.</LI>
    <LI>Close both the command and regular logfiles.</LI>
    </OL>
<LI>Open the command log file with the do file editor.  (Note: when trying to open the file you have to ask the dialog to show you All Files.  By default it will only show you <code>.do</code> and <code>.ado</code> files and command logs are given the <code>.txt</code> extension.)</LI>
<LI>Edit the command log so that only the commands that work and do the tasks correctly are included.  Make sure your code will work even if files already exist that the code is supposed to replace</LI>
<LI>Rename (Save As) the file to be a <code>.do</code> file and <b>do</b> it. Edit it until it runs successfully.</LI>
<LI>Create a new thread in response to the Challenge 1 topic.  Upload the <code>.do</code> file and the <code>.smcl</code> log file, and the (small) <code>.dta</code> file created by the do file.  In the body of your response list which age brackets are the heaviest users and whether alcohol is associated with more or less cocaine and cannabis use.</LI>
</OL>
<h3><a name="s064"><LI>Stata Quiz 1</LI></a></h3>
<span style="font-size: 10pt;line-height : 10pt;">
You are at your computer using Stata and you nod off to sleep for minute.  You wake up and here is what is in the results window (stuff you did before you dozed off). Describe how you would do the tasks listed below. (Preferably, just list the Stata command.  Otherwise, explain how you would use menus and windows.)
<pre>. sysuse nlsw88.dta
. keep age race south married collgrad industry wage hours
. describe
    variable name   type    format     label      variable label
    -----------------------------------------------------------------------
    age             byte    %8.0g                 age in current year
    race            byte    %8.0g      racelbl    race
    married         byte    %8.0g      marlbl     married
    collgrad        byte    %16.0g     gradlbl    college graduate
    south           byte    %8.0g                 lives in south
    industry        byte    %23.0g     indlbl     industry
    wage            float   %9.0g                 hourly wage
    hours           byte    %8.0g                 usual hours worked
. summarize
        Variable |       Obs        Mean    Std. Dev.       Min        Max
    -------------+--------------------------------------------------------
             age |      2246    39.15316    3.060002         34         46
            race |      2246    1.282725    .4754413          1          3
         married |      2246    .6420303    .4795099          0          1
        collgrad |      2246    .2368655    .4252538          0          1
           south |      2246    .4194123    .4935728          0          1
        industry |      2232    8.189516    3.010875          1         12
            wage |      2246    7.766949    5.755523   1.004952   40.74659
           hours |      2242    37.21811    10.50914          1         80</pre>
<span class="">
<OL class="compact">
<LI>Start a log file<DD><pre class="answer"> </br> </br> </pre></DD></LI>
<LI>Find out how many people's job is in Manufacturing.<DD><pre> </br> </br> </pre></DD></LI>
<LI>Find out whether the ratio of college graduates to non-graduates is larger in the south or the rest of the country.<DD><pre class="answer"> </br> </br> </pre></DD></LI>
<LI>Create a variable called <code>agesq</code> that equals the square of age, $age^2$.
<DD><pre class="answer"> </br> </br> </pre></DD></LI>
<LI>Make it so the data set only includes people who work 35 or more hours per week.
<DD><pre class="answer"></br> </br> </pre></DD></LI>
<LI>Estimate this regression: $\ln(wage) = \beta_0 + \beta_1 collgrad + \beta_3 age + \beta_4 agesq + \sigma z.$<DD><pre class="answer"></br> </br>  </br> </br> </pre></DD></LI>
<LI>You now realize that step e was a mistake. Since you changed the data you have to start over.  <em>Briefly </em> describe a good way to do this.<DD><pre class="answer"></br> </br>  </br> </br> </pre></DD></LI>
<LI>Having recovered from the mistake, create a variable that is an indicator for working full time (35+ hours)<DD><pre class="answer"></br> </br> </pre></DD></LI>
<LI>Estimate this probit: $Fulltime^\star = \beta_0 + \beta_1 collgrad + \beta_2 married + z $.
<DD><pre class="answer"></br> </br> </pre></DD></LI>
</OL>
</span>
</span>
<h3><a name="s065"><LI>Stata Quiz 2</LI></a></h3>
<span style="font-size: 10pt;line-height : 10pt;">
PART 1. You are using Stata to study birth outcomes, in particular whether a baby is born with a low birth weight, defined as less than 2500 grams.  An edited/trimmed version of what is in the results window. Show how to do the tasks listed below.
<pre>. webuse lbw
. keep low age lwt race smoke

. summmarize

Variable |   Obs        Mean    Std. Dev.    Min     Max   |  Variable label
---------+-------------------------------------------------+-----------------------------------
     low |   189    .3121693    .4646093       0       1   |  BABY's birthweight < 2500g
     age |   189     23.2381    5.298678      14      45   |  age of mother
     lwt |   189    129.8201    30.57515      80     250   |  MOTHER'S weight at last menstrual period
    race |   189    1.846561    .9183422       1       3   |  race
   smoke |   189    .3915344    .4893898       0       1   |  smoked during pregnancy

. probit low age lwt

    Iteration 0:   log likelihood =   -117.336
    &vellip;
    Probit regression                                 Number of obs   =        189
                                                      LR chi2(2)      =       7.49
                                                      Prob > chi2     =     0.0236
    Log likelihood = -113.58854                       Pseudo R2       =     0.0319
    ------------------------------------------------------------------------------
             low |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
    -------------+----------------------------------------------------------------
             age |  -.0244235   .0194849    -1.25   0.210    -.0626132    .0137662
             lwt |  -.0073945   .0035388    -2.09   0.037    -.0143304   -.0004586

. estimates store R
</pre>
<OL class="compact">
<span class="hide">
<LI>Is a heavier mother less likely to have a low weight birth, all else constant?  Explain briefly how the results lead you to conclude <q>Yes</q> or <q>No</q>.
    <DD><pre class="answer"><br/> </br> </br> </br> </br> </pre></DD></LI>
<LI>The probit above is the <q>restricted</q> model.  The <em>unrestricted</em> model you are interested in allows the probability of low birth weight to <em>also</em> depend on race, smoking and an interaction between age and smoking. Complete this command to run that probit:
<DD><pre class="answer">. probit low  </br> </br> </br> </br> </pre></DD></LI>
<LI>Commands to test the restricted model as the null hypothesis:
<DD><pre class="answer"><br/> </br> </br> </br> </br> </pre></DD></LI>
<!--<LI>Degrees of freedom of your test statistic:<DD><pre class="answer">k = <br/> </br> </br></pre></DD></LI>-->
<LI>Using numbers available, write down the expression that produced the Iteration 0 log-likelihood of -117.336:
<DD><pre class="answer">-117 =  </br> </br> </pre></DD></LI>
</span></OL>
PART 2.  Given the regression output below, identify the following values (answers should be numbers or expressions with numbers in the output).
<pre>
. gen lnwage = log(wage)

. gen xpersq = xper * xper

. regress lnwage grade nonwhite xper xpersq

      Source |       SS       df       MS              Number of obs =    2244
-------------+------------------------------           F(  4,  2239) =  212.88
       Model |  204.157641     4  51.0394104           Prob > F      =  0.0000
    Residual |  536.805158  2239  .239752192           R-squared     =  0.2755
-------------+------------------------------           Adj R-squared =  0.2742
       Total |  740.962799  2243  .330344538           Root MSE      =  .48964
------------------------------------------------------------------------------
      lnwage |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       grade |   .0768256    .004266    18.01   0.000     .0684598    .0851914
    nonwhite |  -.1013362   .0235506    -4.30   0.000    -.1475195   -.0551528
        xper |   .0573668   .0097866     5.86   0.000     .0381751    .0765585
      xpersq |  -.0007267   .0003943    -1.84   0.065       -.0015    .0000466
       _cons |   .3005866   .0724862     4.15   0.000     .1584395    .4427337
</pre><span class="hide">
<OL class="compact">
<LI value="5">$\hat\sigma^2$ = <DD><pre class="answer"><br/><br/></pre></DD></LI>
<LI>${\partial \hat{E}[ \ln\hbox{wage} | X] \over d \hbox{xper} }$ = <DD><pre class="answer"><br/><br/></pre></DD></LI>
<li>The estimated ln-wage gap between white and non-white workers =  <DD><pre class="answer"><br/><br/></pre></DD></LI>
</OL>
</span>
</span> 
</OL>
<h2><a name="s066"><LI>Course Project</LI></a></h2>
<OL  type="1" class="toc3" >
<h3><a name="s067"><LI>Overview</LI></a></h3>

<OL class="steps">
<LI>You decide to work alone or together with a single partner. You do this by joining one of the Course Project groups onQ.</LI>

<LI>You (and your partner) look for a data set and a question/topic to study using one of the econometric techniques we have covered.</LI>

<LI>If you are on the ball, you can use the "Prop Joe Prospectus" task to get feedback on a possible data set and econometric method.  In particular, any team submitting a proposal that has some thought behind it is entitled to detailed feedback and more ongoing support than students/teams that use one of the common data sets or papers to write Prop Joe on. (Note: there is no obligation to use what you propose here for your course project.  I might suggest it is not a good idea and/or you might come up with something better and switch.  So there is essentially no risk in writing a proposal on a topic of your own choosing and there many benefits to do so.</LI>

<LI>You write a report on the analysis that follows the typical structure of an applied econometrics article.</LI>

<LI>You (and your partner) post the report as a single PDF along with the mandatory supporting material (log file, do file, links to the source data).</LI>

<LI>Do not choose data sets &hellip;</LI>
    <OL class="compact">
<LI>we use as examples in class, assignments, etc.</LI>
<LI>sample data sets that come with Stata or other small data sets that are not designed for academic research</LI>
<li>data that are primarily time series (such as stock market indices, single country macro data, etc)</li>
</OL>

<LI>Feel free to ask my advice about a particular data set.  I will give my view of whether the data and your plan makes sense and is doable, but I cannot anticipate many problems that might arise once you get going. (I am more committed to help people who do something original on Prop Joe than teams that come up with a data set after that.)</LI>

<LI>Choose to analyze your data with a method or methods covered in the course.  If you think a different method is better you should discuss it with me ahead of time.</LI>

<LI>As you might expect, a A+ project (score=7) will be both ambitious and well-executed.  But the complexity of the analysis is itself not important.  A modest project that is (very) well executed and reported can receive an A (score=6).</LI>

<LI>Write your report using as a guide applied econometric articles, advice given in class and in these notes. Not all articles are written exactly the same so there is not a single standard or template to emulate.</LI>

</OL>

<h3><a name="s068"><LI>The Process</LI></a></h3>


<h5>&alpha;. Find a Topic, a Data Set, and an Econometric Framework</h5>

<OL class="faq">
<LI>What is a <em>topic</em>?</LI>

A topic for this course is usually a question (or questions) about what determines the value of an *endogenous* variable, call it Y. The question should typically be about how the *expected* value of Y varies with one or more other variables, X1, X2, etc.  These variables will are called <em>exogenous variables</em>. In this class we cover a couple cases in which more than one variable is considered endogenous, but most Econ 452 projects study a single Y variable. Because Y's value is never completely determined by the values of the X variables, the questions(s) about Y will be translated into questions about coefficients on the X variables, including their sign and magnitude. These coefficients appear in the specification of an equation that relates Y to the X variables. Of course, we do not see these coefficients but can only estimate them. Therefore, the question gets converted into statistical inference (hypothesis testing, confidence intervals, and predictions).</DD>

<LI>What kind of topic should I look for?</LI>

For this class I suggest:
<blockquote>Find an applied econometric article that uses a data set similar to one you can access. Then carry out the same or similar estimation.</blockquote>

<LI>What kind of subjects should I look for a topic in?</LI>

<DD>The best places to find a topic are &hellip;
<UL class="ul">
    <LI>&hellip; from your other economics courses or possible social science courses.  Since this is MICROeconometrics, courses such as labour, public finance, etc. are the best fit.</LI>
    <LI>&hellip;reading good but not technical applied economic journals. We will use <em>Canadian Public Policy</em> as a guide for this
        course, but it is likely that you will base your project on articles from other sources.</LI>
</UL></DD>
<DD>Less successful sources of topics &hellip;
 <UL class="ul">
    <LI>&hellip; a current issue in the news</LI>
    <LI>&hellip;your own ideas that are not closely connected to a published research</LI>
 </UL>
 These sources of topics can be successful but they can also be frustrating and difficult. Even if you find no existing study of your question you will find studies of similar questions.  Projects that attempt to answer novel questions without following one or two existing studies tend to be less successful.</DD>

<LI>What Data Set?</LI>

<LI>Which Econometric Framework?</LI>

You will be less frustrated and more successful if you realize Step 1 is usually only completed after trying out one or more topics, data sets or methods.  That is, you proceed to step 2 and even step 3 before realizing your idea won't  go through and you have to return to step 1.  Ultimately topic/data/framework must be chosen together.  That is, completing Step 1 is an iterative process. Further, it is not unusual to not fully understand the topic until you ave started  trying to write a report explaining your results.  You may at this late stage realize that the question you can and have answered is somewhat different than you thought, and this might suggest going back and changing some.
</OL>


<h5>&beta;. Create An Estimation Sample; Explore Patterns</h5>

<OL class="steps">

<LI>Data must always be read into Stata and transformed in order to carry out a proper estimation.</LI>

 <UL class="ul">
 <LI>Load / import the data into Stata</LI>
 <LI>Merge variables obtained from other data sources</LI>
 <LI>Drop observations that do not meet criteria</LI>
 <LI>Handle missing values of variables properly</LI>
 <LI>Transform variables</LI>
 <LI>Create new variables that are needed</LI>
 <LI>Compute summary statistics for the data.</LI>
 <LI>Ensure that you understand the coding and meaning of all variables you use:</LI>
    <DD>Tabulate categorical variables</DD>
    <DD>cross-tabulate variables</DD>
    <DD>Graph data, looking for patterns, coding errors, etc.</DD>
 </UL>

<LI>Ultimately your Stata work results in an <em>estimation sample</em></LI>
</OL>

<h5>&gamma;. Choose Specifications</h5>

<UL class="ul">
    <LI>Run simple econometric models (regressions, probits), looking for unusual coefficient values, standard errors,etc.</LI>
    <LI>Consider the advanced techniques discussed in class and consider if any of them are appropriate and feasible</LI>
    <LI>Study the specifications used in published papers on similar topics and consider adopting them for your specification (which variables to include,
        interactions, etc.</LI>
    <LI>Choose an overall strategy</LI>
</UL>

<h5>&delta;. Look for ways to use the estimated model(s).  Here are some possibilities:</h5>
<UL class="ul">
    <LI>Prediction</LI>
    <LI>Hypothesis Testing</LI>
    <LI>Comparison to other results</LI>
    <LI>Economic Implications.</LI>
</UL>

<h5>&epsilon; Complete Final Estimates and a Draft Report</h5>

<UL class="ul">
<LI>Finalize a <code>do file</code> that:</LI>
<DD>Starts with the original data and does all the work required by the final report.</DD>
<DD>The master do file might in turn do other files so that no one file is more than 10 or so Stata commands.</DD>
<DD>The do file(s) and corresponding <code>log file</code> will be handed in along with the project report.</DD>

<LI>Write a report in the style of research journal articles.</LI>

<li>If you can submit a <em>draft</em> report by the Feedback Deadline then you will get feedback on your work.</li>
<DD>Modify the do file to implement changes to the data and estimation if called for.</DD>
<DD>Edit the report accordingly and along lines of editorial feedback</DD>
</UL>

<h5>&zeta;. Submit the final report on time including all required supporting material: do files, log files, etc.</h5>
<h3><a name="s069"><LI>Some Data Sources</LI></a></h3>

The sites below provide you access to hundreds of Canadian survey data sets, several large and powerful U.S. longitudinal surveys, and internationally comparable household surveys to do cross-country analyses.  Each of these sources was used for previous Econ 452 reports.  However, many reports were based on data from other sources found by students.

<DT>ODESI</DT>

<DD><code>http://search2.odesi.ca/</code></DD>

<DD>Ontario Data Documentation, Extraction Service and Infrastructure is a digital repository for social science data, including polling data.  This site is essentially the only place for students to access Statistics Canada surveys. These include:</DD>
<DD>NGS: National Graduate Surveys</DD>
<DD>NLSCY: National Longitudinal Survey of Children and Youth</DD>
<DD>GSS: General Social Surveys (conducted regularly focussing on rotating topics)</DD>
<DD>Canadian Census Micro data files</DD>

<DT>ICPSR</DT>

<DD><code>https://www.icpsr.umich.edu/icpsrweb/ICPSR/</code></DD>

<DD>Inter-university Consortium for Political and Social Research. Some ICPSR data sets are public use.  Others require creation of an account to access.  Queen's is a member institution, so you should be able to create an account with your Queen's identity to access any of the data.</DD>

<DT>NLS</DT>

<DD><code>https://www.bls.gov/nls/</code></DD>

<DD>National Longitudinal Surveys.  The NLS, sponsored by the U.S. Bureau of Labor Statistics, are nationally representative surveys that follow the same sample of individuals from specific birth cohorts over time. The surveys collect data on labor market activity, schooling, fertility, program participation, health, and much, much more.</DD>

<DT>IPUMS</DT>

<DD><code>https://www.ipums.org/</code></DD>

<DD>Integrated Public Use Microdata Series.  IPUMS-International is a project dedicated to collecting and distributing census data from around the world. IPUMS also creates data sets from U.S. sources.</DD>

<DT>LIS</DT>
<DD><code>http://www.lisdatacenter.org/</code></DD>

<DD>LIS, formerly known as The Luxembourg Income Study, is a data archive and research center dedicated to cross-national analysis and is home to two databases:</DD>

<DD>The Luxembourg Income Study Database (LIS) is the largest available income database of harmonised microdata collected from about 50 countries in Europe, North America, Latin America, Africa, Asia, and Australasia spanning five decades.</DD>

<DD>The Luxembourg Wealth Study Database (LWS) is the only cross-national wealth microdatabase in existence.</DD>

<h3><a name="s070"><LI>Creating a Research Report</LI></a></h3>
<div class="break"></div><blockquote class="upshot"><h5>Creating a Research Report</h5> is not ready. This page is left blank to provide some room for taking notes.</blockquote><div class="break"></div>
<h3><a name="s071"><LI>Some Dos &amp; Don'ts</LI></a></h3>

<OL class="steps">

<LI>Data and tables</LI>
<UL>
<LI><b>Do</b> learn to use Stata's table and estimates table commands</LI>
<LI><b>Do</b> limit use of lines (rules) in Tables. Typically, only horizontal lines on the outside and below the column headers.</LI>
<LI><b>Do</b> round figures to 3 or 4 digits. </LI>
<LI> <b>Do</b>  use *, **, and *** to indicate levels of significance.</LI>
<LI><b>Do</b> report coefficients, standard errors, overall test statistic and its p-value, number of observations.</LI>
<LI>Do <b>not</b> use code names. In the report give variables meaningful letter or abbreviated names. (E.g. "Inc" or "I" instead of PL546_INC") </LI>
</UL>

<LI>Model Specifications</LI>
<UL>
<LI>Do <b>not</b> drop variables that are insignificant.  Chose variables based on a model or clear intuition for what should be important and can be treated as exogenous.</LI>
<LI>Do <b>not</b> code binary outcomes as anything except 0/1.  Make sure your data is not using 1 and 2 or other codes.</LI>
<LI><b>Avoid</b>  including variables like "satisfaction", self-reported Academic Rank, opinions like "are you over-qualified for your job." in your specification. These variables might seem interesting to you, but they are not really. How do we interpret results using these variables to understand economic policy? One exception: health is very important outcome related to economics and often surveys are only able to provide self-reported health.</LI>
<LI>When stating your model's specification as an equation <b>do</b> include the error term.</LI>
</UL>

<li>Writing</li>
<UL>
<LI>Chose the <em>correct</em> imaginary reader of your report.  I (the instructor) am <em>not</em> the right reader to imagine, nor is the TA who will also read your report.  <em>The reader of your report is a busy economics professor at another university who knows some econometrics.</em> She is not reading your report to see if you were paying attention in class. You must give this person a reason to take your results seriously. This person is motivated by academic research, important policy questions, and economic theories that might apply to your data.</LI>
<LI>Motivation: Some students misunderstood this part of the report. You do not explain YOUR motivation for the analysis ("I'm an undergraduate so I think it would be interesting to look at this.") Motivation is for the reader - why should they read this, given that they don't know you personally, are not required to give you or grade.</LI>
<li>Do <b>not</b> use "trends" everyone supposedly knows, media reports (a CBC story from 2013 quoted someone).</li>
<LI><b>Avoid</b> adjectives that are subjective. Be concrete: I analyze, I study, not "I would like to analyze"</LI>
<li><b>Do</b> Use academic citations:
    <DD>Yes: "Betts et al. (2013) use three waves of the NGS to study ..."</DD>
    <DD>No: "In a paper entitled 'University Quality' the authors Julian Betts Christopher Ferrall, and Ross Finnie study ..."</DD></li>
<LI>Do <b>not</b> write a procedural: "First I dropped these observations, then I recode 2 to 1".  Your reader needs to know which observations were dropped or modified, but they want this information in a form for understanding the results not for knowing your thought process.</LI>
</UL>

<LI>Plagiarism and Verification</LI>
<UL>
<LI><b>Do</b> ensure that the Stata code you submit generates the log files you submit, which in turn must be the basis of the reported results you submit. The <code>do</code> file you submit must be created by the team's own effort, unless you have discussed use of someone else's code with the instructor and have properly cited the source.</LI>
<LI><b>Do</b> base your project on results published in academic articles, but do something <em>beyond</em> the published results and any supporting resources for the article.  That is, if data and Stata code  are made available by the original authors you cannot just run their code and re-format their results.</LI>
</UL>
</OL> 
</OL>
<footer><table width="100%"><tr><td width="20px"><a href="s.html">&larr;</a></td><td style="text-align:center">&copy; Christopher Ferrall 2016.  Queen's University.</td><td width="20px"><a href="s.html">&rarr;</a></td></tr></table></footer></body></html>
